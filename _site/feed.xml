<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Vitor Meriat</title>
    <description></description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sun, 10 Nov 2019 16:38:22 -0300</pubDate>
    <lastBuildDate>Sun, 10 Nov 2019 16:38:22 -0300</lastBuildDate>
    <generator>Jekyll v3.4.0</generator>
    
      <item>
        <title>NLP under the hood</title>
        <description>&lt;div align=&quot;center&quot; style=&quot;width: 100%;&quot;&gt;&lt;img src=&quot;https://meriatblob.blob.core.windows.net/draft/capa.png&quot; style=&quot;margin-bottom: 0px !important;&quot; /&gt;&lt;/div&gt;

&lt;h1 id=&quot;introdução&quot;&gt;Introdução&lt;/h1&gt;

&lt;p&gt;O método científico supõe que a observação dos fatos seja anterior ao estabelecimento de uma hipótese e que os fatos observados sejam examinados sistematicamente mediante experimentação e uma teoria adequada.&lt;/p&gt;

&lt;p&gt;Com isso em mente, se faz necessário o estudo de alguns pontos utilizados na disciplina de processamento de linguagem natural de forma a obter a base necessária para uma correta exploração e aplicação das possibilidades.&lt;/p&gt;

&lt;p&gt;Esse trabalho se propõe a trazer uma introdução ao estudo do &lt;code&gt;Processamento de Linguagem Natural&lt;/code&gt; (&lt;code&gt;Natural Language Processing&lt;/code&gt;). Minha intenção é olhar para sua base teórica enquanto disciplina. Sendo assim vamos passar por algumas definições e conceitos antes de avançar nas questões práticas. Vamos falar sobre a estrutura de uma linguagem, compiladores, árvores sintáticas e as complexidades da linguagem natural antes do famoso mão na massa, até por que &lt;code&gt;talk is cheap, show me the code&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Antes de iniciar, se faz importante informar que alguns dos termos utilizados serão apresentados em inlgês e português. Para que não seja causada nenhuma estranheza ao leitor, vou priorizar os termos técnicos em inglês, e achando necessário realizo a explicação/tradução do mesmo. Para facilitar a leitura, algumas referências serão colocadas durante o texto. As demais estão todas na sessão de &lt;strong&gt;Referências&lt;/strong&gt; ao final deste texto.&lt;/p&gt;

&lt;div style=&quot;margin-bottom: 5em; margin-top: 5em; background-color: #35d648; color: #382d2d&quot;&gt;
&lt;p style=&quot;padding: 1.8em; font-family: courier; font-size: 1.4em;&quot;&gt;
“Minha pátria é minha língua.” &lt;b&gt;Fernando Pessoa&lt;/b&gt;
&lt;/p&gt;
&lt;/div&gt;

&lt;h1 id=&quot;conceituação-base&quot;&gt;Conceituação base&lt;/h1&gt;

&lt;p&gt;Este tópico é de grande importância, visto que muito da problemática encontrada na linguagem natural e sua compreensão, fazem parte do domínio computacional. Sendo assim, nosso objetivo quanto pesquisadores em Processamento de Linguagem Natural também inclui a resolução desses desafios, o que nos leva a procurar uma correta compreensão de seus conceitos base.&lt;/p&gt;

&lt;h2 id=&quot;teoria-da-comunicação&quot;&gt;Teoria da comunicação&lt;/h2&gt;

&lt;p&gt;Os seres humanos são considerados animais sociais e como tal, sabemos que a linguagem é nossa principal ferramenta de comunicação. Sabemos que a música é tão remota quanto o início da comunicação verbalizada, mas a principal diferença está nos papéis exercidos. Enquanto os sinais sonoros emitidos por instrumentos rudimentares foram seguindo o caminho da subjetividade, os sons cada vez mais coordenados dos seres humanos foram seguindo para se tornarem mais claros. Partimos dos grunidos para linguagens extramamente sofisticadas.&lt;/p&gt;

&lt;p&gt;Mesmo após tanta evolução na comunicação falada e escrita, ainda vemos que a linguagem é um assunto complexo. A linguagem é cheia de abstrações, fluída, ambígua e muitas vezes confusa. Apesar das definições gramaticais, a linguagem é um organismo vivo e se renova muito rapidamente. Diversos termos novos surgem a cada dia, e termos conhecidos recebem uma nova significância da forma já habitual.&lt;/p&gt;

&lt;p&gt;Quando falamos em teoria da comunicação, em termos básicos temos o papel do emissor, receptor, mensagem, código, contexto e canal. Cada um desses componentes é importante para determinar uma comunicação de sucesso. Se o emissor enviar uma mensagem para um receptor usando um código que não é conhecido pelo mesmo, ou se o contexto for desconhecido pelo receptor, ou se o canal de comunicação for insuficiente, a comunicação pode ser ruidosa e falha. Se tudo isso ainda for certeiro, temos de levar em conta que a mensagem vai ser interpretada por um receptor que vai levar em consideração sua perspectiva de mundo.&lt;/p&gt;

&lt;div align=&quot;center&quot; style=&quot;width: 100%;&quot;&gt;
&lt;img src=&quot;https://meriatblob.blob.core.windows.net/draft/theory_of_communications_2.png&quot; style=&quot;margin-bottom: 0px !important;&quot; /&gt;
&lt;/div&gt;
&lt;div align=&quot;center&quot; style=&quot;width: 100%;&quot;&gt;
&lt;img src=&quot;https://meriatblob.blob.core.windows.net/draft/theory_of_communications.jpg&quot; style=&quot;margin-bottom: 0px !important;&quot; /&gt;
&lt;/div&gt;

&lt;ol&gt;
  &lt;li&gt;A fonte (&lt;strong&gt;source&lt;/strong&gt;) produz uma mensagem. Uma mensagem pode ser um sinal de fumaça, telégrafo, rádio e etc.&lt;/li&gt;
  &lt;li&gt;Um transmissor (&lt;strong&gt;transmitter&lt;/strong&gt;) traduz a mensagem em um sinal que possa ser enviado por um meio específico.&lt;/li&gt;
  &lt;li&gt;O canal (&lt;strong&gt;medium&lt;/strong&gt;) é apenas o meio usado para transmitir o sinal do transmissor para o receptor. Pode ser um par de fios, um cabo coaxial, banda de radiofrequência, feixe de luz e etc.&lt;/li&gt;
  &lt;li&gt;O ruído (&lt;strong&gt;noise&lt;/strong&gt;) é tudo aquilo que possa interferir no sinal.&lt;/li&gt;
  &lt;li&gt;O receptor (&lt;strong&gt;receiver&lt;/strong&gt;) normalmente executa a operação inversa da realizada pelo transmissor, reconstruindo a mensagem a partir do sinal, para que o destino possa compreender.&lt;/li&gt;
  &lt;li&gt;O destino (&lt;strong&gt;destination&lt;/strong&gt;) é a pessoa/coisa a quem a mensagem se destina.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;linguística-básica-e-nomenclaturas&quot;&gt;Linguística básica e nomenclaturas&lt;/h2&gt;

&lt;p&gt;Se tratando em linguística, temos o estudo sobre o uso e funcionamento das línguas naturais, independentemente da sua especificidade e diversidade. Nesta ciência possuímos diversas nomenclaturas, para nosso objetivo, precisamos conhecer os itens abaixo:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Língua&lt;/strong&gt;: Podemos definir que a língua é, sobretudo, um instrumento de comunicação, e é essa a sua maior finalidade. Uma de suas riquezas e dificuldades, é que embora existam as normas gramaticais que regem um idioma, cada falante opta por uma forma de expressão que mais lhe convém, originando aquilo que chamamos de fala. A fala, embora possa ser criativa, deve ser regida por regras maiores e socialmente estabelecidas, caso contrário, cada um de nós criaria sua própria língua, o que impossibilitaria a comunicação. Na fala encontramos as variações linguísticas, visto que a língua é viva e dinâmica.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Idioma&lt;/strong&gt;: É uma língua própria de um povo. Está relacionado com a existência de um Estado político, sendo utilizado para identificar uma nação em relação às demais. Existem países, como o Canadá, por exemplo, em que dois idiomas são considerados como oficiais, nesse caso, o francês e o inglês.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Dialeto&lt;/strong&gt;: Por dialeto, temos o estudo da variedade de uma língua própria de uma região ou território e está relacionado com as variações linguísticas encontradas na fala de determinados grupos sociais. As variações linguísticas podem ser compreendidas a partir da análise de três diferentes fenômenos: exposição aos saberes convencionais (diferentes grupos sociais com maior ou menor acesso à educação formal utilizam a língua de maneiras diferentes); situação de uso (os falantes adequam-se linguisticamente às situações comunicacionais de acordo com o nível de formalidade) e contexto sociocultural (gírias e jargões podem dizer muito sobre grupos específicos formados por algum tipo de “simbiose” cultural).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Fonética&lt;/strong&gt;: Podemos dizer que a fonética é o estudo da realidade acústica, do funcionamento articulatório e anatómico e da interpretação percetiva dos sons de uma determinada língua natural.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Fonologia&lt;/strong&gt;: É o estudo do sistema sonoro de uma língua, das regras subjacentes à combinação desses sons e do modo como esses sons exprimem distinções de significado.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Morfologia&lt;/strong&gt;: A morfologia é a área da linguística que faz o estudo da formação e da estrutura interna das palavras dada uma determinada língua.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Sintaxe&lt;/strong&gt;: Estudo das regras subjacentes à organização das palavras numa frase gramaticalmente bem formada.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Semântica&lt;/strong&gt;: Estudo do significado da produção e interpretação de palavras e frases.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Pragmática&lt;/strong&gt;: Estudo do uso da língua em contexto por oposição ao estudo do sistema da língua.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Gramática&lt;/strong&gt;: A gramática formaliza a língua, seja realizando sua descrição, seja traçando as normas para o seu uso. A linguística analisa os fatos da língua na sua situação de uso e tenta explicá-los. Ambas tratam do mesmo assunto, mas sob ângulos diferentes.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;influências-e-formação-da-linguagem&quot;&gt;Influências e formação da linguagem&lt;/h2&gt;

&lt;div align=&quot;center&quot; style=&quot;width: 100%; font-size: 0.8em; font-family: courier;&quot;&gt;
&lt;img src=&quot;https://meriatblob.blob.core.windows.net/draft/lusofonos.jpg&quot; style=&quot;margin-bottom: 0px !important; width: 80%;&quot; /&gt;
&lt;p&gt;ilustração: Guilhere Lira/Mundo Estranho&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;Vamos fazer um exercício olhando para nossa lingua natal. Quantos países têm a lingua portuguesa como sua lingua mater? Desses países, todos falam exatamente a mesma lingua? Se iniciarmos uma comparação básica entre o que é falado no Brasil, Portugal, Moçanbique e Angola, haverá muita diferença?&lt;/p&gt;

&lt;p&gt;As grandes diferenças são as influências de línguas nativas e estrangeiras, que resultam em palavras e expressões particulares. O português brasileiro tem influência de línguas indígenas e de vários idiomas externos utilizados pelos os imigrantes, como árabes e italianos.&lt;/p&gt;

&lt;p&gt;Em Angola, há 11 línguas e diversos dialetos que transformam o português incluindo diveras palavras ao vocabulário. Em Moçambique, o português é influenciado pelas 20 línguas locais. Apesar de ser o idioma oficial do país, em Moçambique ele é falado por apenas 40% da população. O português de Portugal possui grandes diferenças em relação ao portugês brasileiro. Em questões gramaticais, o português de Angola e Moçambique são mais próximos do português europeu do que o brasileiro.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Por tudo isso, nos três países, há regionalismos que podem deixar o idioma incompreensível mesmo entre os lusófonos.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Por exemplo, aqui no Brasil nós adoramos abusar do tempo verbal gerúndio, muito pouco usado em outros países em questão. Por exemplo, usamos a frase &lt;strong&gt;“estou fazendo isso”&lt;/strong&gt; no lugar de &lt;strong&gt;“estou a fazer isso”&lt;/strong&gt;. Há também o gerundismo (uso desnecessário do gerúndio) como em &lt;strong&gt;“vamos estar averiguando”&lt;/strong&gt;.&lt;/p&gt;

&lt;div align=&quot;center&quot; style=&quot;width: 100%; font-size: 0.8em; font-family: courier;&quot;&gt;
&lt;img src=&quot;https://meriatblob.blob.core.windows.net/draft/same-language.jpg&quot; style=&quot;margin-bottom: 0px !important; width: 80%;&quot; /&gt;
&lt;p&gt; ilustração: Pilar Hernandez&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;Em Moçambique, a língua portuguesa também sofre modificações baseadas nas regras gramaticais de línguas locais. É o caso do verbo “nascer”, que lá é usado como na língua changana: “Meus pais nasceram minha irmã”. O mesmo vale para “Nos disseram que hoje não há aula”, que fica: “Nós fomos ditos que hoje não há aulas”.&lt;/p&gt;

&lt;p&gt;Essas estruturas são fundamentais para um correto entendimento e aplicação dos fundamentos em processamento de linguagem natural.&lt;/p&gt;

&lt;h2 id=&quot;história&quot;&gt;História&lt;/h2&gt;

&lt;p&gt;Quando como seres humanos cientes de nossa colocação no mundo nos preocupamos com o estudo da linguagem? Mais ainda, por que isso seria interessante? Este é um bônus, um ponto importante para ilustrar não só a mutabilidade como o papel da linguagem na construção e identidade de uma sociedade. A curiosidade humana sobre a linguagem é remoto e pode ser percebido por meio de vários mitos, lendas e rituais antigos.&lt;/p&gt;

&lt;p&gt;O início dessa jornada data do século IV a.c, onde os religiosos hindus iniciaram um estudo da língua a fim de preservar os textos sagrados do Veda. Esses estudos levaram a uma rápida evolução, e mais tarde o gramático Panini (século IV a.c.) em conjunto com outros estudiosos, produziram modelos de análise dado uma minunciosa descrição da própria língua. Estes modelos só foram descobertos pelo ocidente no final do século XVIII, quando principalmente os gregos se propuseram a definir as relações entre o conceito e a palavra que o designa.&lt;/p&gt;

&lt;p&gt;Sendo assim os gregos levaram o estudo da linguagem a outro nível. Eles questionaram coisas como: existe relação necessária entre a palavra e o seu significado? Podemos ver Platão discutindo esse ponto específico no Crátilo. Aristóteles desenvolveu estudos em outro foco, tentando proceder a uma análise precisa da estrutura linguística, chegando a elaborar uma teoria sobre distinguir as partes do discurso e a enumerar as categorias gramaticais.&lt;/p&gt;

&lt;p&gt;Entre estudiosos latinos, temos como destaque Varrão que, na esteira dos gregos, dedicou-se à gramática, em um esforço para defini-la como ciência e arte.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;No século XVI, a religiosidade ativada pela Reforma provoca a tradução dos livros sagrados em numerosas línguas, apesar de manter-se o prestígio do latim como língua universal. Viajantes, comerciantes e diplomatas trazem de suas experiências no estrangeiro o conhecimento de línguas até então desconhecidas. Em 1502 surge o mais antigo dicionário poliglota, do italiano Ambrosio Calepino. “Introdução à linguística Volumes 1 e 2, José Luiz Fiorin”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Em relação ao período moderno, podemos citar Franz Bopp como um dos principais criadores da gramática comparada. Sua obra publicada em 1816 se intitulava: &lt;code&gt;Über das Conjugationssystem der Sanskritsprache in Vergleichung mit jenem der griechischen, lateinischen, persischen und germanischen Sprache&lt;/code&gt; &lt;strong&gt;(Sobre o sistema de conjugação do sânscrito em comparação com o do grego, latim, persa e germânico)&lt;/strong&gt;. Esse trabalho evidenciou diversas semelhanças entre as línguas em questão.&lt;/p&gt;

&lt;div align=&quot;center&quot; style=&quot;width: 100%; font-size: 0.8em; font-family: courier;&quot;&gt;
&lt;img src=&quot;https://meriatblob.blob.core.windows.net/draft/indo-european-tree.jpeg&quot; style=&quot;margin-bottom: 0px !important;&quot; /&gt;
&lt;p&gt; ilustração: Minna Sundberg&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;Ao expor as semelhanças entre essas línguas, foi notório uma relação de parentesco que originou o que hoje chamamos de família &lt;code&gt;indo-européia&lt;/code&gt;, em que existe uma origem comum, comprovada pelo método histórico-comparativo.&lt;/p&gt;

&lt;p&gt;Somente no início do século XX a Linguística ganhou status de estudo científico. Como estudo ela sempre foi um anexo em estudos de lógica, filosofia, retórica, história ou crítica literária. O marco foi a divulgação dos trabalhos de Ferdinand de Saussure, professor da Universidade de Genebra. Em 1916, dois alunos de Saussure, a partir de anotações de aula, publicam o Curso de Lingüística geral, obra fundadora da nova ciência.&lt;/p&gt;

&lt;h1 id=&quot;linguagem-natural-e-sua-complexidade&quot;&gt;Linguagem Natural e sua Complexidade&lt;/h1&gt;

&lt;p&gt;A complexidade envolvida na linguagem natural passa por sua estruturação formal (gramática), a questões mais subjetivas como interpretação. Adicione a isso o fato que temos diversas linguagens no mundo, todas com estruturas e significâncias diferentes. Ainda temos toda a problemática envolvendo as questões de engenharia, como por exemplo, processar grandes quantidades de texto. Para ilustração, vamos a um exemplo simples:&lt;/p&gt;

&lt;div align=&quot;center&quot; style=&quot;width: 100%; font-size: 0.8em; font-family: courier;&quot;&gt;
&lt;img src=&quot;https://meriatblob.blob.core.windows.net/draft/linguistics_club.png&quot; style=&quot;margin-bottom: 0px !important;&quot; /&gt;
&lt;p&gt;ref: https://www.xkcd.com/1602/&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;No exemplo acima, temos uma anedota em torno da palavra &lt;strong&gt;sesquiannual&lt;/strong&gt;, que representa um período de 18 meses. O punch aqui é que somente uma pessoa que conhece o significa dessa palavra sabe quando o encontro vai acontecer. Este é um exemplo simples onde temos uma palavra que pertence domínio geral da língua, está nos principais dicionários porém não é de uso comum da população.&lt;/p&gt;

&lt;h2 id=&quot;compreensão-semântica&quot;&gt;Compreensão semântica&lt;/h2&gt;

&lt;p&gt;O estudo da semântica alude à parte da linguagem referente ao significado das palavras e expressões que a mesma pode gerar. Neste contexto, cada palavra possui uma semântica própria, que difere da sua classificação enquanto função sintática ou morfológica.&lt;/p&gt;

&lt;div style=&quot;margin-bottom: 2em; margin-top: 2em; background-color: #dcbc14; color: #382d2d&quot;&gt;
&lt;p style=&quot;padding: 1.6em; font-family: courier;&quot;&gt;
A comunicação humana está essencialmente ligada à capacidade de utilizar meios &lt;b&gt;semióticos&lt;/b&gt; (como a linguagem) para transmitir as &lt;b&gt;&quot;intenções comunicativas&quot;&lt;/b&gt; de um indivíduo e a capacidade de reconhecer tais intenções. DASCAL, Marcelo. Interpretação e compreensão. 2006
&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;Contudo, de maneira geral, a semântica não é tratada de forma isolada em cada palavra, mas sim generalizada a contextos mais amplos. Assim, em um diálogo, por exemplo, pode-se identificar um significado particular em cada frase e um significado mais geral pertinente ao assunto tratado pelas pessoas que promovem o diálogo. Da mesma forma, em um texto dissertativo, embora cada parágrafo possa expressar um sentido particular, é a união de todos os parágrafos que formarão o sentido do texto.&lt;/p&gt;

&lt;p&gt;Como pode ser notado, a função básica de uma linguagem, ou seja, a comunicação, está centrada no significado das expressões linguísticas produzidas. Isto torna o estudo da semântica essencial para implementações computacionais que envolvam compreensão e produção de linguagem. Uma barreira porém a este objetivo gira em torno da necessidade da utilização da intuição na compreensão do sentido de um texto, atos dificilmente aplicáveis a máquinas inspiradas na arquitetura de &lt;code&gt;Von Neumman&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Se utilizarmos um dos programas atuais de tradução de textos, vamos perceber ao final de todo o processamento que o resultado obtido apresenta em grande parte das vezes, inúmeras deficiências como falta de pronomes, erros de gênero e troca de palavras. Nos sistemas modernos, percebemos que essas falhas, mesmo não comprometendo a compreensão do contexto do documento traduzido, irá nos requerer uma revisão para a correta compreensão do assunto tratado.&lt;/p&gt;

&lt;p&gt;O que destaco aqui, é que o processamento e tratamento computacional da linguagem natural, colide com enormes barreiras, as quais são típicas da comunicação humana. Essas dificuldades são tratadas pelo cérebro de forma natural, embora já conhecemos os ríscos que estão associados a comunicação humana.&lt;/p&gt;

&lt;h1 id=&quot;natural-language-processing&quot;&gt;Natural Language Processing&lt;/h1&gt;

&lt;p&gt;Podemos conceituar &lt;code&gt;NLP&lt;/code&gt; da seguinte maneira:&lt;/p&gt;

&lt;div style=&quot;margin-bottom: 2em; margin-top: 2em; background-color: #dcbc14; color: #382d2d&quot;&gt;
&lt;p style=&quot;padding: 1.6em; font-family: courier;&quot;&gt;
&lt;b&gt;Natural Language Processing&lt;/b&gt; é a disciplina que consiste no desenvolvimento de modelos computacionais que utilizam informação expressa em uma determinada língua natural.
&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;Como objetivo, podemos definir que em &lt;code&gt;NLP&lt;/code&gt; queremos construir mecanismos artificiais que permitem o entedimento da linguagem natural para a realização de tarefas que visam simular um comportamento humano (e.g. tradução e interpretação de textos, busca de informações em documentos, detecção de tópicos).&lt;/p&gt;

&lt;p&gt;&lt;code&gt;NLP&lt;/code&gt; se encaixa no mundo da computação como uma subárea de Inteligência Artificial, e constantemente associada a Linguística Computacional, embora sejam matérias diferentes.&lt;/p&gt;

&lt;p&gt;Quando comparamos as aplicações desenvolvidas nessas duas áreas, é fácil entender a confusão. Geralmente compartilham as mesmas conferências, colaboram em artigos, mas tem como essência, objetivos diferentes.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;CL&lt;/strong&gt; – Computational Linguistics
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Profissional:&lt;/strong&gt; Linguistas&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Objetivo:&lt;/strong&gt; Uso da computação para o estudo da linguagem&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;NLP&lt;/strong&gt; – Natural Language Processing
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Profissional:&lt;/strong&gt; Cientistas da Computação&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Objetivo:&lt;/strong&gt; Aplicações envolvendo linguagem natural&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;computational-linguistics&quot;&gt;Computational Linguistics&lt;/h2&gt;

&lt;p&gt;Como matéria segue a linha das ciências naturais como à biologia computacional, e se propõem a desenvolver métodos computacionais a fim de responder às questões científicas sobre linguística.&lt;/p&gt;

&lt;p&gt;As questões centrais da linguística envolvem a natureza das representações linguísticas e do conhecimento linguístico, e como o conhecimento linguístico é adquirido e implantado na produção e compreensão da linguagem. Responder a essas perguntas descreve a capacidade da linguagem humana e pode ajudar a explicar a distribuição de dados e comportamentos linguísticos que realmente observamos.&lt;/p&gt;

&lt;p&gt;Na linguística computacional, são propostas respostas formais para essas questões. Os linguistas estão realmente perguntando o que e como os humanos estão associando. Por isso, definimos matematicamente classes de representações linguísticas e gramáticas formais (ou seja, modelos probabilísticos) que parecem adequadas para capturar a variedade de fenômenos presentes nas línguas humanas. São estudadas suas propriedades matemáticas a fim de guiar o desenvolvimento de algoritmos eficientes para o aprendizado, produção e compreensão da linguagem natural.&lt;/p&gt;

&lt;h2 id=&quot;nlp-uma-questão-de-engenharia&quot;&gt;NLP, uma questão de engenharia&lt;/h2&gt;

&lt;p&gt;Em comparação com o a linguística computacional, podemos perceber que enquando a &lt;code&gt;CL&lt;/code&gt; foca na descoberta de fatos linguísticos, a &lt;code&gt;NLP&lt;/code&gt; tem seu foco no desenvolvimento de tecnologias utilizando linguagem natural.&lt;/p&gt;

&lt;p&gt;Em muitos casos isso é visto como uma questão de ciência versus engenharia. Não se trata de provar que as línguas X ou Y estão relacionadas historicamente. Tem mais ligação com as questões práticas e como produzir ferramentas que vão ajudar as pessoas a se comunicarem melhor (seja com o computar ou entre si), bem como trabalhar e extrair conhecimento da quantidade incomensurável de informação em linguagem natural que temos hoje.&lt;/p&gt;

&lt;p&gt;Dado sua natureza prática, &lt;code&gt;NLP&lt;/code&gt; é muito relacionada a questões comerciais embora tenha um papel importante em outras ciências possibilitando análises que hoje são consideradas fundamentas em áreas como política, medicina e econômia.&lt;/p&gt;

&lt;h2 id=&quot;e-sobre-natrual-language-understanding&quot;&gt;E sobre Natrual Language Understanding?&lt;/h2&gt;

&lt;p&gt;Podemos enquadrar &lt;code&gt;NLU&lt;/code&gt; como uma subárea em &lt;code&gt;NLP&lt;/code&gt;. Para uma boa aplicação baseada em processamento de linguagem natural, um bom &lt;strong&gt;entendimento da linguagem&lt;/strong&gt; é simplesmente fundamental.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;garbage&quot;&gt;Garbage&lt;/h1&gt;

&lt;p&gt;Linguagem de computação e sua relação com o processamento de linguagem natural…&lt;/p&gt;

&lt;p&gt;The Turing Test: Language, computers, and artificial intelligence
Computers and language have gone together for decades. Natural language processing can be traced back to the 1950s, as many computer programmers began experimenting with simple language input to train computers to complete tasks. Then, in the 1960s, natural language understanding began developing out of a desire to get computers to understand more complex language input.&lt;/p&gt;

&lt;div align=&quot;center&quot; style=&quot;width: 100%; font-size: 0.8em; font-family: courier;&quot;&gt;
&lt;img src=&quot;https://meriatblob.blob.core.windows.net/draft/imitation-game.png&quot; style=&quot;margin-bottom: 0px !important;&quot; /&gt;
&lt;p&gt;ref: Computing Machinery and Intelligence, A. M. Turing&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;Gramática…
Gramática é o conjunto de regras que indicam o uso mais correto de uma língua.&lt;/p&gt;

&lt;p&gt;No início, a gramática tinha como função apenas estabelecer regras quanto à escrita e à leitura. É por isso que a palavra gramática, de origem grega (grámma), significa “letra”.&lt;/p&gt;

&lt;p&gt;Tipos de Gramática
Há 4 tipos de gramáticas: normativa, descritiva, histórica e comparativa. Ao mesmo tempo, a gramática da língua portuguesa é dividida em fonologia, morfologia e sintaxe. Nessa divisão, há gramáticos que incluem a semântica.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Normativa: A gramática normativa é sinônimo de norma culta. Ela estabelece os usos certos e errados em oposição ao uso popular. Isso porque, apesar de ser compreensível, no cotidiano, há sérias transgressões ao modelo estabelecido. Essa é a gramática oficial e, que portanto, é ensinada nas escolas.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Descritiva: A gramática descritiva analisa a língua com o objetivo de entender as suas alterações com o passar do tempo, especialmente em decorrência do seu uso oral.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Histórica: A gramática histórica trata justamente da história da língua, desde a sua origem às transformações.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Comparativa: A gramática comparativa estuda a gramática fazendo uma comparação com as gramáticas pertencentes às mesmas famílias linguísticas.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;O português pertence à família das línguas indo-europeias, em que se inclui as línguas itálicas. São exemplos as línguas espanhola e francesa.&lt;/p&gt;

&lt;p&gt;Trabalhar com NLP é um assunto como desafios complexos, como por exemplo, podemos pensar de forma intuítiva na criação de um sistema de perguntas simples e suas respostas. A forma ingenua seria construir um sistema baseado na busca de termos, palavras chave ou padrões de palavras. Essa é uma atividade relativamente fácil, principalmente pela capacidade computacional que temos hoje. Já se pensarmos no mesmo sistema construído para responder perguntas complexas, precisamos solucionar outros problemas com as inferências.&lt;/p&gt;

&lt;p&gt;Conforme, a pesquisa em Pln está voltada, essencialmente, a três aspectos da comunicação em língua natural:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;som: fonologia&lt;/li&gt;
  &lt;li&gt;estrutura: morfologia e sintaxe&lt;/li&gt;
  &lt;li&gt;signiﬁcado: semântica e pragmática&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A fonologia está relacionada ao reconhecimento dos sons que compõem as palavras de uma língua.&lt;/p&gt;

&lt;p&gt;A morfologia reconhece as palavras em termos das unidades primitivas que a compõem (e.g. caçou → caç+ou).&lt;/p&gt;

&lt;p&gt;A sintaxe deﬁne a estrutura de uma frase, com base na forma como as palavras se relacionam nessa frase (ﬁgura 1).&lt;/p&gt;

&lt;p&gt;A semântica associa signiﬁcado a uma estrutura sintática, em termos dos signiﬁcados das palavras que a compõem (e.g. à estrutura da ﬁgura 1, podemos associar o signiﬁcado “um animal perseguiu/capturou outro animal”). Finalmente, a pragmática veriﬁca se o signiﬁcado associado à uma estrutura sintática é realmente o signiﬁcado mais apropriado no contexto considerado (e.g. no contexto predador-presa, “perseguiu/capturou” → “comeu”).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://meriatblob.blob.core.windows.net/draft/nlp-001.png&quot; alt=&quot;0&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Como podemos ver a PNL é uma área da pesquisa muito vasta, que envolve diversas disciplinas do conhecimento humano. Neste artigo vou abordar apenas a análise sintática de algumas frases em português. O objetivo é definir uma gramática capaz de gerar um conjunto finito de sentenças e decidir se uma determinada sentença pertence ou não à linguagem definida nesta gramática. Em seguida, vamos estender essa gramática para tratar concordância de gênero e número, bem como tempos verbais. Com isso conseguimos montar uma árvore sintática de uma sentença de forma automática.&lt;/p&gt;

&lt;h1 id=&quot;gramática-e-análise-sintática&quot;&gt;Gramática e análise sintática&lt;/h1&gt;

&lt;p&gt;Uma gramática é uma especiﬁcação formal da estrutura das sentenças permitidas numa linguagem. O modo mais simples de deﬁnir uma gramática é especiﬁcando um conjunto de símbolos terminais, denotando palavras da linguagem, um conjunto de símbolos não-terminais, denotando os componentes das sentenças, e um conjunto de regras de produção, que expandem símbolos não-terminais numa sequência de símbolos terminais e não-terminais [3]. Além disso, a gramática deve ter um símbolo não-terminal inicial. Por exemplo, a gramática a seguir deﬁne um fragmento da língua portuguesa:&lt;/p&gt;

&lt;h4 id=&quot;gramática-1&quot;&gt;Gramática 1.&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;frase ⇒ sujeito predicado&lt;/li&gt;
  &lt;li&gt;sujeito ⇒ artigo substantivo&lt;/li&gt;
  &lt;li&gt;predicado ⇒ verbo artigo substantivo&lt;/li&gt;
  &lt;li&gt;artigo ⇒ o&lt;/li&gt;
  &lt;li&gt;substantivo ⇒ gato &amp;gt; rato&lt;/li&gt;
  &lt;li&gt;verbo ⇒ caçou&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Nessa gramática, os símbolos terminais são o, gato e rato, sendo os demais símbolos não-terminais. A regra de produção frase ⇒ sujeito predicado estabelece que uma frase é composta de um sujeito seguido de um predicado; enquanto a regra substantivo ⇒ gato &amp;gt; rato estabelece que um substantivo pode ser a palavra “gato” ou “rato”. Além disso, para essa gramática, o símbolo não-terminal inicial será frase. Nas gramáticas livres de contexto (do tipo que consideramos nesse artigo), o lado esquerdo de uma regra de produção será sempre um único símbolo não terminal, enquanto o lado direito pode conter símbolos terminais e não terminais. Como veremos a seguir, uma gramática pode ser usada tanto para reconhecimento, ou seja, para decidir se essa frase pertence à linguagem deﬁnida pela gramática; quanto para geração, ou seja, para construir uma frase pertencente à linguagem deﬁnida pela gramática.&lt;/p&gt;

&lt;hr /&gt;

&lt;blockquote&gt;
  &lt;p&gt;Se a comunicação é algo complexo mesmo para nós os seres humanos, como podemos trabalhar isso em computação?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Bom, o primeiro passo é transformar a nossa linguagem natural em algo que possa ser trabalhado por máquina. Fazemos isso transformando nosso dado textual em algum padrão de representação numérica, algo que seja processável e compreendido por uma máquina. Somente realizando esse passo é possível iniciar o processo de examinar os dados para criação de modelos matemáticos.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;conlusão&quot;&gt;Conlusão&lt;/h1&gt;

&lt;p&gt;Existe muita confusão quanso se fala em NLP. Temos diversas terminologias e conceitos que são errôneamente relacionados a essa materia. NLP é um campo onde uma certa complexidade está associada, então um correto entendimento dos conceitos é fundamental para conseguir atingir um nível avançado de trabalho.&lt;/p&gt;

&lt;p&gt;Fora isso a pesquisa e desenvolviemnto explorando o estado da arte em NLP requer um forte conhecimento em áreas como a linguística, uma vez diversos dos problemas que hoje queremos resolver, extrapolam a engenharia para algo mais conceitual.&lt;/p&gt;

&lt;h1 id=&quot;referências&quot;&gt;Referências&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Computational Linguistics and Natural Language Processing, Jun’ichi Tsujii, University of Tokyo&lt;/li&gt;
  &lt;li&gt;Introduction to Computational Linguistics, Jason Eisner, Johns Hopkins University&lt;/li&gt;
  &lt;li&gt;Interpretação e compreensão. Marcelo DASCAL&lt;/li&gt;
  &lt;li&gt;SILVA, M.C.S; KOCH, I.V. Linguística Aplicada ao Português: Morfologia. 18ª ed. – São Paulo: Cortez, 2012.&lt;/li&gt;
  &lt;li&gt;Diferenças entre língua, idioma e dialeto; PEREZ, Luana Castro Alves, Brasil Escola.&lt;/li&gt;
  &lt;li&gt;Introdução à linguística Volumes 1 e 2, José Luiz Fiorin&lt;/li&gt;
  &lt;li&gt;A Mathematical Theory of Communication, C. E. SHANNON, harvard&lt;/li&gt;
  &lt;li&gt;A Mind at Play: How Claude Shannon Invented the Information Age, Jimmy Soni and Rob Goodman&lt;/li&gt;
  &lt;li&gt;Computing Machinery and Intelligence, A. M. Turing, &lt;a href=&quot;https://www.csee.umbc.edu/courses/471/papers/turing.pdf&quot;&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Tue, 01 Oct 2019 00:00:00 -0300</pubDate>
        <link>http://localhost:4000/2019/10/01/nlp-under-the-hood/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/10/01/nlp-under-the-hood/</guid>
        
        <category>ai</category>
        
        <category>deep learning</category>
        
        <category>nlp</category>
        
        <category>data science</category>
        
        
        <category>IA</category>
        
        <category>Machine Learning</category>
        
        <category>Data Science</category>
        
        <category>NLP</category>
        
      </item>
    
      <item>
        <title>GPU for Deep Learning</title>
        <description>&lt;p align=&quot;center&quot; style=&quot;width: 100%;&quot;&gt;&lt;img src=&quot;http://meriatblob.blob.core.windows.net/images/2018/09/27/capa.png&quot; style=&quot;margin-bottom: 0px !important;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;introdução&quot;&gt;Introdução&lt;/h2&gt;

&lt;p&gt;Chegamos a um tópico importante dentro do campo de estudo da Deep Learning, a programação em &lt;strong&gt;GPU&lt;/strong&gt;. Minha intenção com este artigo é demonstrar qual a relação entre &lt;strong&gt;Deep Learning&lt;/strong&gt; e GPU, e a importância deste hardware especializado nas atividades de programação em redes neurais artificiais profundas.&lt;/p&gt;

&lt;p&gt;Todas as referências utilizadas neste artigo se seguem ao final do texto. Por hora, nos serve como introdução ao assunto o fato que Deep Learning é em sua essência, um apanhado de técnicas baseadas em multiplicações matriciais. Este é o ponto de insterseção entre Deep Learning e as técnicas de programação para GPU. O primeiro passo entretanto é olharmos para a evolução do hardware, o que torna visível sua importância dentro do tema.&lt;/p&gt;

&lt;h2 id=&quot;gpu-e-suas-origens&quot;&gt;GPU e suas origens&lt;/h2&gt;

&lt;p&gt;Em meados dos anos 90, já havia uma vasta utilização dos computadores, mas pouco se conhecia sobre processamento gráfico. Já havia a utilização do &lt;strong&gt;VGA (Video Graphics Array)&lt;/strong&gt;, componente usado como um “vetor de imagens gráficas”, uma área contínua de dados que podem ser lidos e representados em uma tela. Dentro desta arquitetura, a responsabilidade de processar essa memória vinha inteiramente da &lt;strong&gt;CPU&lt;/strong&gt;. Esse processamento devia acontecer várias vezes por segundo, exatamente como fazemos para simularmos a noção de movimento.&lt;/p&gt;

&lt;div style=&quot;margin-bottom: 2em; margin-top: 2em; background-color: #dcbc14; color: #382d2d&quot;&gt;
&lt;p style=&quot;padding: 1.6em; font-family: courier;&quot;&gt;
A CPU (Central Processing Unit) é a unidade de processamento central do computador.
&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;Estamos falando de uma época onde computadores ainda não eram populares. Com isso a utilização das aplicações gráficas ainda não eram um problema. A questão é que a medida em que os computadores foram se tornando populares, as aplicações que antes só trabalhavam com gráficos simples, bidimensionais e de baixa resolução baixa, partiram para aplicações mais elaboradas como jogos, aplicações para manipulação de imagens, &lt;strong&gt;CAD (Computer-Aided Design)&lt;/strong&gt; e outras tantas.&lt;/p&gt;

&lt;p&gt;Com essa crescente evolução, a demanda por mais poder de processamento fez com que os engenheiros de hardware, desenvolvessem um novo processamento para abarcar algumas das atividades que antes ocorriam na CPU, o que dava início as primeiras placas gráficas.&lt;/p&gt;

&lt;p&gt;O que se percebeu é que algumas das atividades do processamento gráfico são extremamente intensivas, como o processo de rasterização, que converte vetores e elementos geométricos em pixels. Este é literalmente o primeiro processamento gráfico que foi implementado nestes novos hardwares.&lt;/p&gt;

&lt;p&gt;Como segundo passo, foi realizado a implementação dos &lt;strong&gt;shaders&lt;/strong&gt;, responsáveis pela renderização de sombras. As sombras são necessárias para que o usuário possa ter a impressão de profundidade. Aqui temos um ponto muito importante nesta história. Diferente da técnica de raterização, a técnica de shader possui uma grande diversidade de algoritmos, dado a diversidade de utilizações. Isso tornava a programação em hardware muito inviável, já que abarcar todas as implementações seria muito “pesado”. A solução para este problema foi a criação de shaders configuráveis, o que tornava estes novos hardwares configuráveis. Um exemplo disso é o famoso &lt;strong&gt;OpenGL&lt;/strong&gt;, que foi a mais famosa biblioteca para programação de shaders.&lt;/p&gt;

&lt;p&gt;Agora havia um hardware configurável, e se se ele era configurável, implica dizer que havia um conjunto de instruções. Se eu posso realizar um conjunto de instruções em um hardware especializado, por que cargas d’água não incluir comandos mais complexos? E se este novo processador também realizase algumas outras atividades para aliviar a utilização da CPU?&lt;/p&gt;

&lt;p&gt;Essas foram algumas das perguntas e necessidade que nos levaram a criação da GPU, do inglês &lt;strong&gt;Graphics Processing Units&lt;/strong&gt;, ou unidades de processamento gráﬁcos.&lt;/p&gt;

&lt;p&gt;Com este novo hardware, os engenheiros foram migrando aos poucos as atividades específicas do processamento gráfico da CPU para a GPU.&lt;/p&gt;

&lt;h2 id=&quot;under-the-hood&quot;&gt;Under the hood&lt;/h2&gt;

&lt;p&gt;Inicialmente a GPU era um processador bem mais simples do que as GPUs atuais. Sua arquitetura era construída com foco no processamento gráfico. Já as GPUs modernas tem focado no aspecto programável, tornando o hardware em um processador com grande capacidade aritmética e de uso geral, não mais restrita as operações gráficas.&lt;/p&gt;

&lt;p&gt;Atualmente, as GPUs são processadores dedicados para processamento gráfico da classe &lt;strong&gt;SIMD (Single Instruction Multiple Data)&lt;/strong&gt;. GPUs são desenvolvidas especificamente para cálculos de ponto flutuante, essenciais para os trabalhos de computação gráfica. Suas principais características são sua alta capacidade de processamento massivo paralelo e sua total programabilidade e desempenho em cálculos que exigem um volume grande de dados, resultando em um grande &lt;strong&gt;throughput&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Arquiteturalmente a GPU possui um número muito superior de unidades de processamento comparado a CPU. Observe a figura abaixo:&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://meriatblob.blob.core.windows.net/images/2018/09/27/01.png&quot; style=&quot;width: 100%; margin-bottom: 0px !important;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;O primeiro ponto a ser observado é que as unidades de processamento de uma GPU são mais simples e mais lentas que as unidades de processamento de uma CPU. Em relação ao barramento de comunicação, o usual é termos uma desvantagem na utilização da GPU.&lt;/p&gt;

&lt;div style=&quot;margin-bottom: 2em; margin-top: 2em; background-color: #dcbc14; color: #382d2d&quot;&gt;
&lt;p style=&quot;padding: 1.6em; font-family: courier;&quot;&gt;
De fato, vamos ver nos testes que é necessário calcular se nossa empreitada é algo que vale ser processado na &lt;b&gt;GPU&lt;/b&gt;. Descobri que às vezes a sobrecarga de transferência de dados para e da &lt;b&gt;GPU&lt;/b&gt; elimina completamente o ganho de velocidade do paralelismo que temos na &lt;b&gt;GPU&lt;/b&gt;. Nem sempre é uma boa ideia ir para a &lt;b&gt;GPU&lt;/b&gt;. 
&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;A maioria das CPUs multicore atuais usam o paradigma de memória compartilhada para comunicação com sincronização através de cache compartilhada. Cada núcleo tem uma thread de processamento por vez, com um conjunto de registradores contendo o estado da thread, uma &lt;strong&gt;ULA (Unidade Logica Aritmética)&lt;/strong&gt; dedicada para a thread atual e uma unidade grande dedicada ao gerenciamento e escalonamento de tarefas.&lt;/p&gt;

&lt;p&gt;Enquanto as CPUs dedicam uma grande quantidade de seus circuitos para o uso geral e de controle, a GPU foca mais nas ULAs ou &lt;strong&gt;ALUs (Arithmetic Logical Units)&lt;/strong&gt;. Isso torna a GPU mais eficiente em termos de custo quando executam um software paralelo. Consequentemente, a GPU é construída para aplicações com demandas diferentes da CPU: cálculos paralelos grandes com mais ênfase no &lt;strong&gt;throughput que na latência&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Dado essa arquitetura, aplicações muito iterativas, tais como editores de texto ou players de áudio, não se beneﬁciariam do poder de processamento das GPUs. Por outro lado, aplicações em que exista muito paralelismo de dados tem um ganho nítido.&lt;/p&gt;

&lt;p&gt;Como vimos, a GPU, em seu core implementa um modelo computacional SIMD, que podemos traduzir como “instrução única, dados múltiplos”. Neste modelo temos vários processadores, mais somente uma unidade de processamento. Isso implica em dizer que uma máquina SIMD faz muitas coisas em paralelo, mas sempre as mesmas coisas.&lt;/p&gt;

&lt;h2 id=&quot;gpu--deep-learning-uma-questão-de-desempenho&quot;&gt;GPU &amp;amp; Deep Learning. Uma questão de desempenho&lt;/h2&gt;

&lt;p&gt;Já vimos sobre o poder de paralelismo presente na GPU. A grande questão é que o ganho de desempenho não é necessariamente apenas fruto do paralelismo.&lt;/p&gt;

&lt;p&gt;Como vimos, a CPU é otimizada para latência enquanto a GPU é otimizada para largura de banda.&lt;/p&gt;

&lt;p&gt;A CPU é muito rápida porém lida com uma quantidade pequena de informação. Já a GPU é mais lenta porém lida com uma enorme quantidade de informação.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;http://meriatblob.blob.core.windows.net/images/2018/09/27/cpu-v-gpu-cap.png&quot; style=&quot;width: 100%; margin-bottom: 0px !important;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Para ambos os hardwares, existe a necessidade do tráfego de pacotes de dados. A CPU pode buscar alguns pacotes da memória RAM de forma muito veloz, enquanto a GPU para a mesma tarefa vai enfrentar uma latência maior. O ponto aqui é que a CPU precisa ir muitas vezes na memória para buscar as informações enquanto a GPU pode trabalhar uma quantidade muito superior.&lt;/p&gt;

&lt;p&gt;Em outras palavras, a CPU é boa em buscar pequenas quantidades de memória de forma extremamente rápida, enquanto a GPU consegue buscar grandes quantidades de memória por vez.&lt;/p&gt;

&lt;div style=&quot;margin-bottom: 2em; margin-top: 2em; background-color: #dcbc14; color: #382d2d&quot;&gt;
&lt;p style=&quot;padding: 1.6em; font-family: courier;&quot;&gt;
Hoje temos boas &lt;b&gt;CPUs&lt;/b&gt; com cerca de &lt;b&gt;50GB/s&lt;/b&gt; para a largura de banda de memória, enquanto boas &lt;b&gt;GPUs&lt;/b&gt; trabalham com &lt;b&gt;750GB/s&lt;/b&gt;.
&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;Quando falamos de desempenho, quanto mais memória suas operações computacionais exigirem, maior será a vantagem de se utilizar a GPU em detrimento da CPU.&lt;/p&gt;

&lt;p&gt;A GPU é um magnífico exemplo de hardware paralelo. Parafraseando o Manual de Boas Práticas da Nvidia: &lt;strong&gt;“As GPUs da Nvidia suportam até 768 threads ativas por multiprocessador; algumas GPUs elevando este número a 1.024. Em dispositivos que possuem 30 multiprocessadores, tais como a GeForce GTX 280, isto faz com que 30,000 threads possam estar ativas simultaneamente”&lt;/strong&gt;. Este hardware poderosíssimo tem permitido que algumas aplicações pudessem executar até 100 vezes mais rapidamente que suas rivais restritas à CPU.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://meriatblob.blob.core.windows.net/images/2018/09/27/geforce-gtx-670.png&quot; style=&quot;width: 100%; margin-bottom: 0px !important;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A GPU separa uma porção muito grande de sua área útil para tarefas de processamento, ao passo que a CPU usa bastante desta área para implementar a sua memória de cache.&lt;/p&gt;

&lt;p&gt;Para uma comparação mais realista, a &lt;strong&gt;NVIDIA GTX 670&lt;/strong&gt; - uma placa gráfica de uso geral - possui 1.344 núcleos CUDA trabalhando em 980 MHz, enquanto um processador Intel Sandybridge i7-2600 com 4 núcleos trabalha em 3,4 GHz.&lt;/p&gt;

&lt;p&gt;Ainda em relação a questão de desempenho, no geral a GPU é instalada no barramento PCIe, que conhecidamente possui uma comunicação mais lenta se comparada com a comunicação da CPU e a memória do sistema.&lt;/p&gt;

&lt;div style=&quot;margin-bottom: 2em; margin-top: 2em; background-color: #dcbc14; color: #382d2d&quot;&gt;
&lt;p style=&quot;padding: 1.6em; font-family: courier;&quot;&gt;
Este é outro ponto a se considerar, temos vantagens em usar a GPU somente quando a quantidade de cálculos a serem feitos, somado ao tempo de transferência do sistema-GPU se torna insignificante em relação ao tempo de cálculo em si.
&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;Não existe uma proporção indicada. Neste momento estou me aprofundando no tema, revisando códigos e estudos que realizei tenando comparar o desempenho de determinadas implementações na CPU e na GPU. Particularmente, minha  tendência foi programar tudo o possível no &lt;strong&gt;Tensorflow&lt;/strong&gt; para GPU, o que hoje, aprendi ser um erro.&lt;/p&gt;

&lt;h2 id=&quot;frameworks-para-gpu&quot;&gt;Frameworks para GPU&lt;/h2&gt;

&lt;p&gt;Em relação a programação para GPU, podemos elencar 2 frameworks como principais:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;OpenCL (solução open-source, uma API de baixo nível para programação paralela em diferentes tipos de processadores incluindo GPU, CPU e FPGA’s)&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;CUDA (Compute Unified Device Architecture)&lt;/strong&gt; da NVIDIA, que só pode ser usado em soluções da NVIDIA.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A principal diferença entre o &lt;strong&gt;CUDA&lt;/strong&gt; e o &lt;strong&gt;OpenCL&lt;/strong&gt; é que o CUDA é uma estrutura proprietária criada pela Nvidia e OpenCL é open source. Cada uma dessas abordagens traz suas próprias vantagens e desvantagens.&lt;/p&gt;

&lt;p&gt;O consenso geral é que, se houver a possibilidade, prefira utilizar o CUDA, uma vez que os seus resultados em relação a desempenho hoje são considerados melhores. A razão deste ganho, é que a &lt;strong&gt;NVIDIA&lt;/strong&gt;, proprietária do CUDA, fornece um grande suporte sem falar em todas as pesquisas e atualizações mantidas pela empresa.&lt;/p&gt;

&lt;p&gt;Não estou elencado aqui &lt;strong&gt;frameworks&lt;/strong&gt; como &lt;strong&gt;Tensorflow&lt;/strong&gt;, &lt;strong&gt;Keras&lt;/strong&gt;, &lt;strong&gt;CNTK&lt;/strong&gt; e afins, uma vez que estes são frameworks criados com foco em &lt;strong&gt;deep learning&lt;/strong&gt;, onde apenas existe suporte a utilização da GPU. Eles não são focados na programação efetiva da GPU.&lt;/p&gt;

&lt;h2 id=&quot;the-future&quot;&gt;The Future&lt;/h2&gt;

&lt;p&gt;Neste texto introdutório estive abordando especificamente a ligação e importância do processamento de GPU para as técnicas de Deep Learning. Porém este é apenas o início da problemática. Da criação das primeiras GPUs até os dias atuais, diversar novas arquiteturas e outros componentes físicos foram entrando na jogada.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://meriatblob.blob.core.windows.net/images/2018/09/27/the_future.png&quot; style=&quot;width: 100%; margin-bottom: 0px !important;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Com a evolução constante da GPU, diversas novas técnicas estão sendo apresentadas, por meio de pesquisas e frentes levantadas por grandes empresas como o caso da Microsoft que está liderando as pesquisas em relação a programação com &lt;strong&gt;FPGA (Field Programmable Gate Array)&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Empresas como a Intel tem prometido melhorar o desempenho da própria CPU, enquanto um crescente número de pesquisas tem apontado a possibilidade da utilização de &lt;strong&gt;ASICs (application-specific integrated circuit)&lt;/strong&gt;, componentes específicos para uma determinada ação, estão sendo considerados mais eficientes para processos como &lt;strong&gt;Convoluções matemáticas&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;O importante é notar que além da evolução nos algorítmos e tecnicas, temos ainda um grande caminho a percorrer em relação a evolução do hardware.&lt;/p&gt;

&lt;h2 id=&quot;testes&quot;&gt;Testes&lt;/h2&gt;

&lt;p&gt;Este é um breve resumo dos testes realizados, toda a demonstração está no vídeo abaixo. Aqui vou apenas fazer um breve resumo e incluir alguns resultados que consegui. Todo o código fonte está no meu &lt;strong&gt;github&lt;/strong&gt;, e pode ser baixado no seguinte link: &lt;a href=&quot;https://github.com/vitormeriat/presentations/tree/master/06-gpu-for-deeplearning&quot;&gt;github.com/vitormeriat/presentations/gpu-for-deeplearning&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;A primeira parte do nosso teste é verificar se temos uma GPU disponível. Fazemos isso rodando alguns comandos do &lt;strong&gt;Linux&lt;/strong&gt; e outros do próprio &lt;strong&gt;Tensorflow&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;O código que eu utilizo é basicamente criar uma matriz de &lt;strong&gt;N&lt;/strong&gt; por &lt;strong&gt;N&lt;/strong&gt;, onde &lt;strong&gt;N&lt;/strong&gt; é um número que vamos aumentando gradativamente para verificar a capacidade dos devices (&lt;strong&gt;GPU, CPU&lt;/strong&gt;), para realizar esses cálculos.&lt;/p&gt;

&lt;p&gt;Para verificar a importância dos &lt;strong&gt;frameworks&lt;/strong&gt; especializados em &lt;strong&gt;Deep Learning&lt;/strong&gt;, fiz o primeiro teste rodando sobre &lt;strong&gt;Numpy&lt;/strong&gt;, depois as mesmas operações rodando sobre &lt;strong&gt;Tensorflow&lt;/strong&gt;, tanto na CPU quanto na GPU. No caso do Tensorflow, isso é determinado pela variável &lt;strong&gt;device_name&lt;/strong&gt;. O Numpy, como descrito no vídeo, não oferece suporte a GPU.&lt;/p&gt;

&lt;h5 id=&quot;numpy&quot;&gt;Numpy&lt;/h5&gt;
&lt;pre style=&quot;font-size: 1.4em !important&quot;&gt;
    &lt;code class=&quot;python&quot;&gt;
  A=np.random.normal(size=(shapeMtx, shapeMtx))
  B=np.random.normal(size=(shapeMtx, shapeMtx))

  subtract = np.subtract(A, B)
  add = np.divide(A, B)
  multiply = np.multiply(A, B)
    &lt;/code&gt;
&lt;/pre&gt;

&lt;h5 id=&quot;tensorflow&quot;&gt;Tensorflow&lt;/h5&gt;
&lt;pre style=&quot;font-size: 1.4em !important&quot;&gt;
    &lt;code class=&quot;python&quot;&gt;
  with tf.device(device_name):
    A = tf.random_uniform(shape=shape, minval=0, maxval=1)
    B = tf.random_uniform(shape=shape, minval=0, maxval=1)
    
    subtract_operation = tf.subtract(A, B)
    divide_operation = tf.divide(A, B)
    multiply_operation = tf.multiply(A, B)

  startTime = datetime.now()
  with tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)) as session:
    session.run(subtract_operation)
    session.run(divide_operation)
    session.run(multiply_operation)
    &lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;Como o exemplificado no vídeo, os tempos se alteram a medida em que aumentamos as dimensões de nossa matriz. Vemos que a CPU tem vantagem quando os cáculos são menores, enquando a GPU vai ganhando a medida em que os cálculos vão se tornando maiores.&lt;/p&gt;

&lt;p&gt;Em relação a performance, podemos observar no segundo teste, que a GPU possui um aumento muito menor de tempo para realizar determinados cálculos se comparado a CPU.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://meriatblob.blob.core.windows.net/images/2018/09/27/graphic_test_performance.png&quot; style=&quot;width: 100%; margin-bottom: 0px !important;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Neste teste especificamente, o tempo de execução na GPU tem um aumento tão pequeno em relação ao da CPU, que aparentemente temos a impressão que para o gráfico que estamos analisando apenas os tempos da CPU sofreram alteração. Como pode ser visto abaixo, ao imprimir os tempos, é possível realizar uma validação mais precisa.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Output
...

GPU times [0.13785886764526367, 0.007532835006713867, 0.00854039192199707, 0.009807109832763672, 0.010930776596069336, 0.011966943740844727, 0.012804269790649414, 0.014283418655395508, 0.01694202423095703, 0.01774454116821289, 0.019214153289794922, 0.019917011260986328, 0.0216977596282959, 0.022628068923950195, 0.025137662887573242, 0.02427196502685547, 0.028975486755371094, 0.029575824737548828, 0.028879880905151367, 0.03321361541748047, 0.032982587814331055, 0.03345513343811035, 0.03653764724731445, 0.035642385482788086, 0.03930830955505371, 0.04390144348144531, 0.045243024826049805, 0.0432429313659668, 0.04667401313781738, 0.051265716552734375, 0.05484771728515625, 0.05348992347717285, 0.05866360664367676, 0.05812430381774902, 0.05793881416320801, 0.0610194206237793, 0.06328725814819336, 0.06955838203430176, 0.06947898864746094, 0.07118368148803711]

CPU times [0.28919482231140137, 0.36597657203674316, 0.4756290912628174, 0.6124699115753174, 0.7608673572540283, 0.9239287376403809, 1.092865228652954, 1.3050789833068848, 1.5583577156066895, 1.8389079570770264, 2.1097397804260254, 2.445690393447876, 2.819725275039673, 3.2182812690734863, 3.6523375511169434, 4.114335775375366, 4.665944337844849, 5.188570737838745, 5.789913654327393, 6.40700364112854, 7.094855308532715, 7.836256980895996, 8.639605522155762, 9.486554145812988, 10.296368837356567, 11.298496007919312, 12.23202109336853, 13.28882646560669, 14.383679151535034, 15.580460548400879, 16.79129123687744, 18.077450037002563, 19.419445753097534, 20.82856273651123, 22.34893012046814, 23.885477781295776, 25.47702121734619, 27.175251007080078, 28.97485899925232, 30.87795114517212] 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Abaixo segue o vídeo da demonstração completa…&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;iframe height=&quot;506&quot; src=&quot;https://www.youtube.com/embed/0qyZkh9OuaY?rel=0&quot; width=&quot;100%&quot; allowfullscreen=&quot;&quot; style=&quot;border: 0px;&quot;&gt;&lt;/iframe&gt;&lt;/p&gt;

&lt;h2 id=&quot;conlusão&quot;&gt;Conlusão&lt;/h2&gt;

&lt;p&gt;Minha ideia neste texto foi trazer uma introdução a questão da grande adoção e utilização de GPUs no processo de Deep Learning.&lt;/p&gt;

&lt;p&gt;Este é um assunto extenso, existem diversas novas arquiteturas computacionais visando diminuir os contras do processamento e GPU. Diversas pesquisas tem mostrado que ainda existe muito espaço para evolução deste tema.&lt;/p&gt;

&lt;p&gt;Sendo assim em um segundo texto, devo ir mais especificamente nas questões de arquitetura, focando mais na questão do &lt;strong&gt;HPC (High Performance Computing)&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 id=&quot;referências&quot;&gt;Referências&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Vários autores. NVIDIA CUDA Compute Unfied Device Architecture – Programming Guide. NVIDIA, 1.0 edition, 2007.&lt;/li&gt;
  &lt;li&gt;Daniel Cederman and Philippas Tsigas. GPU-quicksort: A practical quicksort algorithm for graphics processors. Jour- nal of Experimental Algorithmics, 14(1):4–24, 2009.&lt;/li&gt;
  &lt;li&gt;Victor W Lee, Changkyu Kim, Jatin Chhugani, Michael Deisher, Daehyun Kim, Anthony D Nguyen, Nadathur Sa- tish, Mikhail Smelyanskiy, Srinivas Chennupaty, Per Ham- marlund, Ronak Singhal, and Pradeep Dubey. Debunking the 100X GPU vs. CPU myth: an evaluation of throughput computing on CPU and GPU. In ISCA, pages 451–460. ACM, 2010.&lt;/li&gt;
  &lt;li&gt;Fernando Pereira. Técnicas de Otimização de Código para Placas de Processamento Gráfico.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.nvidia.com/page/geforce256.html&quot;&gt;NVIDIA Corporation. GeForce 256- The world’s first GPU&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Thu, 27 Sep 2018 00:00:00 -0300</pubDate>
        <link>http://localhost:4000/2018/09/27/gpu-for-deeplearning/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/09/27/gpu-for-deeplearning/</guid>
        
        <category>ai</category>
        
        <category>deep learning</category>
        
        <category>hpc</category>
        
        <category>gpu</category>
        
        
        <category>AI</category>
        
        <category>Deep Learning</category>
        
      </item>
    
      <item>
        <title>Provision the Jupyter Notebook in Azure VM for Linux</title>
        <description>&lt;p align=&quot;center&quot; style=&quot;background-color: #f2f2f2; width: 100%;&quot;&gt;&lt;img src=&quot;http://meriatblob.blob.core.windows.net/images/2017/11/20/capa-jupyter-azure.png&quot; style=&quot;margin-bottom: 0px !important;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Neste post vamos passo a passo criar uma máquina virtual Linux no Azure, instalar o Jupyter Notebook via Anaconda. Também vamos conectar em nossa VM via um túnel SSH para conseguir utilinar nossos Notebooks como se estevessem rodando local.&lt;/p&gt;

&lt;h2 id=&quot;justificativa&quot;&gt;Justificativa&lt;/h2&gt;

&lt;p&gt;Para facilatar deixe-me explicar o motivo deste texto: Estes são os passos que executei em algumas demonstrações que fiz abordando o quanto a &lt;code&gt;Cloud Computing&lt;/code&gt; é importante para o avanço da inteligência artificial. Estamos falando de uma área da computação que é estudada desde a década de 40.&lt;/p&gt;

&lt;p&gt;Os grande avanços nessa área se dão basicamente por que hoje (em grande parte graças a Cloud Computing), temos um grande &lt;code&gt;poder computacional&lt;/code&gt; bem como &lt;code&gt;poder de armazenamento&lt;/code&gt; de dados, isso de forma acessível. Já preparei um vídeo apenas sobre este assunto que vai estar disponível em breve no meu canal.&lt;/p&gt;

&lt;p&gt;Sendo assim nada mais justo do que usar todo este poder computacional ao nosso favor, utilizando os grande players de mercado para provisionar em minutos a infra necessária para treinar nossos modelos de manira rápida. Assim conseguimos nos concetrar em atividades mais importantes.&lt;/p&gt;

&lt;h2 id=&quot;demonstração-da-instalação&quot;&gt;Demonstração da instalação&lt;/h2&gt;

&lt;p align=&quot;center&quot;&gt;&lt;iframe height=&quot;506&quot; src=&quot;https://www.youtube.com/embed/5aWCxfHYhlg?rel=0&quot; width=&quot;900&quot; allowfullscreen=&quot;&quot; style=&quot;border: 0px;&quot;&gt;&lt;/iframe&gt;&lt;/p&gt;

&lt;h2 id=&quot;requisitos&quot;&gt;Requisitos&lt;/h2&gt;

&lt;p&gt;Para finalizar este tutorial, você vai precisar de uma conta ativa no Microsoft Azure, e caso sua máquina seja Windows, de acesso ao &lt;code&gt;bash&lt;/code&gt; ou utilizar o &lt;code&gt;cmd&lt;/code&gt; com ajuda do &lt;code&gt;putty&lt;/code&gt;. Abaixo segue as referências.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://azure.microsoft.com/pt-br/free/&quot;&gt;Como criar uma conta gratuita no Microsoft Azure&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.techtudo.com.br/dicas-e-tutoriais/noticia/2016/04/como-instalar-e-usar-o-shell-bash-do-linux-no-windows-10.html&quot;&gt;Como habilitar o bash no Windows 10&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://technet.microsoft.com/pt-br/library/hh225041(v=sc.12).aspx&quot;&gt;Como conectar via SSH no Windows utilizando o Putty&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;hands-on&quot;&gt;Hands on&lt;/h2&gt;

&lt;p&gt;Acesse o portal do Microsoft Azure: &lt;a href=&quot;https://portal.azure.com/&quot;&gt;portal.azure.com&lt;/a&gt;. Após o login execute os seguintes passo como o exemplificado no vídeo:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Clique em &lt;code&gt;+ New&lt;/code&gt; no menu a esquerda;&lt;/li&gt;
  &lt;li&gt;Em &lt;code&gt;Search the Market Place&lt;/code&gt; pesquise por &lt;strong&gt;Ubuntu Server 17.10&lt;/strong&gt;;&lt;/li&gt;
  &lt;li&gt;Clique em &lt;code&gt;Create&lt;/code&gt;;&lt;/li&gt;
  &lt;li&gt;Informe o nome da sua VM. Insira um Username e um Password. Crie um Resource Group e clique em &lt;strong&gt;OK&lt;/strong&gt;;&lt;/li&gt;
  &lt;li&gt;Escolha o Size da sua máquina virtual e clique em &lt;strong&gt;OK&lt;/strong&gt;;&lt;/li&gt;
  &lt;li&gt;No próximo passo acesso o &lt;code&gt;Network security group&lt;/code&gt; e adicione uma nova &lt;code&gt;inbound rule&lt;/code&gt; para a porta &lt;strong&gt;8888&lt;/strong&gt;, que é onde vamos acessar nosso &lt;strong&gt;Notebook&lt;/strong&gt;;&lt;/li&gt;
  &lt;li&gt;Em &lt;code&gt;Public ip address&lt;/code&gt; selecione &lt;strong&gt;um ip estático&lt;/strong&gt; e prossiga para a confirmação;&lt;/li&gt;
  &lt;li&gt;Finalize a instalação, aguarde o provisionamento e na tela que se seguirá, copie o &lt;code&gt;IP&lt;/code&gt; que será criado para sua nova VM.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Agora que temos nossa VM criada no Azure, vamos executar os passos para a instalação do Jupyter Notebook.&lt;/p&gt;

&lt;p&gt;O primeiro passo será acessar o servidor recém criado via SSH. Informe o usuário e o ip conforme o exemplo abaixo:&lt;/p&gt;

&lt;pre style=&quot;font-size: 1.6em !important&quot;&gt;
    &lt;code class=&quot;bash&quot;&gt;
$ ssh {username}@{server_address}
    &lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;Assim que você realizar o acesso, informe sua senha e pronto. Você já vai estar conectado em sua VM. Agora vamos baixar o &lt;code&gt;Anaconda installer bash script&lt;/code&gt;. Neste ponto acesso o site abaixo e verifique qual a última versão do &lt;strong&gt;Anaconda&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;No momento deste post, a versão da última distribuição do Anaconda é a &lt;code&gt;5.0.1 For Linux&lt;/code&gt;. Todos os exemplos vão utilizar essa versão, então lembre-se de trocar caso necessário.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.anaconda.com/download/#linux&quot;&gt;Anaconda&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Por questões de boas práticas, vamos acessar nossa pasta &lt;code&gt;tmp&lt;/code&gt; para realizar o donwload do script acima.&lt;/p&gt;

&lt;pre style=&quot;font-size: 1.6em !important&quot;&gt;
    &lt;code class=&quot;bash&quot;&gt;
$ cd /tmp
    &lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;Agora vamos utilizar o comando &lt;code&gt;curl&lt;/code&gt; para fazer o download do arquivo.&lt;/p&gt;

&lt;pre style=&quot;font-size: 1.6em !important&quot;&gt;
    &lt;code class=&quot;bash&quot;&gt;
$ curl -O https://repo.continuum.io/archive/Anaconda3-5.0.1-Linux-x86_64.sh
    &lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;Outra boa prática é validar a integridade do download antes da instalação. Neste caso específico vamos executar o script abaixo para fazer o double-check junto ao site do &lt;strong&gt;Anaconda&lt;/strong&gt;.&lt;/p&gt;

&lt;pre style=&quot;font-size: 1.6em !important&quot;&gt;
    &lt;code class=&quot;bash&quot;&gt;
$ sha256sum Anaconda3-5.0.1-Linux-x86_64.sh
    &lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;Acesse &lt;a href=&quot;https://docs.anaconda.com/anaconda/install/hashes/Anaconda3-5.0.1-Linux-x86_64.sh-hash&quot;&gt;esse link&lt;/a&gt; para verificar se os &lt;code&gt;hashes&lt;/code&gt; correspondem.&lt;/p&gt;

&lt;p&gt;Com as informações validadas, execute o comando abaixo para inicializar a instalação.&lt;/p&gt;

&lt;pre style=&quot;font-size: 1.6em !important&quot;&gt;
    &lt;code class=&quot;bash&quot;&gt;
$ bash Anaconda3-5.0.1-Linux-x86_64.sh
    &lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;Neste ponto você vai receber algumas mensagens, para confirmar a instalação, para confirmar que leu e concorda com os termos da instalação e posteriormente para confirmar qual o caminho onde a instalação será realizada.&lt;/p&gt;

&lt;p&gt;Confirme todos os passos e por último, após a instalação ser realizada com sucesso você vai receber a mensagem abaixo:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Output
...
installation finished.
Do you wish the installer to prepend the Anaconda3     install location
to PATH in your /home/vitormeriat/.bashrc ? [yes|no]
[no] &amp;gt;&amp;gt;&amp;gt; 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Esta é uma confirmação necessária para utilizarmos o &lt;code&gt;conda command utility&lt;/code&gt;, que devo explicar melhor em um próximo vídeo. Inform &lt;strong&gt;yes&lt;/strong&gt; e finalize o procedimento.&lt;/p&gt;

&lt;p&gt;Agora é só executar o comando abaixo para ativar a instalação e vamos estar prontos para testar nosso &lt;strong&gt;Jupyter Notebook&lt;/strong&gt;.&lt;/p&gt;

&lt;pre style=&quot;font-size: 1.6em !important&quot;&gt;
    &lt;code class=&quot;bash&quot;&gt;
$ source ~/.bashrc
    &lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;Para verficar se a instalação ocorreu como o esperado, vamos listar todos os pacotes instalados pelo Conda utilizando o comando abaixo:&lt;/p&gt;

&lt;pre style=&quot;font-size: 1.6em !important&quot;&gt;
    &lt;code class=&quot;bash&quot;&gt;
$ conda list
    &lt;/code&gt;
&lt;/pre&gt;

&lt;h2 id=&quot;acessando-a-vm-via-ssh-tunneling&quot;&gt;Acessando a VM via SSH tunneling&lt;/h2&gt;

&lt;p&gt;Use o comando abaixo para especificar qual porta será utilizada para rodar o Jupyter Notebook na máquina loca, qual a porta onde o Jupyter Notebook está rodando no servidor e as informações base do acesso via &lt;code&gt;SSH&lt;/code&gt;.&lt;/p&gt;

&lt;pre style=&quot;font-size: 1.6em !important&quot;&gt;
    &lt;code class=&quot;bash&quot;&gt;
$ ssh -L 8080:localhost:8888 {username}@{server_address}
    &lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;Você vai precisar informar sua senha, e se tudo estiver correto vamos conseguir estabelecer o túnel SSH com sucesso. Execute o próximo comando e pronto, é só acessar seu browser local utilizando o endereço &lt;code&gt;http://localhost:8080&lt;/code&gt; que você vai acessar o seu Notebook rodando na nuvem ;)&lt;/p&gt;

&lt;pre style=&quot;font-size: 1.6em !important&quot;&gt;
    &lt;code class=&quot;bash&quot;&gt;
$ jupyter notebook --no-browser
    &lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;Não esqueça copiar o &lt;code&gt;token&lt;/code&gt; gerado para acessar seu Notebook. Ele vai aparecer no seu &lt;code&gt;command&lt;/code&gt; após o último comando.&lt;/p&gt;

&lt;h2 id=&quot;verificando-minha-vm&quot;&gt;Verificando minha VM&lt;/h2&gt;

&lt;p&gt;Como o demonstrado no vídeo, você pode utilizar o pacote &lt;code&gt;psutil&lt;/code&gt; para ter acesso ao &lt;code&gt;SO&lt;/code&gt;. Execute o o scritp abaixo em seu Jupyter Notebook para verificar o mesmo rodando em sua VM.&lt;/p&gt;

&lt;pre style=&quot;font-size: 1.6em !important&quot;&gt;
    &lt;code class=&quot;python&quot;&gt;
 import psutil
 psutil.cpu_count()
    &lt;/code&gt;
&lt;/pre&gt;

&lt;h2 id=&quot;conclusão&quot;&gt;Conclusão&lt;/h2&gt;

&lt;p&gt;Agora é só meter a mão na massa. Minha dica é utilizar essa VM para executar treinamentos pesados, já que aqui você pode contar com muitos &lt;code&gt;vCPUs&lt;/code&gt; para te auxiliar. Outra utilidade será quando você necessitar de grande quantidade memória &lt;code&gt;RAM&lt;/code&gt;, como no caso do &lt;code&gt;PySpark&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Não sofra esperando horas de treinamento… use esse tempo para o seu aprendizado.&lt;/p&gt;

&lt;h2 id=&quot;referências&quot;&gt;Referências&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://poweruphosting.com/blog/install-anaconda-python-ubuntu-16-04/&quot;&gt;How to install Anaconda Python on Ubuntu&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;&quot;&gt;How to create a Virtual Machine Linux on Azure&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Mon, 20 Nov 2017 00:00:00 -0200</pubDate>
        <link>http://localhost:4000/2017/11/20/jupyter-notebook-vm-azure/</link>
        <guid isPermaLink="true">http://localhost:4000/2017/11/20/jupyter-notebook-vm-azure/</guid>
        
        <category>ai</category>
        
        <category>machine learning</category>
        
        <category>data science</category>
        
        
        <category>AI</category>
        
        <category>Machine Learning</category>
        
        <category>Data Science</category>
        
      </item>
    
      <item>
        <title>Conversational Interfaces, NLP and bots</title>
        <description>&lt;p align=&quot;center&quot; style=&quot;background-color: #202020; width: 100%;&quot;&gt;&lt;img src=&quot;http://meriatblob.blob.core.windows.net/images/2017/07/20/new-bot.png&quot; style=&quot;margin-bottom: 0px !important;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;agenda&quot;&gt;Agenda&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Introdução&lt;/li&gt;
  &lt;li&gt;A origem do problema&lt;/li&gt;
  &lt;li&gt;A arte da compreensão semântica&lt;/li&gt;
  &lt;li&gt;Geração de significado&lt;/li&gt;
  &lt;li&gt;Analisando um Diálogo - Under the hood&lt;/li&gt;
  &lt;li&gt;Evolução tecnológica&lt;/li&gt;
  &lt;li&gt;Dos serviços de NLP
    &lt;ul&gt;
      &lt;li&gt;Api.ai&lt;/li&gt;
      &lt;li&gt;Luis.ai&lt;/li&gt;
      &lt;li&gt;Wit.ai&lt;/li&gt;
      &lt;li&gt;IBM Watson&lt;/li&gt;
      &lt;li&gt;Google Cloud Natural Language&lt;/li&gt;
      &lt;li&gt;Amazon Alexa Skill&lt;/li&gt;
      &lt;li&gt;Amazon Lex&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Conclusão&lt;/li&gt;
  &lt;li&gt;Referências&lt;/li&gt;
&lt;/ul&gt;

&lt;div style=&quot;margin-bottom: 4em;&quot;&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;O objetivo&lt;/strong&gt; neste post é passar por uma leve introdução ao &lt;code&gt;NLP&lt;/code&gt;, &lt;code&gt;Interfaces Conversacionais&lt;/code&gt;, &lt;code&gt;bots&lt;/code&gt; e suas significancias, e em conjunto analisar o seu funcionamento adaptando todos os conceitos de forma que seja possível ao fim do mesmo, melhorar suas habilidades em criação de diálogos, bem como o fazê-lo utilizando o serviço de sua preferência.&lt;/p&gt;

&lt;h2 id=&quot;intodução&quot;&gt;Intodução&lt;/h2&gt;

&lt;p&gt;Um dos assuntos do momento tem sido a construção de interfaces conversacionais, tanto para &lt;code&gt;bots&lt;/code&gt; quanto &lt;code&gt;assitentes de voz&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Uma interface conversacional é qualquer &lt;code&gt;UI&lt;/code&gt; que simula uma conversa real junto a um ser humano. A idéia aqui é que, em vez de se comunicar com um computador em seus próprios termos, seja usando sua tradução gráfica, seja inserindo comandos específicos de sintaxe, nós possamos utilizar a linguagem diária tal qual falamos com outros seres humanos.&lt;/p&gt;

&lt;div style=&quot;margin-bottom: 2em; margin-top: 2em; background-color: #dcbc14; color: #382d2d&quot;&gt;
&lt;p style=&quot;padding: 1.6em; font-family: courier;&quot;&gt;
O objetivo das interfaces de conversacionais é chegar ao ponto em que permitirão aos os humanos que conversem com os computadores de uma maneira na qual a responsabilidade da interpretação e de descobrir o que deve ser feito seja do software, e não no usuário. Esse é o objetivo na evolução do NLP, e certamente tende a influenciar a maneira como interagimos com os computadores.
&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;Apesar desta não ser uma disciplina moderna (se assim posso dizer), alguns problemas de ordem linguistica tem colocado as implementações atuais de &lt;strong&gt;bots&lt;/strong&gt; em um patamar ainda amador. Deixe-me explicar melhor isso…&lt;/p&gt;

&lt;h2 id=&quot;a-origem-do-problema&quot;&gt;A origem do problema&lt;/h2&gt;

&lt;p&gt;O primeiro e maior problema quando falamos na construção de &lt;strong&gt;interfaces de conversação&lt;/strong&gt;, é ter um diálogo que possa parecer o mais perto o possível da linguagem natural e fluida falada por duas pessoas.&lt;/p&gt;

&lt;p&gt;Para conseguir construir uma boa interface conversacional, é necessário ir além das expressões regulares, é necessário entender a estrutura da linguagem falada a fim de conseguir abstrair o sentido dado o contexto.&lt;/p&gt;

&lt;h2 id=&quot;a-arte-da-compreensão-semântica&quot;&gt;A arte da compreensão semântica&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;A comunicação humana está essencialmente ligada à capacidade de utilizar meios semióticos (como a linguagem) para transmitir as “intenções comunicativas” de um indivíduo e a capacidade de reconhecer tais intenções. &lt;em&gt;DASCAL, Marcelo. Interpretação e compreensão. 2006&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;O estudo da semântica alude à parte da linguagem referente ao significado das palavras e expressões que a mesma pode gerar. Neste contexto, cada palavra possui uma semântica própria, que difere da sua classificação enquanto função sintática ou morfológica.&lt;/p&gt;

&lt;p&gt;Contudo, de maneira geral, a semântica não é tratada de forma isolada em cada palavra, mas sim generalizada a contextos mais amplos. Assim, em um diálogo, por exemplo, pode-se identificar um significado particular em cada frase e um significado mais geral pertinente ao assunto tratado pelas pessoas que promovem o diálogo. Da mesma forma, em um texto dissertativo, embora cada parágrafo possa expressar um sentido particular, é a união de todos os parágrafos que formarão o sentido do texto.&lt;/p&gt;

&lt;p&gt;Como pode ser notado, a função básica de uma linguagem, ou seja, a comunicação, está centrada no significado das expressões lingüísticas produzidas. Isto torna o estudo da semântica essencial para implementações computacionais que envolvam compreensão e produção de linguagem. Uma barreira porém a este objetivo gira em torno da necessidade da utilização da intuição na compreensão do sentido de um texto, atos dificilmente aplicáveis a máquinas inspiradas na arquitetura de Von Neumman.&lt;/p&gt;

&lt;p&gt;O fato é que quando um usuário comum utiliza um programa de tradução de textos e percebe que ao final de todo o processamento - que geralmente requer um tempo considerável se analisado sob parâmetros computacionais - o texto resultante da tradução apresenta inúmeras deficiências, como faltam de pronomes, erros nos gêneros das palavras, bem como outros detalhes que embora não comprometam - ao menos nos sistemas atuais - a compreensão do contexto do documento, exigem sua revisão, pode ocorrer que aquele se sinta enganado pelo sistema.&lt;/p&gt;

&lt;p&gt;Ocorre que o processo de tratamento computacional da linguagem natural esbarra em enormes dificuldades, as quais são típicas da comunicação entre os seres humanos e que são tratadas pelo cérebro de uma forma tão normal e aparentemente simples que passam desapercebidas por exemplo a um usuário de computador que imagina que sua máquina possua as mesmas capacidades de seu cérebro no processamento de informações.&lt;/p&gt;

&lt;h2 id=&quot;geração-de-significado&quot;&gt;Geração de significado&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Entender&lt;/strong&gt; um texto significa reconhecer o contexto, fazer análise sintática, semântica, léxica e morfológica, criar resumos, extrair informação, interpretar os sentidos, analisar sentimentos e até aprender conceitos com os textos processados.&lt;/p&gt;

&lt;p&gt;No próximo tópico vamos analizar uma possível estrutura de diálogo com o objetivo de entender os conceitos por trás das ferramentas de processamento de linguagem natural, focando nos serviços comumente utilizados hoje.&lt;/p&gt;

&lt;h2 id=&quot;analisando-um-diálogo---under-the-hood&quot;&gt;Analisando um Diálogo - Under the hood&lt;/h2&gt;

&lt;p&gt;Vamos pensar na estrutura de um diálogo step-by-setp. Aproveitando a brincadeira da pizza, vamos tentar pedir uma pizza utilizando um bot imaginário.&lt;/p&gt;

&lt;p align=&quot;center&quot; style=&quot;width: 100%;&quot;&gt;&lt;img src=&quot;http://meriatblob.blob.core.windows.net/images/2017/07/20/conversation-1.png&quot; style=&quot;margin-bottom: 0px !important;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Olhando para o exemplo acima, podemos imaginar uma variedade de significados e formas diferentes de realizar a mesma pergunta:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Lugares com pizza&lt;/li&gt;
  &lt;li&gt;Pizzarias nas proximidades&lt;/li&gt;
  &lt;li&gt;Pizzas na região de Vila Mariana&lt;/li&gt;
  &lt;li&gt;Quero comprar pizza aqui perto de casa&lt;/li&gt;
  &lt;li&gt;Me lista as pizzarias no meu bairro, por favor&lt;/li&gt;
  &lt;li&gt;Onde tem pizza para entrega perto da minha localidade?&lt;/li&gt;
  &lt;li&gt;etc.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Se olharmos para o cerne da questão, todas as perguntas estão relacionadas ao que chamamos de &lt;code&gt;intenção&lt;/code&gt;, que neste caso é a busca por uma pizzaria em Vila Mariana. Para fins de classificação vamos atribuir uma &lt;code&gt;label&lt;/code&gt; a nossa intenção. Para este caso vamos chamar de &lt;strong&gt;localizar-pizzaria&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Ao definir uma determinada intenção, geramos um conjunto de sentenças, ou &lt;code&gt;enunciados&lt;/code&gt; que no geral denotam um sentido comum.&lt;/p&gt;

&lt;p&gt;Sendo assim se pergunto ao bot &lt;strong&gt;“onde acho pizzarias em Vila Mariana”&lt;/strong&gt;, posso &lt;code&gt;classificar&lt;/code&gt; essa nova sentença como sendo pertencente a intenção &lt;strong&gt;localizar-pizzaria&lt;/strong&gt;.&lt;/p&gt;

&lt;p align=&quot;center&quot; style=&quot;width: 100%;&quot;&gt;&lt;img src=&quot;http://meriatblob.blob.core.windows.net/images/2017/07/20/conversation-2.png&quot; style=&quot;margin-bottom: 0px !important;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Podemos perceber com base neste exemplo simples, que toda &lt;code&gt;comunicação&lt;/code&gt; dos usuários tem uma intenção que leva a alguma ação. Logo concluimos que a &lt;code&gt;intenção&lt;/code&gt; é o conceito central na construção de uma interface conversacional. Sendo assim a primeira coisa que podemos fazer com a mensagem enviada pelo usuário é entender sua intenção, ou seja, devemos mapear a sentença para uma ação específica.&lt;/p&gt;

&lt;p&gt;Junto com o intenção, é necessário identificar e extrair parâmetros, ou que na literatura clássica chamamos como &lt;code&gt;entidades&lt;/code&gt;, que são a base para a tomada de ações na frase. Se olharmos para o nosso exemplo &lt;strong&gt;“localizar pizzaria”&lt;/strong&gt;, as palavras &lt;strong&gt;“próximo”&lt;/strong&gt;, &lt;strong&gt;“na minha região”&lt;/strong&gt; correspondem à localização atual do usuário.&lt;/p&gt;

&lt;p&gt;O reconhecimento de entidades nomeadas é uma técnica amplamente utilizada em Processamento da Linguagem Natural e consiste na identificação de nomes de entidades-chave, presentes na forma livre de dados textuais. Nesse sentido, a entrada para um sistema de extração de entidades nomeadas e um texto em sua forma livre, e sua saída é um conjunto de textos anotados, ou seja, uma representação estruturada a partir da entrada de um texto não estruturado. Vamos ao exemplo:&lt;/p&gt;

&lt;h4&gt;&quot;&lt;span style=&quot;color:#62a84e&quot;&gt;Vitor Meriat&lt;/span&gt; mora em &lt;span style=&quot;color:#62a84e&quot;&gt;Vila Mariana&lt;/span&gt; e trabalha na &lt;span style=&quot;color:#62a84e&quot;&gt;ESX&lt;/span&gt;.&quot;&lt;/h4&gt;

&lt;p align=&quot;center&quot; style=&quot;width: 100%;&quot;&gt;&lt;img src=&quot;http://meriatblob.blob.core.windows.net/images/2017/07/20/c1.png&quot; style=&quot;margin-bottom: 0px !important;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Ao efetuar a extração das entidades nomeadas, temos &lt;strong&gt;[Vitor Meriat]&lt;/strong&gt;, &lt;strong&gt;[Vila Mariana]&lt;/strong&gt; e &lt;strong&gt;[ESX]&lt;/strong&gt;. Olhando para as entidades podemos atribuir para cada uma delas uma categoria apropriada, neste caso temos, &lt;strong&gt;pessoa&lt;/strong&gt;, &lt;strong&gt;localização&lt;/strong&gt; e &lt;strong&gt;organização&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Reconhecer e classificar categorias a entidades nomeadas presentes em um texto não é uma tarefa fácil. Isso fica evidente quando o assunto são nomes próprios e organizações. Este caso pode remeter a mais de uma categoria, ou simplesmente não possuir um contexto mais determinístico, em nível suficiente para auxiliar a tarefa de desambiguação. Vamos ao exemplo:&lt;/p&gt;

&lt;p align=&quot;center&quot; style=&quot;width: 100%;&quot;&gt;&lt;img src=&quot;http://meriatblob.blob.core.windows.net/images/2017/07/20/conversation-4.png&quot; style=&quot;margin-bottom: 0px !important;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Para o primeiro exemplo, o contexto poderia auxiliar na desambiguação. Ou seja, havendo uma relação entre os sintagmas [a Mercedes] e [a empresa], podemos aferir a categoria “Organização” a entidade “Mercedes”, uma vez que o sintagma nominal [a empresa] identifica uma organização.&lt;/p&gt;

&lt;div style=&quot;margin-bottom: 2em; margin-top: 2em; background-color: #dcbc14; color: #382d2d&quot;&gt;
&lt;p style=&quot;padding: 1.6em; font-family: courier;&quot;&gt;
O termo &lt;b&gt;&quot;sintagma&quot;&lt;/b&gt; designa uma sequência hierarquizada de elementos linguísticos, que compõem uma unidade na sentença.
&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;Isso ajuda a ilustrar o processo, bem como as dificuldades&lt;/p&gt;

&lt;p&gt;Até agora já vimos a representação das intenções e entidades. Um outro fator complicador é a questão do &lt;code&gt;contexto&lt;/code&gt; conversacional.&lt;/p&gt;

&lt;p&gt;Um exemplo disto é quando eu pergunto &lt;strong&gt;“onde compro pizza na minha região?”&lt;/strong&gt;. O contexto compreende o período do início da conversação, as interações (perguntas e respostas) até chegar na conclusão, que é &lt;strong&gt;localizar uma pizzaria&lt;/strong&gt; na sua atual localidade.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;desenho da conversação completa até o resultado final. Usuário com cara de feliz.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Notem que no processo temos um estado intermediário, que é em muitas vezes essencial para a tomada de decisão final. É ai que entra o contexto conversacional. Para armazenar o estado intermediário usamos o contexto.&lt;/p&gt;

&lt;p&gt;Podemos pensar no contexto como uma memória de curto prazo. Por exemplo, durante o bate-papo a procura de uma pizzaria, podemos armazenar a intenção &lt;strong&gt;“comprar pizza”&lt;/strong&gt; e posteriormente adicionar outros parâmetros como localidade e hora (que podem já ter sido informados), e assim tomar uma decisão que pode dar início a uma &lt;code&gt;ramificação&lt;/code&gt; do diálogo original.&lt;/p&gt;

&lt;p&gt;No geral podemos ter mais de um contexto conversacional dentro do mesmo diálogo. Contudo isso faz sentido se em algum momento eles convergirem para o resultado que o final seja uma resposta mais completa e assim leve o usuário a ideia de uma conversação fluída.&lt;/p&gt;

&lt;p&gt;Sendo assim, após identificar que o usuário quer comprar uma pizza, eu posso iniciar um novo contexto com o objetivo de obter as informações necessárias para um pedido.&lt;/p&gt;

&lt;p&gt;Lógico que isso é uma visão genérica levando em consideração disciplinas de linguagem natural, linguagem formal e que já foi traduzido como serviços focando nas habilidades necessárias para a montagem de diálogos para interfaces conversacionais.&lt;/p&gt;

&lt;h2 id=&quot;evolução-tecnológica&quot;&gt;Evolução tecnológica&lt;/h2&gt;

&lt;p&gt;Já é algo bem consolidado mais vale notar que &lt;code&gt;AI&lt;/code&gt; é um campo amplamente explorado nas últimas 5 décadas. Alguns fatores influenciaram o &lt;strong&gt;boom&lt;/strong&gt; que vivemos hoje, porém o mais importante para o nosso assunto é que recentemente tanto a maturidade quanto a disponibilidade de AI como serviço fornecida por grandes players de mercado como Microsoft, Google, IBM e Amazon, tem levado ao grande público as ferramentas necessárias para a criação de interfaces conversacionais robustas.&lt;/p&gt;

&lt;p&gt;Ferramentas de processamento de linguagem natural (NLP), com capacidade de entender as intenções e  expressas em uma frase e extrair suas entidades estão se tornando amplamente disponíveis e a custo acessível.&lt;/p&gt;

&lt;p&gt;Outro fator é a quantidade de ferramentas.&lt;/p&gt;

&lt;p&gt;Além disso, os frameworks focados na criação de ChatBots já pensando em toda a estrutura para integração com diversos &lt;code&gt;canais&lt;/code&gt; de comunicação vem crescendo e se tornando mais completos.&lt;/p&gt;

&lt;p&gt;A conclusão é que a tecnologia necessária para construir bots inteligentes já está disponível para desenvolvedores e corporações de forma satisfatória.&lt;/p&gt;

&lt;h2 id=&quot;resumindo-a-conversação&quot;&gt;Resumindo a conversação&lt;/h2&gt;

&lt;p&gt;No geral temos os seguintes termos para expressar todos os recursos de um diálogo:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Intenção:&lt;/strong&gt; Intenção é o conceito central de construção de interface conversacional. Em suma, intenção é a tarefa que o usuário deseja realizar ou o problema que o usuário deseja resolver.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Utterance:&lt;/strong&gt; Por Utterances (ou Stories), temos os exemplos de frases que o usuário pode digitar ou dizer quando ele se refere a uma determinada intenção.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Entidade:&lt;/strong&gt; Entidades (ou Parâmetros) incluem os detalhes importantes para uma melhor resolução da intenção do usuário. Pode ser qualquer coisa: localização, data, hora etc.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Contexto:&lt;/strong&gt; Contexto ajuda a salvar e compartilhar os parâmetros de sua conversação.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;dos-serviços-de-nlp&quot;&gt;Dos serviços de NLP&lt;/h2&gt;

&lt;p&gt;Na data em que escrevo este texto, seja por pesquisa ou por experiência, resolvi classificar as ferramentas citadas abaixo como os principais serviços para ser consumir &lt;strong&gt;NLP&lt;/strong&gt; de forma simples e confiável focados em interfaces conversacionais. Todas as ferramentas listadas neste texto seguem os princípios listados no tópico sobre analise de diálogos.&lt;/p&gt;

&lt;hr style=&quot;border: 1px dashed #f00; margin-top: 5em;&quot; /&gt;

&lt;h3 id=&quot;apiai&quot;&gt;Api.ai&lt;/h3&gt;
&lt;p align=&quot;center&quot; style=&quot;width: 100%;&quot;&gt;&lt;img src=&quot;http://meriatblob.blob.core.windows.net/images/2017/07/20/api.png&quot; style=&quot;margin-bottom: 0px !important;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;O &lt;strong&gt;Api.ai&lt;/strong&gt; é um serviço diferenciado. Recentemente adquirido pela Google, ele surgiu como um serviço de NLP para suportar um assistente pessoal em app e evoluiu rapidamente dado seu uso e adoção. O Api.ai fornece todos os recursos que já vimos inclusive uma habilidade ainda não citada, &lt;code&gt;speech to text&lt;/code&gt; e &lt;code&gt;text to speech&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Até o momento deste post, o Api.ai tem uma oferta free com limitação de banda e no reconhecimento de voz.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;SITE:&lt;/strong&gt; &lt;a href=&quot;https://www.api.ai&quot;&gt;api.ai&lt;/a&gt;&lt;/p&gt;

&lt;hr style=&quot;border: 1px dashed #f00; margin-top: 4em;&quot; /&gt;

&lt;h3 id=&quot;luisai&quot;&gt;Luis.ai&lt;/h3&gt;
&lt;p align=&quot;center&quot; style=&quot;background-color: #219680; width: 100%;&quot;&gt;&lt;img src=&quot;http://meriatblob.blob.core.windows.net/images/2017/07/20/luis.jpg&quot; style=&quot;margin-bottom: 0px !important;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;O &lt;strong&gt;Microsoft Language Understanding Intelligent Service&lt;/strong&gt;, ou como é mais citado LUIS, é a plataforma da Microsoft que compoem dentre outras o chamado &lt;code&gt;Bot Framework&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;No LUIS é possível de forma simples e intuitiva treinar seus modelos, criar e selecionar suas entidades, visualizar os gráficos de utilização e tudo que temos na web conseguimos fazer via API. Após nosso modelo devidamente treinado é só consumir tudo via API.&lt;/p&gt;

&lt;p&gt;Até o momento deste post, o LUIS encontra-se free para testes e em modo preview.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;SITE:&lt;/strong&gt; &lt;a href=&quot;https://www.luis.ai&quot;&gt;wuis.ai&lt;/a&gt;&lt;/p&gt;

&lt;hr style=&quot;border: 1px dashed #f00; margin-top: 4em;&quot; /&gt;

&lt;h3 id=&quot;witai&quot;&gt;Wit.ai&lt;/h3&gt;
&lt;p align=&quot;center&quot; style=&quot;width: 100%;&quot;&gt;&lt;img src=&quot;http://meriatblob.blob.core.windows.net/images/2017/07/20/wit.png&quot; style=&quot;margin-bottom: 0px !important;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;O &lt;strong&gt;Wit.ai&lt;/strong&gt; assim como seu concorrente Api.ai veio como foco de auxiliar os desenvolvedores nas capacidades de linguagem natural. Seu bom trabalho e plataforma brilharam os olhos do &lt;strong&gt;Facebook&lt;/strong&gt; que em 2015 fez a aquisição da mesma. Durante o próximo ano o Facebook realizou algumas alterações levando a ferramenta a compor seu &lt;code&gt;Bot Engine&lt;/code&gt;, o que mudou a orientação principal do Wit.ai de uma ferramenta guiada por intenções, para uma ferramenta guiada por &lt;code&gt;história&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Construir uma interface de conversação guiada pela “história” pode ser mais natural e mais fácil do que se basear em intenção, já que vamos trabalhado o todo. No geral o mecanismo não se altera já que continuamos defindo nossas intenções, entidades e etc.&lt;/p&gt;

&lt;p&gt;Até o momento deste post, o Wit.ai encontra-se free.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;SITE:&lt;/strong&gt; &lt;a href=&quot;https://wit.ai&quot;&gt;wit.ai&lt;/a&gt;&lt;/p&gt;

&lt;hr style=&quot;border: 1px dashed #f00; margin-top: 4em;&quot; /&gt;

&lt;h3 id=&quot;ibm-watson&quot;&gt;IBM Watson&lt;/h3&gt;
&lt;p align=&quot;center&quot; style=&quot;width: 100%;&quot;&gt;&lt;img src=&quot;http://meriatblob.blob.core.windows.net/images/2017/07/20/watson.jpg&quot; style=&quot;margin-bottom: 0px !important;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Talvez o &lt;strong&gt;IBM Watson&lt;/strong&gt; seja o mais famoso da lista. Isto se dá pela IBM ser um dos primeiros players de mercado a investir tanto na tecnologia quanto no marketing de suas soluções, tanto no poder de processamento, quanto no quesito IA.&lt;/p&gt;

&lt;p&gt;Temos a participação do Watson no famoso jogo da TV “Jeopardy” em 2011. Essa foi uma vitória memorável por vários motivos, mas o que importa agora é que temos todo este poder em nossas mãos, ou melhor, em uam API.&lt;/p&gt;

&lt;p&gt;Seguindo nossa listagem temos &lt;strong&gt;Watson Conversation&lt;/strong&gt;, que é a ferramenta direcionada para a construção de bots. Ele trabalha em um esquema direcional, onde você constroi um diálogo estruturado baseado em perguntas e respostas.&lt;/p&gt;

&lt;p&gt;A IBM também nos oferece o seu &lt;strong&gt;Watson Natural Language Understanding&lt;/strong&gt;, que permite ao usuário extrair metadados chave de seus textos, incluindo entidades, relações, conceitos, sentimentos e emoções.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;SITE:&lt;/strong&gt; &lt;a href=&quot;https://www.ibm.com/watson/services/conversation/&quot;&gt;watson conversation&lt;/a&gt;&lt;/p&gt;

&lt;hr style=&quot;border: 1px dashed #f00; margin-top: 4em;&quot; /&gt;

&lt;h3 id=&quot;google-cloud-natural-language&quot;&gt;Google Cloud Natural Language&lt;/h3&gt;
&lt;p align=&quot;center&quot; style=&quot;width: 100%;&quot;&gt;&lt;img src=&quot;http://meriatblob.blob.core.windows.net/images/2017/07/20/google-cloud.jpg&quot; style=&quot;margin-bottom: 0px !important;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;O &lt;strong&gt;Google Cloud Natural Language&lt;/strong&gt; oferece aos desenvolvedores acesso à análise de sentimentos, ao reconhecimento de entidades e análises de sintaxe.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;SITE:&lt;/strong&gt; &lt;a href=&quot;https://cloud.google.com/natural-language/&quot;&gt;cloud.google.com/natural-language&lt;/a&gt;&lt;/p&gt;

&lt;hr style=&quot;border: 1px dashed #f00; margin-top: 4em;&quot; /&gt;

&lt;h3 id=&quot;amazon-alexa-skill&quot;&gt;Amazon Alexa Skill&lt;/h3&gt;
&lt;p align=&quot;center&quot; style=&quot;background-color: #009ad4; width: 100%;&quot;&gt;&lt;img src=&quot;http://meriatblob.blob.core.windows.net/images/2017/07/20/alexa.png&quot; style=&quot;margin-bottom: 0px !important;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Primeiro ponto a ser notado é que este serviço só é utilizado para o device Alexa. Fora isso o mecanismo é o mesmo, definimos intenções, entidades e ações. O grande ponto do Alexa Skill é o seu foco na linguagem falada.&lt;/p&gt;

&lt;p&gt;O treino é mais complexo já que você necessita informar um conjunto maior de “dados” para que as expressões possam ser reconhecidas e classificadas em uma determinada intenção.&lt;/p&gt;

&lt;p&gt;Outro ponto interessante é a integração do Alexa Skills Kit e o Amazon Lambda, que traz uma grande facilidade no desenvolvimento.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;SITE:&lt;/strong&gt; &lt;a href=&quot;https://developer.amazon.com/public/solutions/alexa/alexa-skills-kit&quot;&gt;developer.amazon.com/alexa-skills-kit&lt;/a&gt;&lt;/p&gt;

&lt;hr style=&quot;border: 1px dashed #f00; margin-top: 4em;&quot; /&gt;

&lt;h3 id=&quot;amazon-lex&quot;&gt;Amazon Lex&lt;/h3&gt;
&lt;p align=&quot;center&quot; style=&quot;background-color: #FFFFFF; width: 100%;&quot;&gt;&lt;img src=&quot;https://d0.awsstatic.com/Digital%20Marketing/House/Editorial/products/ai/AI-380x186.png&quot; style=&quot;margin-bottom: 0px !important;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;O &lt;strong&gt;Amazon Lex&lt;/strong&gt; é a solução da Amazon para a construção de interfaces conversacionais se utilizando dos recursos de NLP e integração com outros serviços da Amazon, como Lambda, Dynamo DB entre outros.&lt;/p&gt;

&lt;p&gt;O Lex foi construído com base no Amazon Alexa Skill Kit. Isso é uma notícia verdadeiramente emocionante, uma vez que o Alexa já foi testada e treinada por mais de 3 milhões de usuários da Amazon Alexa.&lt;/p&gt;

&lt;p&gt;A interface web Lex te fornece não só os recursos de configuração, mas também um painel de monitoramento onde você pode analisar diferentes métricas do serviço.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;SITE:&lt;/strong&gt; &lt;a href=&quot;https://aws.amazon.com/pt/lex/&quot;&gt;aws.amazon.com/lex&lt;/a&gt;&lt;/p&gt;

&lt;hr style=&quot;border: 1px dashed #f00; margin-top: 4em;&quot; /&gt;

&lt;h3 id=&quot;menção-honrosa&quot;&gt;Menção honrosa&lt;/h3&gt;
&lt;p&gt;Vou deixar aqui registrado alguns serviços que não consegui testar ainda, mas que são utilizados pela comunidade e também devem ter sua consideração. São eles:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://recast.ai/&quot;&gt;Recast.ai&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://kueri.me/&quot;&gt;Kueri.me&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.opencalais.com/&quot;&gt;Thomson Reuters Open Calais&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;div style=&quot;margin-bottom: 6em;&quot;&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Disclaimer:&lt;/strong&gt; Até este momento não testei de forma massiva todos os serviços, logo, se houver algum erro por favor me informe. Todos os comentários são bem-vindos. Se houver mais algum serviço não mencionado, sinta-se convidade o colaborar ;)&lt;/p&gt;

&lt;h1 id=&quot;conclusão&quot;&gt;Conclusão&lt;/h1&gt;
&lt;p&gt;Espero que esta pequena jornada seja útil para te ajudar a encontrar um melhor caminho na construção de diálogos inteligentes e cada vez mais naturais. Entender um pouco da problemática é importante para este fim.&lt;/p&gt;

&lt;p&gt;Outro ponto que abordei aqui foram as plataformas que temos hoje para iniciar de maneira rápida e barata no mundo das interfaces conversacionais. No geral mesmo as APIs citadas sendo projetadas para trabalhar em resposta a uma única intenção ou ação, elas possuem todos os requisitos para que você consiga montar um bom diálogo.&lt;/p&gt;

&lt;p&gt;Ainda falta uma melhora no quesito do conhecimento especializado, já que o único ponto de aprendizado é por meio das sentenças que informamos as APIs, sem um aprendizado coletivo das intenções.&lt;/p&gt;

&lt;p&gt;Este é apenas uma introdução ao tema. Espero em breve poder trazer mais conteúdo sobre este tema que tem tudo para evoluir a maneira como interagimos quanto sociedade.&lt;/p&gt;

&lt;h1 id=&quot;referências&quot;&gt;Referências&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;DASCAL, Marcelo. Interpretação e compreensão. Tradução de Márcia Heloisa Lima da Rocha. São Leopoldo: Editora Unisinos, 2006, p.729.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://stanfy.com/blog/advanced-natural-language-processing-tools-for-bot-makers/&quot;&gt;Advanced NLP tools for bot makers&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sun, 06 Aug 2017 00:00:00 -0300</pubDate>
        <link>http://localhost:4000/2017/08/06/conversational-interfaces-nlp-bots/</link>
        <guid isPermaLink="true">http://localhost:4000/2017/08/06/conversational-interfaces-nlp-bots/</guid>
        
        
        <category>AI</category>
        
        <category>NLP</category>
        
        <category>Bot</category>
        
      </item>
    
      <item>
        <title>Custom Vision, Cognitive Services and Computer Vision</title>
        <description>&lt;p&gt;Os serviços cognitivos da Microsoft tem evoluído muito desde seu início. Como é sabido o mesmo se iniciou no Microsoft Research com o famoso &lt;strong&gt;Project Oxford&lt;/strong&gt;, que já disponibilizava uma série de &lt;code&gt;API's&lt;/code&gt; para se trabalhar especialmente com Visão Computacional e Processamento de Linguagem Natural.&lt;/p&gt;

&lt;p&gt;Focando na Visão Computacional, quando a Microsoft lançou o &lt;strong&gt;Cognitive Services&lt;/strong&gt;, as API’s de serviços cognitivos já disponibilizavam uma grande capacidade e variedade, te posssibilitando trabalhar de uma simples análise de imagens a detecção de faces com análise de sentimentos. Algumas combinações de serviços nos proporcionam inúmeras possibilidades de uso, como por exemplo, uma análise de sentimento em texto que dava a possibilidade do usuário enviar uma imagem que era interpretada via OCR, e tinha sua analise de sentimento exatamente igual ao texto digitado.&lt;/p&gt;

&lt;p&gt;Contudo não era possível trabalhar de forma específica. Imagine que você precisa identificar um determinado elemento. Você tem diversas possibilidades, porém não era possível realizar um treino para a obtenção de um resultado definido… até agora…&lt;/p&gt;

&lt;p&gt;No Build deste ano (2017), a Microsoft lançou o Serviço de Visão Customizada. Este novo serviço possibilita que você trabalhe com seu próprio dataset de treino a fim de aprender e identificar estes mesmos padrões em outras imagens.&lt;/p&gt;

&lt;p&gt;Seu funcionamento é extremamente simples, basta enviar seu dataset de treino, definir um label para o mesmo, mandar treinar e verificar a acurácia do modelo. O próximo passo e gerar sua API de acesso para consumir seu modelo personalizado.&lt;/p&gt;

&lt;div style=&quot;margin-bottom: 3em;&quot;&gt;&lt;/div&gt;

&lt;h3 id=&quot;under-the-hood&quot;&gt;Under the Hood&lt;/h3&gt;

&lt;p&gt;Existe uma série de conceitos por debaixo dos panos. Não vou entrar em questões mais específicas sobre visão computacional ou esse aprendizado, nem tão pouco dicutir os algorítmos em si. Entretando alguma base é necessária para se utilizar o Custom Vision.&lt;/p&gt;

&lt;p&gt;De forma mais básica e genérica o possível, podemos pensar neste aprendizado e treino como o input de dados e extração de características para uma posterior classificação/reconhecimento de imagens.&lt;/p&gt;

&lt;p&gt;Ok, como é possível usar o Cognitive Services que já está treinado para atividades específicas, e usá-lo para reconhecer um padrão meu?&lt;/p&gt;

&lt;p&gt;Olhando mais a fundo estamos falando aqui do conceito de &lt;code&gt;Transfer Learning&lt;/code&gt;, já que na realidade, temos os algorítimos de Deep Learning e modelos pré-treinados que são ensinados a procurar detalhes a recurso distintos em um novo dataset que é informado posteriormente.&lt;/p&gt;

&lt;p&gt;Transferência de aprendizagem, é um campo de pesquisa na aprendizagem de máquinas que se concentra no armazenamento de conhecimento adquirido ao resolver um problema, e o usa para resolver um problema diferente, mas relacionados.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Sendo assim, Transfer Learning se trata da capacidade de usar modelos pré-treinados para solucionar problemas relacionados com um treinamento reduzido.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Em nosso caso o &lt;code&gt;serviço de visão computacional&lt;/code&gt; do Azure já possui em amplo treinamento em diversos domínios, o que nos da a possibilidade de transferir essa aprendizagem a um domínio menor… e assim temos nosso &lt;strong&gt;Custom Vision API&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Para conseguirmos um resultado satisfatório, nosso objetivo deve ser o reconhecimento de padrões de algo específico. O melhor cenário aqui é trabalhar na detecção de algo único, com um bom dataset das diversas posições, perspectiva, iluminação e etc.&lt;/p&gt;

&lt;hr style=&quot;margin-bottom: 4em;margin-top: 6em;&quot; /&gt;

&lt;h2 id=&quot;show-me-the-code---first-try&quot;&gt;Show me the code - First Try&lt;/h2&gt;

&lt;p&gt;Uma vez que já temos uma base vamos aos passos necessários para realizar nosso primeiro treino com o &lt;strong&gt;Microsoft Congnitive Service Vision Custom&lt;/strong&gt; e discutir um pouco sobre sua aplicação prática…&lt;/p&gt;

&lt;div style=&quot;margin-bottom: 3em;&quot;&gt;&lt;/div&gt;

&lt;h4 id=&quot;criando-um-novo-projeto&quot;&gt;Criando um novo projeto&lt;/h4&gt;

&lt;p&gt;Primeiro você vai precisar acessar o site específico do Custom Vision em &lt;a href=&quot;https://customvision.ai&quot;&gt;customvision.ai&lt;/a&gt;. Agora é só fazer o login com sua conta do Microsoft Azure.&lt;/p&gt;

&lt;p&gt;Assim que você estiver logado vai ver a tela com os seus projetos, e a opção para a criação de um novo projeto. Assim como segue abaixo:&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;http://meriatblob.blob.core.windows.net/images/2017/06/07/01-custom-vision.png&quot; class=&quot;absolute-bg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Crie um novo projeto informando o nome do mesmo, uma descrição e selecione a opção &lt;code&gt;General&lt;/code&gt;. Você pode treinar um modelo usando um cenário específico, é bastante útil em caso de já utilizarmos uma &lt;strong&gt;memória&lt;/strong&gt; de auxílio. No meu caso eu usei como nome do projeto &lt;strong&gt;vehicles&lt;/strong&gt;, e como modo de treino a opção &lt;strong&gt;General&lt;/strong&gt;.&lt;/p&gt;

&lt;div style=&quot;margin-bottom: 3em;&quot;&gt;&lt;/div&gt;

&lt;h4 id=&quot;carregando-nosso-dataset&quot;&gt;Carregando nosso dataset&lt;/h4&gt;

&lt;p&gt;Com o projeto criado precisamos treinar nosso modelo. Isso só é possível se tivermos dados. Então vamos lá, clique no botão upload e envie suas fotos.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Lembre-se da importância de um bom conjunto de dados, com fotos em diversos ângulos, tamanhos, variações de iluminação e etc. Quanto mais variado, mais caracteristicas serão aprendidas.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;http://meriatblob.blob.core.windows.net/images/2017/06/07/02-custom-vision.png&quot; class=&quot;absolute-bg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Para este teste estive procurando uma opção simples de treino. Eu utilizei conjunto de datasets disponibilizado pela &lt;a href=&quot;http://www.caltech.edu/&quot;&gt;Caltech&lt;/a&gt; com foco em visão computacional. Estou utilizando neste primeiro momento o dataset &lt;strong&gt;CARS&lt;/strong&gt; de 2001.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;http://meriatblob.blob.core.windows.net/images/2017/06/07/03-custom-vision.png&quot; class=&quot;absolute-bg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Aqui eu tenho minha primeira surpresa: Existe uma limitação na quantidade de arquivos a serem enviados. No total, para cada projeto podemos enviar apenas 1000 imagens para o treino.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;http://meriatblob.blob.core.windows.net/images/2017/06/07/04-custom-vision.png&quot; class=&quot;absolute-bg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Quem já trabalhou com este tipo de treinamento, provavelmente pode cair na mesma cilada, já que geralmente temos grandes quantidades de imagens para este tipo de treino.&lt;/p&gt;

&lt;div style=&quot;margin-bottom: 5em; margin-top: 4em; background-color: #dcbc14; color: #382d2d&quot;&gt;
&lt;p style=&quot;padding: 1.6em; font-family: courier;&quot;&gt;
É importante ler todas as limitações e cotas de utilização de um serviço antes de usá-lo. Por exemplo, temos limitação de 1000 imagens, imagens somente até 4MB, somente JPG, PNG e BMP e etc.
&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;Sendo assim resolvi mudar minha estratégia: Dividir minhas imagens em grupos de 300, já que estou falando de 3 tipos de veículos que quero identificar:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Cars&lt;/li&gt;
  &lt;li&gt;Motorbikes&lt;/li&gt;
  &lt;li&gt;Airplanes&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;O que eu fiz foi criar um simples script em python para selecionar 300 imagens aleatórias do meu dataset, para cada categoria.&lt;/p&gt;

&lt;pre style=&quot;font-size: 1.2em !important&quot;&gt;
    &lt;code class=&quot;python&quot;&gt;
import os
import random
import numpy as np
from sklearn.model_selection import train_test_split

dir_src = &quot;{diretório fonte}&quot;
dir_test = &quot;{diretório destino de teste}&quot;
dir_train = &quot;{diretório destino de train}&quot;

# Quantidade de imagens a serem selecionadas no dataset
qtd_images = 300

print('\nDiretório\n')

print('-'*30)

def get_filepaths(directory):

	file_paths = [] 
	
	for root, directories, files in os.walk(directory):
		for filename in files:
			# Supported image formats: JPEG, PNG, GIF, BMP.
			if filename[-4::] == 'jpeg' or filename[-3::] == 'jpg' or filename[-3::] == 'png' or filename[-3::] == 'gif' or filename[-3::] == 'bmp':
			    file_paths.append(filename)

	return file_paths
	
full_file_paths = get_filepaths(dir_src)
	
result = random.sample(set(full_file_paths), qtd_images)

print('\nNúmero de arquivos do dataset: ' + str(len(full_file_paths)))

print('\nItens selecionados randomicamente: ' +str(len(result)))

X = y = result

# use 1/4 data for testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)

for xt in X_test:
	os.rename(dir_src + xt, dir_test + xt)

for yt in X_train:
	os.rename(dir_src + yt, dir_train + yt)
	
print('\nQuantidade de arquivos no conjunto de treino: ' + str(len(X_train)))

print('\nQuantidade de arquivos no conjunto de teste: ' + str(len(X_test)))
    &lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;Você pode fazer o download deste código direto no meu gist &lt;a href=&quot;https://gist.github.com/vitormeriat/98785f92763bcc775d0a49704a0d33fd&quot;&gt;diretamente neste link&lt;/a&gt;. O output será como o descrito abaixo:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;λ python split.py

Diretório

------------------------------

Número de arquivos do dataset: 526

Itens selecionados randomicamente: 300

Quantidade de arquivos no conjunto de treino: 225

Quantidade de arquivos no conjunto de teste: 75
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Nosso diretório vai ficar parecido como o descrito na imagem abaixo:&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;http://meriatblob.blob.core.windows.net/images/2017/06/07/cars-folder.png&quot; class=&quot;absolute-bg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Não esqueça que esse procedimento deve ser realizado para cada uma das categorias de veículos que queremos testar.&lt;/p&gt;

&lt;p&gt;Agora eu posso fazer o upload dessas imagens… Quando cada uma das categorias são carregadas, é necessário informar uma &lt;code&gt;tag&lt;/code&gt;. A tag vai representar aquele recurso, logo ao executarmos um teste, teremos de resposta a porcentagem da mesma ser ou não correspondente ao conjunto representado pela tag.&lt;/p&gt;

&lt;div style=&quot;margin-bottom: 3em;&quot;&gt;&lt;/div&gt;

&lt;h4 id=&quot;um-pouquinho-mais-sobre-o-reconhecimento-de-objetos-e-o-tagueamento&quot;&gt;Um pouquinho mais sobre o reconhecimento de objetos e o tagueamento&lt;/h4&gt;

&lt;p&gt;O serviço de visão computacional do Cognitive Service hoje, é treinado para o reconhecimento de mais de 2000 objetos, sendo eles seres vivos, cenários ou ações.&lt;/p&gt;

&lt;p&gt;Este reconhecimento é classificado e categorizado seguindo a seguinte taxonomia:&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;http://meriatblob.blob.core.windows.net/images/2017/06/07/analyze_categories.jpg&quot; class=&quot;absolute-bg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Em nosso caso estamos criando um modelo simples com apenas 4 classes e que são distintas entre si. Como já vimos anteriormente temos carros, motos e aviões. Se passarmos um de nossos dados de treino para o serviço de análise de imagem do Cognitive Services teremos como resultado o que se segue abaixo:&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;http://meriatblob.blob.core.windows.net/images/2017/06/07/car.jpg&quot; class=&quot;absolute-bg&quot; /&gt;&lt;/p&gt;

&lt;pre&gt;
    &lt;code class=&quot;json&quot;&gt;
 &quot;categories&quot;: [
    {
      &quot;name&quot;: &quot;others_&quot;,
      &quot;score&quot;: 0.00390625
    },
    {
      &quot;name&quot;: &quot;outdoor_road&quot;,
      &quot;score&quot;: 0.296875
    },
    {
      &quot;name&quot;: &quot;trans_car&quot;,
      &quot;score&quot;: 0.67578125
    }
  ],
 &quot;description&quot;: {
    &quot;captions&quot;: [
      {
        &quot;confidence&quot;: 0.9560722703980943,
        &quot;text&quot;: &quot;a car parked on the side of a road&quot;
      }
    ],
 &quot;tags&quot;: [
    {
      &quot;confidence&quot;: 0.9992092251777649,
      &quot;name&quot;: &quot;outdoor&quot;
    },
    {
      &quot;confidence&quot;: 0.9986016154289246,
      &quot;name&quot;: &quot;car&quot;
    },
    {
      &quot;confidence&quot;: 0.9985461235046387,
      &quot;name&quot;: &quot;sky&quot;
    },
    {
      &quot;confidence&quot;: 0.9938879609107971,
      &quot;name&quot;: &quot;road&quot;
    },
    {
      &quot;confidence&quot;: 0.9547197818756104,
      &quot;name&quot;: &quot;way&quot;
    },
    {
      &quot;confidence&quot;: 0.8574815392494202,
      &quot;name&quot;: &quot;scene&quot;
    },
    {
      &quot;confidence&quot;: 0.8200963139533997,
      &quot;name&quot;: &quot;street&quot;
    },
    {
      &quot;confidence&quot;: 0.6744081974029541,
      &quot;name&quot;: &quot;highway&quot;
    },
    {
      &quot;confidence&quot;: 0.37572911381721497,
      &quot;name&quot;: &quot;stopped&quot;
    }
 ]
    &lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;Observe que nossa imagem foi categorizada em 3 locais, onde as duas melhores classificações são realtivas a carros. Em relação ao tagueamento, vemos &lt;code&gt;car&lt;/code&gt; na segunda posição. A descrição indica &lt;strong&gt;a car parked on the side of a road&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Faça o mesmo teste com outras images, incluindo avião e moto. Você vai perceber que o serviço tem uma boa acurácia em relação ao reconhecimento dessas imagens.&lt;/p&gt;

&lt;p&gt;Do nosso lado, o tagueamento é importante para representar corretamente as imagens, e ter um retorno claro, já que a ideia final é consumir esse modelo via &lt;code&gt;REST&lt;/code&gt;.&lt;/p&gt;

&lt;div style=&quot;margin-bottom: 3em;&quot;&gt;&lt;/div&gt;

&lt;h4 id=&quot;treinando-o-modelo&quot;&gt;Treinando o modelo&lt;/h4&gt;

&lt;p&gt;Agora vamos ao passo mais simples, clique no botão &lt;code&gt;Train&lt;/code&gt;. No meu caso o treinamento levou &lt;code&gt;23.09&lt;/code&gt; segundos.&lt;/p&gt;

&lt;p&gt;Com o modelo treinado, você pode ir na página &lt;code&gt;PERFORMANCE&lt;/code&gt;, onde encontramos o seguinte gráfico:&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;http://meriatblob.blob.core.windows.net/images/2017/06/07/06-custom-vision.png&quot; class=&quot;absolute-bg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Este gráfico possui duas medidas, &lt;code&gt;Precision&lt;/code&gt; e &lt;code&gt;Recall&lt;/code&gt;, sendo que &lt;strong&gt;Precision&lt;/strong&gt; representa a probabilidade de seu classificador conseguir identificar corretamente uma imagem. &lt;strong&gt;Recall&lt;/strong&gt; representa a porcentagem de imagens contendo os itens que queremos identificar no conjunto enviado.&lt;/p&gt;

&lt;p&gt;Em nosso caso temos uma presição de &lt;code&gt;99.7%&lt;/code&gt;, o que indica que alguma imagem enviada não foi reconhecida e portanto foi descartada.&lt;/p&gt;

&lt;p&gt;Para fazer o &lt;code&gt;double check&lt;/code&gt;, acesse a guia &lt;strong&gt;TRAINING IMAGES&lt;/strong&gt; e depois &lt;strong&gt;Iteration History&lt;/strong&gt;. Agora podemos ver qual imagem confundiu nosso modelo.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;http://meriatblob.blob.core.windows.net/images/2017/06/07/12-custom-vision.png&quot; class=&quot;absolute-bg&quot; /&gt;&lt;/p&gt;

&lt;div style=&quot;margin-bottom: 3em;&quot;&gt;&lt;/div&gt;

&lt;h4 id=&quot;hora-do-teste&quot;&gt;Hora do Teste&lt;/h4&gt;

&lt;p&gt;Agora vamos aos testes. Podemos fazer isso de forma simples utilizando o próprio site. Vamos para a aba &lt;strong&gt;Quick Test&lt;/strong&gt;, selecione a opção &lt;strong&gt;Browse local files&lt;/strong&gt; e selecione uma imagem para teste.&lt;/p&gt;

&lt;p&gt;No meu caso estou utilizando uma das imagens de teste que separei anteriormente. Você pode selecionar uma imagem da própria web.&lt;/p&gt;

&lt;p&gt;Ao fazer o upload, note que você já terá a classificação da imagem segundo seu modelo.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;http://meriatblob.blob.core.windows.net/images/2017/06/07/13-custom-vision.png&quot; class=&quot;absolute-bg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Tente utilizar outras imagens de teste… temos os carros, aviões ou até mesmo coisas que não tenham nenhuma ligação com o modelo treinado.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;http://meriatblob.blob.core.windows.net/images/2017/06/07/14-custom-vision.png&quot; class=&quot;absolute-bg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Outra coisa interessante é que ao realizar estes testes, você pode acessar a guia &lt;strong&gt;PREDICTIONS&lt;/strong&gt;. Lá você vai ver todas as imagens que foram enviadas para teste.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;http://meriatblob.blob.core.windows.net/images/2017/06/07/15-custom-vision.png&quot; class=&quot;absolute-bg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Note que aqui temos a classificação que foi realizada para cada imagem. Você pode deletar uma ou todas as imagens, como também realizar um novo treino com essas imagens sendo adicionadas ao dataset original. Isso pode ou não melhorar a precisão do nosso modelo.&lt;/p&gt;

&lt;div style=&quot;margin-bottom: 5em; margin-top: 4em; background-color: #dcbc14; color: #382d2d&quot;&gt;
&lt;p style=&quot;padding: 1.6em; font-family: courier;&quot;&gt;
Este artigo está fortemente baseado na utilização do portal. Lembre-se que podemos fazer da &lt;b&gt;ingestão&lt;/b&gt; ao &lt;b&gt;treino&lt;/b&gt; via código.
&lt;/p&gt;
&lt;/div&gt;

&lt;h4 id=&quot;gerando-novos-modelos&quot;&gt;Gerando novos modelos&lt;/h4&gt;

&lt;p&gt;Durante os testes você pode verificar que seu modelo precisa melhor. Você já sabe que um bom modelo vai depender da qualidade de seus dados, e segundo essa linha você adiciona novas imagens com mais angulos, cores e perspectivas diferentes.&lt;/p&gt;

&lt;p&gt;Pronto, agora você só precisa realizar outro treino para verificar se o novo conjunto vai ou não melhorar sua classificação.&lt;/p&gt;

&lt;p&gt;O resultado deste processo é que será gerado a cada treino um novo modelo que será chamado aqui de &lt;code&gt;Iteration&lt;/code&gt;.&lt;/p&gt;

&lt;h4 id=&quot;gerando-nosso-modelo-as-a-service&quot;&gt;Gerando nosso Modelo as a Service&lt;/h4&gt;

&lt;p&gt;Já temos nosso modelo treinado, realizamos alguns testes e agora chegou o momento de usar consumir nosso modelo.&lt;/p&gt;

&lt;p&gt;Para isso você só precisa clicar em &lt;strong&gt;Prediction URL&lt;/strong&gt;. Com isso veremos o &lt;code&gt;endpoint&lt;/code&gt; da nossa aplicação, que vamos usar para consumir o modelo. Aqui também temos nossa &lt;code&gt;Prediction-Key&lt;/code&gt;, que deve ser informada no cabeçalho da requisição.&lt;/p&gt;

&lt;p&gt;Um detalhe importante é que você pode definir qual &lt;strong&gt;Iteration&lt;/strong&gt; você quer consumir. Sendo assim é possível usar o modelo padrão ou definir um modelo com melhor precisão.&lt;/p&gt;

&lt;p&gt;Em relação ao código é tudo muito simples. Você pode enviar a url ou o binário. Para efeitos práticos realizei o primeiro teste utilizando o postman.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;http://meriatblob.blob.core.windows.net/images/2017/06/07/custom-vision-postman.png&quot; class=&quot;absolute-bg&quot; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A imagem utilizada segue abaixo. Você pode acessar a imagem no seguinte link: &lt;a href=&quot;http://meriatblob.blob.core.windows.net/images/2017/06/07/car-train.jpg&quot;&gt;car-train&lt;/a&gt;. Essa foi uma imagem retirada da internet, você pode passar um link qualquer para realizar seu teste.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;http://meriatblob.blob.core.windows.net/images/2017/06/07/car-train.jpg&quot; class=&quot;absolute-bg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Neste caso estou utilizando a &lt;code&gt;API&lt;/code&gt; de Custom Vision Prediction, que aponta diretamente para o nosso modelo.&lt;/p&gt;

&lt;div style=&quot;margin-bottom: 6em;&quot;&gt;&lt;/div&gt;

&lt;h1 id=&quot;conclusão&quot;&gt;Conclusão&lt;/h1&gt;
&lt;p&gt;Esta é uma parte introdutória do assunto e do serviço, porém já é possível perceber todo o potencial oferecido pelo produto. Existem ainda algumas observações importantes a serem feitas, tanto na questão mais prática em relação ao desenvolvimento, quanto na questão mais teórica, a fim de entender os propósitos e assim construir modelos satisfatórios.&lt;/p&gt;

&lt;p&gt;Estou roterizando um vídeo sobre o assunto, e creio que lá será mais simples expor todo o conteúdo e realizar melhor os testes. Assim que o mesmo estiver publicado, atualizo este artigo.&lt;/p&gt;

&lt;p&gt;Por hora, é isso ai pessoal.&lt;/p&gt;

&lt;h1 id=&quot;referências&quot;&gt;Referências&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Artificial_neural_network&quot;&gt;Artificial Neural Networks, Wikipedia&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.saedsayad.com/artificial_neural_network.htm&quot;&gt;Artificial Neural Networks, by Saed Sayad&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=bxe2T-V8XRs&quot;&gt;Neural Networks Demystified, by Stephen Welch&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Activation_function&quot;&gt;Activation function, Wikipedia&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Transfer_learning&quot;&gt;Transfer Learning&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.microsoft.com/en-us/azure/cognitive-services/computer-vision/home&quot;&gt;Computer Vision, Cognitive Services&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://southcentralus.dev.cognitive.microsoft.com/docs/services/d9a10a4a5f8549599f1ecafc435119fa/operations/58d5835bc8cb231380095be3&quot;&gt;Custom Vision Training&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://southcentralus.dev.cognitive.microsoft.com/docs/services/eb68250e4e954d9bae0c2650db79c653/operations/58acd3c1ef062f0344a42814&quot;&gt;Custom Vision Prediction&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Wed, 07 Jun 2017 00:00:00 -0300</pubDate>
        <link>http://localhost:4000/2017/06/07/custom-vision-ms/</link>
        <guid isPermaLink="true">http://localhost:4000/2017/06/07/custom-vision-ms/</guid>
        
        
        <category>AI</category>
        
        <category>Cognitive Computing</category>
        
      </item>
    
      <item>
        <title>Microsoft and Artificial Intelligence for the future</title>
        <description>&lt;p&gt;Aproveitando o famoso &lt;a href=&quot;https://build.microsoft.com/&quot;&gt;Microsoft Build&lt;/a&gt; que em 2017 ocorreu em Seattle, e juntando ao convite para resumir o que foi apresentando durante esta conferência em relação a AI no &lt;a href=&quot;https://www.meetup.com/DeliveringSoftware/events/239680578/&quot;&gt;Delivering Software meetup&lt;/a&gt;, vou expor um pouco do histórico da Microsoft e conjecturar sobre sua visão de futuro no quesito Artificial Intelligence.&lt;/p&gt;

&lt;p&gt;Segundo Harry Shum (Executive Vice President, Microsoft AI and Research), durante seu discurso no Build:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Alguns anos atrás, era difícil pensar em uma ferramenta de tecnologia popularmente usada que fazia uso do poder da AI. Em poucos anos, será difícil imaginar qualquer tecnologia que não aproveite o poder da AI.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Isso nos mostra como as grandes companias como Microsoft, Google, Amazon e etc, estão focando em produtos que explorem o poder na inteligência artificial.&lt;/p&gt;

&lt;h2 id=&quot;sobre-o-build-2017&quot;&gt;Sobre o Build 2017&lt;/h2&gt;

&lt;p&gt;Sobre o que foi apresentado no Build 2017, temos algumas novidades interessantes. Se comparado com o Build passado (2016), podemos dizer que a Microsoft trouxe bastante coisa nova ao cenário.&lt;/p&gt;

&lt;h4 id=&quot;cognitive-services&quot;&gt;Cognitive Services&lt;/h4&gt;
&lt;p&gt;E relação aos cognitive services, foram acrescentados 4 novos serviços aos 25 já existentes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://azure.microsoft.com/pt-br/services/cognitive-services/bing-custom-search/&quot;&gt;Bing Custom Search&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://azure.microsoft.com/en-us/services/cognitive-services/custom-vision-service/&quot;&gt;Custom Vision Service&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://azure.microsoft.com/en-us/services/cognitive-services/custom-decision-service/&quot;&gt;Custom Decision Service&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://azure.microsoft.com/en-us/services/cognitive-services/video-indexer/&quot;&gt;Video Indexer&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;O que vale salientar aqui é que agora temos outros 3 serviços customizáveis, o que traz uma outra linha de atuação aqui. Tirando o LUIS (Language Understanding Intelligent Service) e o Custom Speech Service, todos os demais serviços se tratam de aprendizado prévio e consumível via API. Como LUIS é possível haver uma customização e aprendizagem da intenção com base no que foi estipulado. Podemos utilizar o Custom Speech Service para aprender com um determinado conjunto de linguagem, seja técnica ou não, e adaptar esse vocabulário para seu usuário final. Na prática os novos serviços customizáveis dão mais poder aos usuários, permitindo aos mesmos realizar aprendizados e experiências direcionadas aos seus problemas específicos.&lt;/p&gt;

&lt;h4 id=&quot;cognitive-services-labs&quot;&gt;Cognitive Services Labs&lt;/h4&gt;
&lt;p&gt;O Cognitive Services Labs é um ambiente para testar e dar feedback sobre as novas tecnologias sobre serviços cognitivos que estão sendo desenvolvidos pela Microsoft. Sendo assim você pode se inscrever para testar o que provavelmente serão os novos serviços a serem disponibilizados na plataforma Azure.&lt;/p&gt;

&lt;p&gt;A Microsoft já havia feio o mesmo por meio do projeto Oxford, que é precessor do Cognitive Services.&lt;/p&gt;

&lt;p&gt;Vale dar uma olhada nesse cara, aqui temos alguns serviços disruptivos na área, como por exemplo o Gesture API, doravante chamado de Project Prague, que cria experiências mais intuitivas e naturais, permitindo aos utilizadores controlar e interagir através de gestos.&lt;/p&gt;

&lt;p&gt;Você pode acessar o &lt;a href=&quot;https://labs.cognitive.microsoft.com/&quot;&gt;Cognitive Services Labs aqui!&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;azure-batch-ai-training&quot;&gt;Azure Batch AI Training&lt;/h4&gt;
&lt;p&gt;O &lt;a href=&quot;https://batchaitraining.azure.com/&quot;&gt;Azure Batch AI Training&lt;/a&gt; é fenomenal. Este serviço possibilita que os desenvolvedores possam treinar suas próprias redes neurais profundas, utilizando as principais plataformas, incluindo o Microsoft Cognitive Toolkit, TensorFlow e etc. Outro ponto aqui é que você pode simplesmente enviar seu código em um container Docker e deixar a Microsoft lidar com isso.&lt;/p&gt;

&lt;p&gt;O grande ganho aqui é poder utilizar a mesma infra-estrutura que a Microsoft usa para o seu desenvolvimento de AI por demanda. Na prática agora temos GPUs NVIDIA de ponta que serão utilizadas para o processamento. Agora não vamos mais ter que nos preocupar com a distribuição e paralelização deste processamento, jogando a responsabilidade para a nuvem.&lt;/p&gt;

&lt;p&gt;Vale notar que no Build, foi exibido que será bem simples levar esses modelos de aprendizagem profunda para onde estão os dados, usando o analytics integration fornecido pelo Azure Data Lake, Azure Cosmos DB ou SQL Server.&lt;/p&gt;

&lt;h4 id=&quot;bots&quot;&gt;Bots&lt;/h4&gt;
&lt;p&gt;Usando os novos Adaptive Cards ​​suportados pelo Microsoft Bot Framework, os desenvolvedores podem gravar cartões que ficam ótimos em vários aplicativos e plataformas. Outra novidade foi a possibilidade de publicar em novos canais, incluindo Bing, Cortana e Skype for Business, fora o poder de implementar a API de solicitação de pagamento da Microsoft para verificação rápida e fácil em seus bots.&lt;/p&gt;

&lt;p&gt;A Microsoft também atualizou o LUIS (Language Understanding Intelligent Service),a fim de oferecer um reconhecimento de fala mais preciso, sem falar no número crescente de entidades e intenções que o sistema pode reconhecer.&lt;/p&gt;

&lt;h4 id=&quot;cortana-skills-kit&quot;&gt;Cortana Skills Kit&lt;/h4&gt;
&lt;p&gt;O Cortana Skills Kit foi lançado em preview, e em suma oferece o poder de desenvolver habilidades para Cortana criando um bot e publicando-o no novo canal da Cortana Bot Framework. Isso está disponível no Windows 10, Android, iOS e no novo alto-falante Harman Kardon Invoke,que é powered by Cortana.&lt;/p&gt;

&lt;h4 id=&quot;presentation-translator&quot;&gt;Presentation Translator&lt;/h4&gt;
&lt;p&gt;Certamente outro recurso que chamou atenção: A tradução em “em tempo real” de uma apresentação realizada no PowerPoint. Certamente temos aqui a utilização de um serviço cognitivo, porém já é possível notar o movimento da Microsoft integrando AI em seu famoso pacote Office.&lt;/p&gt;

&lt;p&gt;Este serviço já está disponível para teste, basta apenas você se inscrever &lt;a href=&quot;https://www.aka.ms/PresentationTranslator&quot;&gt;neste formulário&lt;/a&gt; para ter a oportunidade de avaliar.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;http://meriatblob.blob.core.windows.net/images/2017/05/17/ppt-translator.png&quot; class=&quot;absolute-bg&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;cortana-speaker&quot;&gt;Cortana speaker&lt;/h4&gt;

&lt;p align=&quot;center&quot; style=&quot;background-color: black;&quot;&gt;&lt;img src=&quot;http://meriatblob.blob.core.windows.net/images/2017/05/17/cortana.png&quot; width=&quot;200&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Em relação a Cortana o que vimos neste Build é sobre adoção. Foi anunciado oficialmente o uso da Cortana como assistente para o controle por voz do &lt;a href=&quot;http://www.harmankardon.com/invoke.html&quot;&gt;Invoke&lt;/a&gt;, que até então iria utilizar o Alexa da Amazon. Este produto será lançado no quarto trimestre nos EUA, e por hora não tem nenhum detalhe em relação ao preço.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;http://meriatblob.blob.core.windows.net/images/2017/05/17/invoke.png&quot; class=&quot;absolute-bg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Também tivemos o anúncio da parceria fechada com a HP para a construção do seu &lt;strong&gt;smart speaker&lt;/strong&gt; utilizando a Cortana.&lt;/p&gt;

&lt;p&gt;Apesar de a Microsoft já ter ficado bem próxima de Apple e Google no desenvolvimento de assistentes digitais, as iniciativas recentes da Amazon e do Google para extender sua participação para as casas de seus usuários deixaram a Microsoft um pouco para trás. Mesmo agora, o Invoke será lançado apenas no final do ano, bem depois de seus concorrentes.&lt;/p&gt;

&lt;h4 id=&quot;new-ai-mvp-award-category&quot;&gt;New AI MVP Award Category&lt;/h4&gt;

&lt;p&gt;Firmando o compromisso da Microsoft com essas ações em AI, tivemos também o anúncio da nova categoria para o &lt;a href=&quot;https://mvp.microsoft.com/&quot;&gt;programa MVP&lt;/a&gt;, a categoria de AI.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;http://meriatblob.blob.core.windows.net/images/2017/05/17/award-mvp-ai.png&quot; class=&quot;absolute-bg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Como podemos ver acima essa área envolve do aprendizado de máquinas ao serviços cognitivos, ou seja, AI em toda a sua Glória ;)&lt;/p&gt;

&lt;div style=&quot;margin-bottom: 3em;&quot;&gt;&lt;/div&gt;
&lt;hr /&gt;

&lt;p&gt;Parte da estratégia da Microsoft é dar um “empurrão” a fim de trazer inteligência artificial para a computação mainstream. Em resumo, sobre o que vimos no Build, na visão da Microsoft é certo dizer:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Artificial Intelligence está saindo do Research &amp;amp; Development para ficar mais próximo dos desenvolvedores&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Resumindo o motivo de estamos ouvindo falar tanto sobre AI, podemos dizer que chegamos a esse ponto graças à convergência de três forças principais: O aumento da potência computacional que hoje está exponencial na nuvem, poderosos algoritmos que funcionam em redes neurais profundas e acesso a enormes quantidades de dados.&lt;/p&gt;

&lt;h2 id=&quot;project-oxford&quot;&gt;Project Oxford&lt;/h2&gt;
&lt;p&gt;O investimento da Microsoft em AI não é novo. Como citado por Harry Shum, o departamento de Research &amp;amp; Development tem realizados pesquisas voltadas para AI nos últimos 20 anos, o que pode ser bem notado se olharmos para o histórico.&lt;/p&gt;

&lt;p&gt;O projeto Oxford é a coleção da Microsoft de APIs para serviços de aprendizado de máquina. Antes de seu lançamento oficial, a Microsoft laçou o (que na época se tornou um viral) sua aplicação para “adivinhação de idade”, o How-Old.net. Poucas horas depois de ser lançado, o site reuniu centenas de milhares de submissões de imagens em que os usuários brincaram com as tentativas do site de descobrir as idades e os gêneros das pessoas nas fotos submetidas. É claro que isso ajudou a treinar o modelo de reconhecimento e analise de sentimento, tornando o serviço mais confiável.&lt;/p&gt;

&lt;p&gt;O projeto Oxfor iniciou com os seguintes serviços:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Face recognition&lt;/li&gt;
  &lt;li&gt;Speech processing&lt;/li&gt;
  &lt;li&gt;Visual tools&lt;/li&gt;
  &lt;li&gt;Language Understanding Intelligent Service (LUIS)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;deep-learning&quot;&gt;Deep Learning&lt;/h2&gt;
&lt;p&gt;Apesar do termo datar como recente para a maioria das pessoas, as pesquisas envolvendo redes neurais de N camadas para problemas complexos está no campo de pesquisa da Microsoft há decadas, e mais especificamente no área do reconhecimento de fala, como podemos ver neste paper da IEEE, datado de 2013: &lt;a href=&quot;http://ieeexplore.ieee.org/abstract/document/6639345/&quot;&gt;Recent advances in deep learning for speech research at Microsoft&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;microsoft-research&quot;&gt;Microsoft Research&lt;/h2&gt;

&lt;p&gt;Quando Bill Gates criou a Microsoft Research, em 1991, ele previa que os computadores um dia veriam, ouviriam e entenderiam os seres humanos – e essa perspectiva atraiu algumas das melhores mentes para os seus laboratórios, segundo a Microsoft.&lt;/p&gt;

&lt;p&gt;Vários projetos desenvolvidos no Microsoft Resarch serviram de inovação e base para novas tendências que viraram projetos de grande sucesso para a empresa. O sucesso dos bots, serviços cognitivos, cortana, HoloLens e compania sevem como base para novos projetos que hoje já sabemos ser bem aguardados como por exemplo, o Skype Translator.&lt;/p&gt;

&lt;p&gt;Em outubro de 2016, a Microsoft se tornou a primeira empresa do setor a chegar à paridade com humanos em reconhecimento de voz, segundo o anunciado por Harry Shum.&lt;/p&gt;

&lt;p&gt;A visão da Microsoft é realmente expandir, tanto que vemos anuncios como o da colaboração entre a Microsoft Research e o grupo OpenAI, que é uma empresa de pesquisa de AI sem fins lucrativos, e que é destaque por sua atuação e produção.&lt;/p&gt;

&lt;h2 id=&quot;referências&quot;&gt;Referências&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://blogs.microsoft.com/blog/2017/05/10/microsoft-build-2017-microsoft-ai-amplify-human-ingenuity/&quot;&gt;Resumo do anúncio realizado por Harry Shum&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.projectoxford.ai/&quot;&gt;Project Oxford&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://blogs.microsoft.com/blog/2016/11/15/advancing-ambition-democratize-artificial-intelligence/&quot;&gt;Advancing our ambition to democratize artificial intelligence&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Wed, 17 May 2017 00:00:00 -0300</pubDate>
        <link>http://localhost:4000/2017/05/17/microsoft-and-ai-for-the-future/</link>
        <guid isPermaLink="true">http://localhost:4000/2017/05/17/microsoft-and-ai-for-the-future/</guid>
        
        
        <category>IA</category>
        
        <category>Cognitive Computing</category>
        
        <category>Microsoft Azure</category>
        
        <category>Deep Learning</category>
        
      </item>
    
      <item>
        <title>Variáveis, Estatística e Macnhine Learning</title>
        <description>&lt;p&gt;A grosso modo podemos dividir a Estatística em três áreas:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Estatística Descritiva&lt;/li&gt;
  &lt;li&gt;Probabilidade&lt;/li&gt;
  &lt;li&gt;Inferência Estatística&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Estatística Descritiva é, em geral, utilizada na etapa inicial da análise, quando tomamos contato com os dados pela primeira vez.&lt;/p&gt;

&lt;p&gt;Probabilidade pode ser pensada como a teoria matemática utilizada para se estudar a incerteza oriunda de fenômenos de caráter aleatório.&lt;/p&gt;

&lt;p&gt;Inferência Estatística é o estudo de técnicas que possibilitam a extrapolação, a um grande conjunto de dados, das informações e conclusões obtidas a partir de subconjuntos de valores, usualmente de dimensão muito menor. [1]&lt;/p&gt;

&lt;p&gt;Falando em Machine Learning, sabemos que em algum determinado momento o pesquisador vai ser deparar com problemas para analisar e entender um determinado conjunto de dados que possa ser relevante aos seu objeto de estudo. Ele necessita trabalhar os dados a fim de transformá-los em informações que possam ser comparadas com outros resultados, ou ainda para julgar sua adequação a alguma teoria estipulada para o problema.&lt;/p&gt;

&lt;p&gt;Resumindo, a essência da Ciência é a observação e seu objetivo básico é a inferência que pode ser dedutiva ou indutiva. Quando falamos em estatística, a inferência estatística se trata da metodologia que objetiva a coleta, redução, análise e modelagem dos dados, a partir dos quais é possível fazer a inferência para uma população que nos permite chegar as &lt;strong&gt;previsões&lt;/strong&gt; e assim tomar &lt;strong&gt;decisões&lt;/strong&gt;. [2]&lt;/p&gt;

&lt;p&gt;Ao falar Machine Lerning falamos em estática, e para ambos existe um elemento fundamental (você já deve ter notado na descrição acima): Os &lt;strong&gt;&lt;u&gt;dados&lt;/u&gt;&lt;/strong&gt;. Quando olhamos para os dados precisamos notar o que há de mais especial, e geralmente conseguimos isso identificando nossas variáveis. Descobrir o porquê de uma determinada variação é um bom início para qualquer problema estatístico.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Uma variável é como uma pergunta que pode ter várias respostas possíveis.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Neste sentido vamos observar a tabela abaixo, que servirá de base para expressar os conceitos básicos que pretendo apresentar neste post:&lt;/p&gt;

&lt;table class=&quot;table-fill&quot;&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;Nome&lt;/th&gt;
      &lt;th&gt;Idade&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Machado de Assis&lt;/td&gt;
      &lt;td&gt;69&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Carlos Drummond de Andrade&lt;/td&gt;
      &lt;td&gt;85&lt;/td&gt;
    &lt;/tr&gt;
     &lt;tr&gt;
      &lt;td&gt;Graciliano Ramos&lt;/td&gt;
      &lt;td&gt;61&lt;/td&gt;
    &lt;/tr&gt;
     &lt;tr&gt;
      &lt;td&gt;Mário Quintana&lt;/td&gt;
      &lt;td&gt;38&lt;/td&gt;
    &lt;/tr&gt;
     &lt;tr&gt;
      &lt;td&gt;Guimarães Rosa&lt;/td&gt;
      &lt;td&gt;59&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;!-- &lt;div style=&quot;margin-bottom: 3em;&quot;&gt;&lt;/div&gt; --&gt;

&lt;p&gt;Em nosso contexto, vamos considerar as linhas como observações/recursos, e as colunas vamos chamar de variáveis. As perguntas que nossas variáveis respondem são: “Qual o seu nome?” e “Qual a sua idade?”.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Primeira dica: Apenas o título da variável não representa toda sua história.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Como assim? O que você quer dizer com isso?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Se olharmos para a variável idade, não sabemos se ela se refere a idade do escritor hoje, a idade no momento de algum escrito famoso ou se foi a idade que ele tinha na época de seu falecimento.&lt;/p&gt;

&lt;p&gt;Uma de nossas tarefas é investigar nossa fonte de dados em busca de compreender o verdadeiro significado de cada variável.&lt;/p&gt;

&lt;p&gt;Outro ponto importante é saber identificar uma variável. Se neste contexto você me perguntar meu nome, vou te responder &lt;strong&gt;Vitor&lt;/strong&gt;. Não importa quantas pessoas diferentes me perguntarem eu vou continuar respondendo Vitor. Neste caso meu nome não é uma variável, e sim uma constante.&lt;/p&gt;

&lt;p&gt;Seguindo essa linha vamos ver que uma variável depende da pergunta que está sendo feita. A resposta para &lt;u&gt;&quot;Qual a ordem em que os dias da semana ocorrem&quot;&lt;/u&gt; será uma constante. Sempre teremos a mesma ordem, terça sempre virá após a segunda e etc. Agora se pergunto &lt;u&gt;&quot;Que dia é hoje&quot;&lt;/u&gt;, vamos ter como resposta uma variável, já que a resposta tem relação direta com o dia em que a pergunta é feita. &lt;u&gt;&quot;Que dia hoje&quot;&lt;/u&gt; pode ser, segunda, terça, quarta… ou seja, teremos &lt;strong&gt;várias&lt;/strong&gt; possíveis respostas corretas.&lt;/p&gt;

&lt;h2 id=&quot;variáveis-numéricas-e-categóricas&quot;&gt;Variáveis numéricas e categóricas&lt;/h2&gt;

&lt;p&gt;Podemos diferenciar variáveis pelo tipo de dado. Em estatística consideramos dois tipos de variáveis: &lt;strong&gt;Númericas&lt;/strong&gt; e &lt;strong&gt;Categóricas&lt;/strong&gt;.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;http://meriatblob.blob.core.windows.net/images/2017/04/20/variables.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Variáveis podem ser classificadas da seguinte forma:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Variáveis Numéricas&lt;/strong&gt;: são as características que podem ser medidas em uma escala quantitativa, ou seja, apresentam valores numéricos que fazem sentido. Podem ser contínuas ou discretas.
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Variáveis discretas&lt;/strong&gt;: características mensuráveis que podem assumir apenas um número finito ou infinito contável de valores e, assim, somente fazem sentido valores inteiros. Geralmente são o resultado de contagens. Exemplos: número de filhos, número de bactérias por litro de leite, número de cigarros fumados por dia.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Variáveis contínuas&lt;/strong&gt;, características mensuráveis que assumem valores em uma escala contínua (na reta real), para as quais valores fracionais fazem sentido. Usualmente devem ser medidas através de algum instrumento. Exemplos: peso (balança), altura (régua), tempo (relógio), pressão arterial, idade.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Variáveis Categóricas&lt;/strong&gt;: são as características que não possuem valores quantitativos, mas, ao contrário, são definidas por várias categorias, ou seja, representam uma classificação dos indivíduos. Podem ser nominais ou ordinais.
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Variáveis nominais&lt;/strong&gt;: não existe ordenação dentre as categorias. Exemplos: sexo, cor dos olhos, fumante/não fumante, doente/sadio.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Variáveis ordinais&lt;/strong&gt;: existe uma ordenação entre as categorias. Exemplos: escolaridade (1o, 2o, 3o graus), estágio da doença (inicial, intermediário, terminal), mês de observação (janeiro, fevereiro,…, dezembro).&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;blockquote&gt;
  &lt;p&gt;As distinções são menos rígidas do que a descrição acima nos apresenta.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Sendo assim, podemos definir que se o resultado a pergunta for numérica &lt;u&gt;(&quot;Qual a sua idade?&quot;)&lt;/u&gt;, então temos uma variável numérica. Se a reposta não pode ser representada de forma númerica &lt;u&gt;(&quot;Qual a raça do seu cachorro?&quot;)&lt;/u&gt;, então temos uma variável categórica.&lt;/p&gt;

&lt;p&gt;Uma variável originalmente numérica pode ser coletada de forma categórica.
Por exemplo, a variável idade, medida em anos completos, é quantitativa (contínua); mas, se for informada apenas a faixa etária (0 a 5 anos, 6 a 10 anos, etc…), é qualitativa (ordinal). Outro exemplo é o peso dos lutadores de boxe, uma variável quantitativa (contínua) se trabalhamos com o valor obtido na balança, mas qualitativa (ordinal) se o classificarmos nas categorias do boxe (peso-pena, peso-leve, peso-pesado, etc.).&lt;/p&gt;

&lt;p&gt;Outro ponto importante é que nem sempre uma variável representada por números é quantitativa/numérica.&lt;/p&gt;

&lt;p&gt;O número do telefone de uma pessoa, o número da casa, o número de sua identidade. Às vezes o sexo do indivíduo é registrado na planilha de dados como 1 se macho e 2 se fêmea, por exemplo. Isto não significa que a variável sexo passou a ser quantitativa!&lt;/p&gt;

&lt;h3 id=&quot;variáveis-ordinais&quot;&gt;Variáveis Ordinais&lt;/h3&gt;
&lt;p&gt;Geralmente vemos isso nos formulários e questionários que muitas vezes somos obrigados a responder. Geralmente essas pesquisar vem com opções como “Discordo Fortemente”, “Discordo”, “Neutro”, “Concordo” ou “Concordo Plenamente”. Estes dados têm uma estrutura especial, uma vez que refletem uma hierarquia, onde 0 representa o item de valor mais baixo, e 4 representa o item de valor mais alto.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;0 = Discordo Totalmente&lt;/li&gt;
  &lt;li&gt;1 = Discordo&lt;/li&gt;
  &lt;li&gt;2 = Neutro&lt;/li&gt;
  &lt;li&gt;3 = Concordo&lt;/li&gt;
  &lt;li&gt;4 = Concordo Totalmente&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Um primeiro cuidado em relação a codificação numérica é nunca destuir a hierarquia real dos dados. Se fizermos como está abaixo, nosso trabalho estará destruído.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;0 = Discordo Totalmente&lt;/li&gt;
  &lt;li&gt;2 = Discordo&lt;/li&gt;
  &lt;li&gt;1 = Neutro&lt;/li&gt;
  &lt;li&gt;4 = Concordo&lt;/li&gt;
  &lt;li&gt;5 = Concordo Totalmente&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;variáveis-nominais&quot;&gt;Variáveis Nominais&lt;/h3&gt;
&lt;p&gt;Às vezes não há hierarquia em dados categóricos. Se a cor dos olhos foi codificada 0 “Azul” 1 “Verde” 2 “Castanhos”, temos que escolher aleatoriamente qual opção recebe qual número. Não importa se os olhos azuis são zero, ou um, ou dois, porque não há hierarquia na cor dos olhos.&lt;/p&gt;

&lt;h3 id=&quot;variáveis-discretas&quot;&gt;Variáveis Discretas&lt;/h3&gt;
&lt;p&gt;Todas as variáveis ​​contínuas são numéricas, mas  nem todas as variáveis ​​numéricas são contínuas.&lt;/p&gt;

&lt;p&gt;“Quantas crianças você tem?” Não tem um número infinito de respostas, tem um número finito ou “discreto”. Você não pode ter 2,7 filhos, é 2 ou 3. Você pode estar pensando “Ok - números inteiros significa variável discreta”, mas isso é uma armadilha. E quanto à variável “tamanho do sapato”? Este é muitas vezes um número como 6,5 ou 10,5, mas não há um número infinito de tamanhos de sapato que existem, que seria o fim da fabricação de calçados como sabemos!&lt;/p&gt;

&lt;p&gt;As variáveis ​​discretas não precisam de codificação porque são numéricas&lt;/p&gt;

&lt;h3 id=&quot;variáveis-contínuas&quot;&gt;Variáveis Contínuas&lt;/h3&gt;
&lt;p&gt;Este conceito é difícil, mas você vai ficar bem porque já definimos o conceito de uma variável como resposta a uma pergunta .&lt;/p&gt;

&lt;p&gt;Algumas perguntas têm um monte de respostas  . Se você perguntar a 100 pessoas “Qual é o número da rua de sua casa?”, Você pode obter perto de 100 respostas diferentes. Isso é um pouco irritante, mas não vai quebrar seu software estatístico.&lt;/p&gt;

&lt;p&gt;Algumas perguntas têm um número infinito de  respostas . Literalmente, não existem números suficientes para capturar todas as possibilidades. Pode surpreender você saber quais variáveis ​​se enquadram nesta categoria; Como altura, peso e idade.&lt;/p&gt;

&lt;p&gt;Isto é porque 1,543 metros não é o mesmo que 1,5429 metros. Estes são números diferentes. 1.54299 é diferente novamente. Assim é 1.542999 metros. Acho que você vê o que estou dizendo, há um número ilimitado de números disponíveis para nós para representar a altura de alguém. O mesmo é verdade para peso e idade (e pressão arterial, e um monte de outras medições médicas). Na prática, estamos presos com um número mais limitado de opções, mas isso não muda o fato de que a própria variável tem possibilidades infinitas. Por favor, faça uma pergunta sobre isso nos comentários se você está confuso. Uma boa regra é que quase todas as medições são contínuas.&lt;/p&gt;

&lt;p&gt;Quem se importa se uma variável é contínua? Você não precisa se importar muito frequentemente. Mas quando começamos a falar sobre a probabilidade de pesos e alturas particulares, esse detalhe teórico se torna extremamente importante.&lt;/p&gt;

&lt;p&gt;As variáveis ​​contínuas não exigem codificação, pois elas são sempre numéricas.&lt;/p&gt;

&lt;h2 id=&quot;codificando-variáveis&quot;&gt;Codificando Variáveis&lt;/h2&gt;

&lt;p&gt;Eu não posso analisar diretamente uma sentença como &lt;u&gt;&quot;Hoje é terça-feira&quot;&lt;/u&gt; em termos estatísticos. Quando nossa variável não é númerica, precisamos transformar cada resposta à nossa pergunta em um número, para que o mesmo possa ser analisado. Este processo de conversão é chamado de “codificação”. Como codificar uma variável vai depender do seu tipo de dado.&lt;/p&gt;

&lt;p&gt;Vou escrever um post específico sobre este conteúdo.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;referências&quot;&gt;Referências&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;[1]&lt;/strong&gt; Noções de Probabilidade e Estatística - 6ª Edição Revista e Ampliada - Magalhães,Marcos Nascimento / Lima,Antonio Carlos Pedroso de Edusp&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;[2]&lt;/strong&gt; Estatística Básica - 8ª Edição Bussab,Wilton de Oliveira / Morettin,Pedro Alberto&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Por hoje é só galera. Em breve devo soltar mais conteúdo direcionado as bases do Machine Learning.&lt;/p&gt;
</description>
        <pubDate>Thu, 20 Apr 2017 00:00:00 -0300</pubDate>
        <link>http://localhost:4000/2017/04/20/variables-in-statistic/</link>
        <guid isPermaLink="true">http://localhost:4000/2017/04/20/variables-in-statistic/</guid>
        
        
        <category>Statistics</category>
        
        <category>Data Science</category>
        
      </item>
    
      <item>
        <title>Hello World R - Da base à Regressão Linear</title>
        <description>&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://meriatblob.blob.core.windows.net/images/2017/04/14/capa-logo-r.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Antes de iniciar vale notar que além da minha impressão com a linguagem, documentei aqui alguns pontos que me fizeram atentar ao R, bem como alguns pontos de atenção que devem ser levados em consideração principalmente se você estiver pensando em realizar algum projeto com R.&lt;/p&gt;

&lt;p&gt;A linguagem R é um projeto GNU de software livre. O &lt;strong&gt;R&lt;/strong&gt; derivou de uma linguagem chamada &lt;strong&gt;S&lt;/strong&gt; (de “statistics”), criada na &lt;strong&gt;Bell Laboratories&lt;/strong&gt; nos anos 70. Esta é uma linguagem usada por programadores e cientistas de dados para computação estatística.&lt;/p&gt;

&lt;p&gt;R foi desenvolvida por estatísticos para estatística, dado seu foco na resolução de problemas matemáticos, ele possui um estrutura simples o que facilita a implementação e traz ganhos na otimização no tempo de desenvolvimento. Você pode obter todas as referncias na documentação oficial em &lt;a href=&quot;https://www.rdocumentation.org/&quot;&gt;rdocumentation.org&lt;/a&gt;, fora que é possível contar com um riquíssimo ecossistema de pacotes para diferentes áreas de aplicação.&lt;/p&gt;

&lt;h2 id=&quot;por-quê-eu-devo-considerar-o-r&quot;&gt;Por quê eu devo considerar o R?&lt;/h2&gt;

&lt;p&gt;Se trata de uma linguagem free (cran.r-project.org), altamente extensível e hoje já conta com mais de 10.000 pacotes na &lt;a href=&quot;http://crantastic.org/packages&quot;&gt;CRAN&lt;/a&gt;, e o melhor, todos orientados a estatística, aprendizado de máquina e suas técnicas.&lt;/p&gt;

&lt;p&gt;R tem crescido como linguagem e na preferência de utilização. Abaixo segue o ranking da &lt;a href=&quot;http://www.ieee.org&quot;&gt;IEEE&lt;/a&gt; de 2016 para “Top Programming Languages”.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;http://meriatblob.blob.core.windows.net/images/2017/04/14/spectrum-ieee.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Se tomarmos como o &lt;a href=&quot;https://www.kaggle.com&quot;&gt;Kaggle&lt;/a&gt; como referência, vemos que a maioria dos competidores preferem usar R para a criação dos modelos.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;http://meriatblob.blob.core.windows.net/images/2017/04/14/kaggle-tools.png&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Para começar vamos criar um simples exemplo de regressão linear, que neste caso serve como uma espécie de &lt;strong&gt;“Hello World”&lt;/strong&gt; da computação estatística. Então, mãos a obra…&lt;/p&gt;

&lt;h2 id=&quot;regressão-linear&quot;&gt;Regressão Linear&lt;/h2&gt;

&lt;p&gt;A regressão linear é uma técnica estatística usada para descrever a relação entre uma variável numérica (na estatística, chamada de variável dependente) e uma ou mais variáveis explicativas (chamadas de variáveis independentes) que podem ser numéricas ou categóricas. Quando há apenas uma única variável explicativa/de previsão, a técnica é chamada de regressão linear simples. Quando houver duas ou mais variáveis independentes, a técnica é chamada de regressão linear múltipla.&lt;/p&gt;

&lt;p&gt;Para iniciar vamos começar importando o arquivo de texto que será utilizado no exemplo.&lt;/p&gt;

&lt;pre style=&quot;font-size: 1.6em !important&quot;&gt;
    &lt;code class=&quot;r&quot;&gt;
  data &amp;lt;- read.table(&quot;sementes.txt&quot;, header=TRUE, sep=&quot;,&quot;)
    &lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;No trecho de código acima estamos utilizando a importação de um &lt;strong&gt;TXT&lt;/strong&gt; como DataSet. Na operação que estamos utilizando, podemos observar que a linguagem &lt;strong&gt;R&lt;/strong&gt; usa parâmetros nomeados. O parâmetro de cabeçalho informa se a primeira linha é de cabeçalho ou não. O parâmetro &lt;strong&gt;sep&lt;/strong&gt; (separador) indica como os valores em cada linha são separados. Por exemplo, (\t) indica valores delimitados por tabulação, e (“ “) indica valores delimitados por espaço.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;O R diferencia maiúsculas de minúsculas e você geralmente pode usar o operador (&amp;lt;-) para atribuir valores, ou o operador (=). Essa escolha é uma questão de preferência pessoal, já que ambos exercem a mesma função. Tipicamente, a boa prática aponta para o uso do operador (&amp;lt;-) na atribuição de objetos e (=) para atribuição de valores de parâmetro.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Para se trabalhar com o &lt;strong&gt;CSV&lt;/strong&gt;, formato mais comun nestes casos, seria tão simples quanto passar o caminho do arquivo.&lt;/p&gt;

&lt;pre style=&quot;font-size: 1.6em !important&quot;&gt;
    &lt;code class=&quot;r&quot;&gt;
  data &amp;lt;- read.csv(&quot;sementes.csv&quot;)
    &lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;Vamos olhar a estrutura de nosso arquivo. Se trata de uma listagem de sementes classificada por cor, comprimento, largura e índice de nutrientes. Nosso objetivo aqui é prever os valores da coluna nutrientes com base nos dados anteriores da semente.&lt;/p&gt;

&lt;p&gt;Uma vez que temos nosso arquivo carregado, podemos utilizar alguns comandos a fim de vizualizar o nosso conjunto de dados.&lt;/p&gt;

&lt;pre style=&quot;font-size: 1.6em !important&quot;&gt;
    &lt;code class=&quot;r&quot;&gt;
  # Exibe o shape dos dados
  dim(data)

  # Concatena duas strings
  paste(&quot;Linhas: &quot;, dim(data)[1], sep=&quot; &quot;)
  paste(&quot;Colunas: &quot;, dim(data)[2], sep=&quot; &quot;)

  # Exibe o dados
  print(data)
    &lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;A primeira função exibe a estrutura básica dos dados, retornando a quantidade de linhas e colunas do dataset. Notem que estou utilizando a função &lt;strong&gt;paste()&lt;/strong&gt; para concatenar duas strings. Aqui podemos ver que o resultado pode ser lido como um &lt;strong&gt;array&lt;/strong&gt;, onde tenho na posição 1 o primeiro item, e na posição 2 o segundo item. Depois apenas nomeio qual é a coluna e qual é a linha. A próxima função irá exibir os dados como extraído do arquivo. Usamos a função &lt;strong&gt;print()&lt;/strong&gt; para exibir qualquer saída para o nosso programa.&lt;/p&gt;

&lt;p&gt;Nossa saída seria algo como o que se segue abaixo:&lt;/p&gt;

&lt;pre style=&quot;font-size: 1.2em !important&quot;&gt;
    &lt;code class=&quot;prolog&quot;&gt;
  #OUTPUT
  
  8 4

  'Linhas:  8'
  'Colunas:  4'

     Color Length Width Nutrients
  1   blue    5.4   1.8       0.9
  2   blue    4.8   1.5       0.7
  3   blue    4.9   1.6       0.8
  4 yellow    5.0   1.9       0.4
  5 yellow    5.2   1.5       0.3
  6 yellow    4.7   1.9       0.4
  7  green    3.7   2.2       1.4
  8  green    4.2   1.9       1.2
    &lt;/code&gt;
&lt;/pre&gt;

&lt;blockquote&gt;
  &lt;p&gt;Observe que a saída acima exibe os índices do &lt;strong&gt;array de dados, começando em 1&lt;/strong&gt;. Este é um ponto importante a se notar, já que na maioria das linguagens estamos acostumados com os índices de &lt;strong&gt;base 0&lt;/strong&gt;, como em C# ou Python.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Agora vamos executar a análise da função de &lt;strong&gt;Modelo Linear&lt;/strong&gt;. Para isso execute os comandos abaixo:&lt;/p&gt;

&lt;pre style=&quot;font-size: 1.4em !important&quot;&gt;
    &lt;code class=&quot;r&quot;&gt;
  model &amp;lt;- lm(data$Nutrients ~ (data$Color + data$Length + data$Width))

  summary(model)
    &lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;Neste trecho de código temos a variável &lt;strong&gt;model&lt;/strong&gt; que é o resultado da nossa análise de regressão linear. Aqui estamos usando a função &lt;strong&gt;lm()&lt;/strong&gt; &lt;strong&gt;(linear model)&lt;/strong&gt;, na qual temos a &lt;strong&gt;variável dependente&lt;/strong&gt; a ser prevista (data$Nutrients), e as variáveis independentes, a saber: &lt;strong&gt;data$Color, data$Length, data$Width&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;O próximo commando &lt;strong&gt;(summary(model))&lt;/strong&gt;, vai exibir o resultádo básico formatado da análise que foi armazenda no objeto model. Teremos a seguinte saída para o código acima:&lt;/p&gt;

&lt;pre style=&quot;font-size: 1.2em !important&quot;&gt;
    &lt;code class=&quot;r&quot;&gt;
  Call:
  lm(formula = data$Nutrients ~ (data$Color + data$Length + data$Width))

  Residuals:
          1         2         3         4         5         6         7         8
   0.009418 -0.030030  0.020611 -0.028320  0.044164 -0.015844  0.042596 -0.042596

  Coefficients:
                   Estimate Std. Error t value Pr(&amp;gt;|t|)   
  (Intercept)      -0.14758    0.48286  -0.306  0.77986   
  data$Colorgreen   0.35672    0.09990   3.571  0.03754 *
  data$Coloryellow -0.49083    0.04507 -10.891  0.00166 **
  data$Length       0.04159    0.07876   0.528  0.63406   
  data$Width        0.45200    0.11973   3.775  0.03255 *
  ---
  Signif. codes:  0 ‘\***\’ 0.001 ‘\**\’ 0.01 ‘\*\’ 0.05 ‘.’ 0.1 ‘ ’ 1

  Residual standard error: 0.05179 on 3 degrees of freedom
  Multiple R-squared:  0.9927,	Adjusted R-squared:  0.9829
  F-statistic: 101.6 on 4 and 3 DF,  p-value: 0.00156
    &lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;Olhando para o nosso conjunto de dados responda a seguinte pergunta: Qual seria o valor de &lt;u&gt;Nutrientes&lt;/u&gt; para uma determinada semente se a &lt;u&gt;cor&lt;/u&gt; da mesma fosse &lt;u&gt;yellow&lt;/u&gt; e suas &lt;u&gt;largura&lt;/u&gt; e &lt;u&gt;altura&lt;/u&gt; fossem &lt;u&gt;1.9&lt;/u&gt; e &lt;u&gt;4.7&lt;/u&gt; respectivamente?&lt;/p&gt;

&lt;p&gt;E ai? Já sabe a resposta?&lt;/p&gt;

&lt;p&gt;Olhando para o nosso dataset é fácil de responder esta pergunta. Estamos falando do item &lt;u&gt;[6]&lt;/u&gt; de nossa lista. Sendo assim sabemos que o valor de nutrientes será &lt;u&gt;0.4&lt;/u&gt;.&lt;/p&gt;

&lt;p&gt;Agora queremos &lt;u&gt;prever&lt;/u&gt; este valor usando nosso modelo. Como faremos isso?&lt;/p&gt;

&lt;p&gt;Vamos olhar para o resultado da análse que fizemos anteriormente. Em Coefficients vamos utilizar apenas a primeira e segunda colunas. Temos aqui nossas variáveis independetes e suas estimativas.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Vale notar que temos uma constante chamada (Intercept), não associada a qualquer variável.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Quando você tem variáveis explicativas categóricas, um dos valores é descartado. Em nosso caso o valor escolhido foi o blue.&lt;/p&gt;

&lt;p&gt;Para fazer uma previsão usando o modelo, é necessário calcular a soma linear dos valores das estimativas multiplicados pelos valores correspondentes em relação as variáveis independentes. Em nosso caso a equação seria:&lt;/p&gt;

&lt;pre style=&quot;font-size: 1.2em !important&quot;&gt;
    &lt;code class=&quot;r&quot;&gt;
  -0.14758 + (0.35672 * 0) + (-0.49083 * 1) + (0.04159 * 4.7) + (0.45200 * 1.9)
    &lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;Vamos entender como chegar neste resultado. Com base nos quadro abaixo fica mais simples compreender:&lt;/p&gt;

&lt;table class=&quot;table-fill&quot;&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;NA&lt;/th&gt;
      &lt;th&gt;yellow&lt;/th&gt;
      &lt;th&gt;green&lt;/th&gt;
      &lt;th&gt;Length&lt;/th&gt;
      &lt;th&gt;Width&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;-0.14758&lt;/td&gt;
      &lt;td&gt;0.35672&lt;/td&gt;
      &lt;td&gt;-0.49083&lt;/td&gt;
      &lt;td&gt;0.04159&lt;/td&gt;
      &lt;td&gt;0.45200&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;!-- &lt;div style=&quot;margin-bottom: 3em;&quot;&gt;&lt;/div&gt; --&gt;

&lt;p&gt;Primeiro listamos os valores da coluna Estimate. Depois para cada valor, multiplicamos a variável correspondente. No caso do primeiro valor estamos falando da constante Intercept, logo não existe valor a ser multiplicado. Para o segundo valor, temos como resultado &lt;u&gt;(0.35672 * 0)&lt;/u&gt;. É o sugundo valor multiplicado por 0, já que nossa previsão não inclui a variável green. Multiplicamos o valor de data$Length, 0.04159 por 4.7 e temos (0.04159 * 4.7). A mesma lógica para data$Width e temos (0.45200 * 1.9).&lt;/p&gt;

&lt;p&gt;O resultado será 0.415863, um valor muito próximo de 0.40, que é o valor real descrito no arquivo.&lt;/p&gt;

&lt;p&gt;Na saída acima temos uma sessão, iniciada com &lt;u&gt;Residual standard error&lt;/u&gt;. Aqui temos a relação entre as variáveis independentes e a variável dependente.&lt;/p&gt;

&lt;pre style=&quot;font-size: 1.2em !important&quot;&gt;
    &lt;code class=&quot;r&quot;&gt;
  Residual standard error: 0.05179 on 3 degrees of freedom
  Multiple R-squared:  0.9927,	Adjusted R-squared:  0.9829
  F-statistic: 101.6 on 4 and 3 DF,  p-value: 0.00156
    &lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;O valor de &lt;u&gt;Multiple R-squared (0,9927)&lt;/u&gt; é a porcentagem de variação na variável dependente explicada pela combinação linear das variáveis independentes. Neste caso os valores de R-squared são valores entre 0 e 1, onde os valores mais altos significam um modelo de previsão melhor. Em nosso caso, R-squared tem um valor extremamente alto, indicando que Color, Length e Width podem prever o resultado com boa precisão. F-statistic, Adjusted R-squared e p-value são outras medidas de ajuste do modelo.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Bom, nem tudo é céu azul e arco-íris… Existem alguns pontos que devem ser levantados em relação a utilização do R.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;r-é-single-threaded&quot;&gt;R é single-threaded&lt;/h2&gt;
&lt;p&gt;Temos aqui um ponto interessante. R por padrão é single-threaded, ou seja, é executado apenas por um único segmento na CPU. Isso pode ser limitante já que mesmo usando R em uma VM na nuvem com 64 núcleos de CPU, o R só usará um deles.&lt;/p&gt;

&lt;p&gt;Para ilustrar: Você deseja encontrar a soma de um vetor numérico, esta é uma operação que pode ser executada em paralelo na CPU com bastante facilidade. Se houver quatro núcleos de CPU disponíveis, cada núcleo pode receber cerca de um quarto dos dados a serem processados. Cada núcleo calcula o subtotal do pedaço de dados que é dado, e os quatro subtotais são adicionados para cima para encontrar a soma total do conjunto de dados inteiro.&lt;/p&gt;

&lt;p&gt;No entanto, em R, se usarmo a função &lt;strong&gt;sum()&lt;/strong&gt;, ela será executada em série, processando todo o conjunto de dados em um núcleo de CPU. De fato, muitas operações de Big Data são de natureza semelhante a função &lt;strong&gt;sum()&lt;/strong&gt;, com a mesma tarefa executando independentemente em muitos subconjuntos de dados. Sendo assim, realizar tais operações sequencialmente seria uma subutilização das arquiteturas de computação em paralelo, e se falarmos do poder da nuvem seria mais disperdício ainda.&lt;/p&gt;

&lt;p&gt;É possível escrever programas em R para um melhor desempenho com foco em computação paralela, mas seria melhor a linguagem ter essa premissa de forma nativa.&lt;/p&gt;

&lt;h2 id=&quot;data-stored-in-memory&quot;&gt;Data stored in memory&lt;/h2&gt;
&lt;p&gt;Todos os dados processados em R tem de ser &lt;strong&gt;totalmente carregados na RAM&lt;/strong&gt;. Isso significa que uma vez que os dados foram carregados, tudo isso está disponível para processamento pela CPU, o que é ótimo para o desempenho. Por outro lado, isso também significa que o tamanho máximo de dados que você pode processar depende da quantidade de RAM livre disponível em seu sistema. Lembre-se de que nem toda a RAM do seu computador está disponível para o uso do R. O sistema operacional, os processos em segundo plano e quaisquer outros aplicativos que estão sendo executados na CPU também competem pela RAM. O que está disponível para R usar pode ser uma fração da RAM total instalada no sistema.&lt;/p&gt;

&lt;p&gt;Durante o Workshop, &lt;u&gt;Microsoft R for Data Science&lt;/u&gt; no qual eu participei, usamos uma máquina no azure com 468GB de RAM para hospedar o R Server e ser utilizado por 12 pessoas. Fora isso eventualmente era necessário olhar o desempenho da mesma já que algumas execuções se tornaram pesadas.&lt;/p&gt;

&lt;p&gt;Frequentemente usamos comandos como &lt;strong&gt;mem_used()&lt;/strong&gt; ou &lt;strong&gt;object_size(myDF)&lt;/strong&gt; para monitorar nosso uso das capacidades computacionais.&lt;/p&gt;

&lt;p&gt;Além disso, R também requer &lt;strong&gt;RAM livre para armazenar&lt;/strong&gt; os resultados de seus cálculos. Dependendo do tipo de computação que você for executar, talvez seja necessário que a RAM disponível seja duas vezes ou mais vezes maior do que o tamanho de seus dados. Fora isso as versões de &lt;strong&gt;32 bits&lt;/strong&gt; de R também são limitadas pela quantidade de RAM que podem acessar. Dependendo do sistema operacional, eles podem ser limitados a &lt;strong&gt;2GB a 4GB de RAM&lt;/strong&gt;, mesmo quando há realmente mais RAM disponível.&lt;/p&gt;

&lt;div style=&quot;margin-bottom: 3em; margin-top: 2em;&quot;&gt;

Aparentemente tanto o problema de **single-threaded** como a questão do **data stored in memory** já estão na lista do [R Consortium](https://www.r-consortium.org/), então vamos esperar que em um futuro próximo tenhamos melhores estratégias em R.
&lt;/div&gt;

&lt;h2 id=&quot;essential-open-source-packages&quot;&gt;Essential Open Source Packages&lt;/h2&gt;
&lt;p&gt;Alguns dos principais pacotes em R que você deve conhecer:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Data Management: &lt;code&gt;dplyr&lt;/code&gt;, &lt;code&gt;tidyr&lt;/code&gt;, &lt;code&gt;data.table&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Visualization: &lt;code&gt;ggplot2&lt;/code&gt;, &lt;code&gt;ggvis&lt;/code&gt;, &lt;code&gt;htmlwidgets&lt;/code&gt;, &lt;code&gt;shiny&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Data Importing: &lt;code&gt;haven&lt;/code&gt;, &lt;code&gt;RODBC&lt;/code&gt;, &lt;code&gt;readr&lt;/code&gt;, &lt;code&gt;foreign&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Other favorites: &lt;code&gt;magrittr&lt;/code&gt;, &lt;code&gt;rmarkdown&lt;/code&gt;, &lt;code&gt;caret&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;mrs---mircrosoft-r-server&quot;&gt;MRS - Mircrosoft R Server&lt;/h2&gt;
&lt;p&gt;Essa é uma menção honrosa ao &lt;strong&gt;MRS&lt;/strong&gt; ou &lt;strong&gt;Mircrosoft R Server&lt;/strong&gt;. Vou escrever um post específico sobre este cara, mas com o MRS podemos realizar operações Multi-Threading, trabalhar com Parallel Processing e utilizar o recurso para remover limitação de dados apenas na RAM, fazendo uma combinação de RAM e Disco.&lt;/p&gt;

&lt;h2 id=&quot;impressões&quot;&gt;Impressões&lt;/h2&gt;
&lt;p&gt;No geral minha primeira impressão da linguagem &lt;u&gt;&lt;b&gt;R&lt;/b&gt;&lt;/u&gt; foi muito boa. Achei uma sintaxe limpa, fácil e bem direcionada ao seu propósito. Já tinha em mente que esta seria uma linguagem orientada a estatística computacional, mas esperava algo mais rebuscado, diferente da facilidade de compreensão que tive.&lt;/p&gt;

&lt;h2 id=&quot;conclusão&quot;&gt;Conclusão&lt;/h2&gt;
&lt;p&gt;Minha ideia aqui não era fazer um comparativo, ou muito menos explicar os principais comandos da linguagem. A documentação por si só já é eficaz, e ainda existe uma infinidade de materiais sobre este tema. Preferi aqui fazer uma analise da minha impressão ao implementar uma técnica básica como a regressão linear, além de buscar alguns dos benefícios e possíveis problemas ao optar por utilizar R em futuros projetos.&lt;/p&gt;

&lt;h2 id=&quot;referências&quot;&gt;Referências&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.rdocumentation.org/&quot;&gt;rdocumentation.org&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Linear_regression&quot;&gt;Linear Regression&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.r-consortium.org/&quot;&gt;R Consortium&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;R High Performance Programming - Aloysius Lim, William Tjhi&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://spectrum.ieee.org/static/interactive-the-top-programming-languages-2016&quot;&gt;IEEE Ranking 2016&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.kaggle.com/wiki/Software&quot;&gt;Kaggle Tools Used By Competitors&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.revolutionanalytics.com/&quot;&gt;Revolutions Blog&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.rstudio.com/resources/cheatsheets/&quot;&gt;RStudio Cheat Sheets&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.r-bloggers.com/&quot;&gt;R-Bloggers&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://blogs.msdn.microsoft.com/microsoftrservertigerteam/&quot;&gt;Microsoft R Server Tiger Blog&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Fri, 14 Apr 2017 00:00:00 -0300</pubDate>
        <link>http://localhost:4000/2017/04/14/hello-world-r/</link>
        <guid isPermaLink="true">http://localhost:4000/2017/04/14/hello-world-r/</guid>
        
        
        <category>R</category>
        
        <category>Data Science</category>
        
      </item>
    
      <item>
        <title>Kaggle Competition - Titanic: Machine Learning from Disaster</title>
        <description>&lt;p&gt;&lt;img style=&quot;width: 100%;&quot; src=&quot;http://blob.vitormeriat.com.br/images/2017/03/18/capa.jpg&quot; class=&quot;absolute-bg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;O &lt;strong&gt;Projeto Titanic&lt;/strong&gt; é uma competição de &lt;strong&gt;Data Science&lt;/strong&gt; promovida pelo &lt;a href=&quot;kaggle.com&quot;&gt;Kaggle.com&lt;/a&gt;. O objetivo deste desafio é deduzir os índices de sobrevivência dos passageiros do Titanic.&lt;/p&gt;

&lt;p&gt;Meu objetivo aqui é resumir de forma simples este problema e como chegar a sua solução, aproveitando para introduzir alguns dos conceitos envolvido no tão falado &lt;strong&gt;Machine Learning&lt;/strong&gt;. Não vou me aprofundar em nenunhum ponto, espero embreve fazê-lo com mais tempo e foco.&lt;/p&gt;

&lt;p&gt;Este é um dos problemas iniciais favoritos dos estudantes de Data Science, e tem um dataset com a complexidade ideal para explorar alguns problemas que funcionam de pano de fundo para introduzir os conceitos de Machine Learning mais básicos.&lt;/p&gt;

&lt;h2 id=&quot;agenda&quot;&gt;Agenda&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Introdução&lt;/li&gt;
  &lt;li&gt;Os dados&lt;/li&gt;
  &lt;li&gt;Descobrindo Padrões&lt;/li&gt;
  &lt;li&gt;Exploratory Data Analysis
    &lt;ul&gt;
      &lt;li&gt;Missing Data
        &lt;ul&gt;
          &lt;li&gt;Embarked&lt;/li&gt;
          &lt;li&gt;Fare&lt;/li&gt;
          &lt;li&gt;Age&lt;/li&gt;
          &lt;li&gt;Name&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Feature Engineering
        &lt;ul&gt;
          &lt;li&gt;Title&lt;/li&gt;
          &lt;li&gt;Cabin&lt;/li&gt;
          &lt;li&gt;Surname&lt;/li&gt;
          &lt;li&gt;Family Size&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Data Transformation&lt;/li&gt;
      &lt;li&gt;Feature Selection&lt;/li&gt;
      &lt;li&gt;Feature selection&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introdução&quot;&gt;Introdução&lt;/h2&gt;

&lt;p&gt;Para quem assiste os &lt;a href=&quot;https://www.youtube.com/watch?v=JVgkvaDHmto&quot;&gt;Mythbusters&lt;/a&gt;, sabe que em um de seus programas o assunto do Titanic foi mais do explorado. Não como iremos fazer aqui, já que o foco era provar que era possível Jack e Rose terem sobrevivido utilizando a porta flutuante.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Nosso foco entretanto, é medir a chance real de sobrevivência dos passageiros a bordo do Titanic em 14 de abril de 1912.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;A história do Titanic é muito conhecida, tendo originado diversos livros, filmes, HQs e afins. É válido lembrar que a história narrada de forma célebre por James Cameron em seu filme de 1997, ilustra perfeitamente o motivo deste desafio. Vamos começar com uma breve perspectiva sobre o tema: Em Abril de 1912 o Titanic zarpou rumo a New York com &lt;code&gt;2224&lt;/code&gt; passageiros, dos quais estima-se que apenas &lt;code&gt;710&lt;/code&gt; tenham sobrevivido.&lt;/p&gt;

&lt;p&gt;É aqui que entra nosso desafio: Desenvolver um padrão simples para identificar o perfil dos sobreviventes deste desastre.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Nota: Muitos dos barcos salva-vidas não estavam com a sua capacidade máxima de pessoas a bordo. Se estivessem, seria possível salvar &lt;code&gt;53,4%&lt;/code&gt; dos passageiros, mas apenas &lt;code&gt;31,6%&lt;/code&gt; deles sobreviveram.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Neste caso não temos complicação com elementos aleatórios de sorte. A maioria dos sobreviventes eram mulheres, crianças e pessoas da alta sociedade.&lt;/p&gt;

&lt;h2 id=&quot;os-dados&quot;&gt;Os dados&lt;/h2&gt;

&lt;p&gt;A lista de sobreviventes e não sobreviventes foi transformada em dois datasets, &lt;strong&gt;train.csv&lt;/strong&gt; e &lt;strong&gt;test.csv&lt;/strong&gt;. Existe apenas uma diferença entre os dois arquivos, a lista de status de sobrevivência que está presente apenas nos dados de &lt;strong&gt;treino&lt;/strong&gt;. Nos dados de teste este valor precisa ser deduzido.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Todos os aprendizados começam com uma experiência e são um processo contínuo. Raramente tem um ponto final. Todos os aprendizados são apenas uma aproximação.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Geralmente trabalhamos um experimento utilizando dois conjuntos de dados. Um de treino e um para o teste, tipicamente dividido em 80% ou 70% para treino e e restante para o teste.&lt;/p&gt;

&lt;p&gt;A aprendizagem acontece usando o &lt;code&gt;Conjunto A&lt;/code&gt; e a validação acontece usando o &lt;code&gt;Conjunto B&lt;/code&gt;. Uma vez que a aprendizagem é aperfeiçoada, aplicamos nosso modelo nos dados de teste. Vale lembrar que não temos o conhecimento do estado de sobrevivência do passageiro nos dados do teste.&lt;/p&gt;

&lt;p&gt;Neste desafio vamos utilizar os dados disponíveis no próprio site do Kaggle para desenvolver nosso modelo, a fim de prever as taxas de sobrevivência para os passageiros do Titanic.&lt;/p&gt;

&lt;p&gt;O conjunto de teste conta com 418 passageiros e o conjunto de treinamento é composto por 891 passageiros. O dataset é composto por:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;PassengerId&lt;/code&gt;: Número de identificação do passageiro;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;Survived&lt;/code&gt;: Indica se o passageiro sobreviveu ao desastre. É atribuído o valor de 0 para aqueles que não sobreviveram, e 1 para quem sobreviveu;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;Pclass&lt;/code&gt;: Classe na qual o passageiro viajou. É informado 1 para primeira classe; 2 para segunda; e 3 para terceira;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;Name&lt;/code&gt;: Nome do passageiro;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;Sex&lt;/code&gt;: Sexo do passageiro;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;Age&lt;/code&gt;: Idade do passageiro em anos;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;SibSp&lt;/code&gt;: Quantidade de irmãos e cônjuges a bordo;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;Parch&lt;/code&gt;: Quantidade de pais e filhos a bordo;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;Ticket&lt;/code&gt;: Número da passagem;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;Fare&lt;/code&gt;: Preço da passagem;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;Cabin&lt;/code&gt;: Número da cabine do passageiro;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;Embarked&lt;/code&gt;: Indica o porto no qual o passageiro embarcou. Há apenas três valores possíveis: Cherbourg, Queenstown e Southampton, indicados pelas letras &lt;strong&gt;C&lt;/strong&gt;, &lt;strong&gt;Q&lt;/strong&gt; e &lt;strong&gt;S&lt;/strong&gt;, respectivamente.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Como na maioria das competições Kaggle, você recebe dois conjuntos de dados:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Um conjunto de treino completo com o resultado (outcome ou target variable), para um grupo de passageiros, bem como uma coleção de outros parâmetros, como sua idade, sexo, etc. Este é o conjunto de dados em que você deve treinar seu modelo preditivo.&lt;/li&gt;
  &lt;li&gt;Um conjunto de teste, para o qual você deve prever a variável de destino agora desconhecida com base nos outros atributos de passageiros que são fornecidos para ambos os conjuntos de dados.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Este é um ótimo ponto de entrada para aprender machine learning com um conjunto de dados pequeno, gerenciável, interessante e com variáveis ​​de fácil compreensão.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;http://blob.vitormeriat.com.br/images/2017/03/18/ml-pattern.png&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;descobrindo-padrões&quot;&gt;Descobrindo padrões&lt;/h1&gt;

&lt;p&gt;A tragédia do “Titanic” está associada com a regra não escrita do salvamento no mar: “mulheres e crianças primeiro”.&lt;/p&gt;

&lt;p&gt;Nesta tarefa, os competidores precisam analisar a probabilidade de sobrevivência das diferentes categorias de passageiros.&lt;/p&gt;

&lt;p&gt;Vamos estipular o sexo dos passageiros e tripulantes. Este é um  dado importante, uma vez que o sexo do passageiro desempenha um papel crucial para determinar a sobrevivência do mesmo. Um simples count revela que temos 891 passageiros dos quais 65% são homens e 35% são mulheres.&lt;/p&gt;

&lt;table class=&quot;table-fill&quot;&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;Gender&lt;/th&gt;
      &lt;th&gt;Count of Passengers&lt;/th&gt;
      &lt;th&gt; % &lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;female&lt;/td&gt;
      &lt;td&gt;314&lt;/td&gt;
      &lt;td&gt;35%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;male&lt;/td&gt;
      &lt;td&gt;577&lt;/td&gt;
      &lt;td&gt;65%&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Se adicionarmos a taxa de sobrevivência a equação vamos ver que existe um padrão aqui.&lt;/p&gt;

&lt;table class=&quot;table-fill&quot;&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;Count of Passengers&lt;/th&gt;
      &lt;th&gt;Survived Status&lt;/th&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt; &lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;background: #63dd69; color: white;&quot;&gt;Gender&lt;/td&gt;
      &lt;td style=&quot;background: #63dd69; color: white;&quot;&gt;Not Survived&lt;/td&gt;
      &lt;td style=&quot;background: #63dd69; color: white;&quot;&gt;Survived&lt;/td&gt;
      &lt;td style=&quot;background: #63dd69; color: white;&quot;&gt;Total&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;female&lt;/td&gt;
      &lt;td&gt;81&lt;/td&gt;
      &lt;td&gt;233&lt;/td&gt;
      &lt;td&gt;314&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;male&lt;/td&gt;
      &lt;td&gt;468&lt;/td&gt;
      &lt;td&gt;109&lt;/td&gt;
      &lt;td&gt;577&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;background: #777777; color: white;&quot;&gt;Total&lt;/td&gt;
      &lt;td style=&quot;background: #777777; color: white;&quot;&gt;549&lt;/td&gt;
      &lt;td style=&quot;background: #777777; color: white;&quot;&gt;342&lt;/td&gt;
      &lt;td style=&quot;background: #777777; color: white;&quot;&gt;891&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;O seguinte pode ser observado a partir dos dados acima:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;62% (549/891) dos passageiros não sobreviveram&lt;/li&gt;
  &lt;li&gt;38% (342/891) sobreviveram&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Dos 38% que sobreviveram, os passageiros do sexo feminino sobreviveram em maior número maior que os passageiros do sexo masculino.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;De todos os passageiros do sexo feminino (233/314) 74% sobreviveram&lt;/li&gt;
  &lt;li&gt;De todos os passageiros do sexo masculino (109/577), apenas 19% sobreviveram&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Podemos ilustrar este comportamento usando uma árvore de decisão. Cada nó determina o resultado final com base na maioria. Os votos do nó à direita representam o status de sobrevivência, já que a maioria nesta opção sobreviveu. Os votos do à esquerda representam o status de não sobrevivência, uma vez que a maioria não sobreviveu.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;http://blob.vitormeriat.com.br/images/2017/03/18/ml-pattern-all.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Este é nosso primeiro padrão. Par facilitar vamos conceituar algumas coisas: O dataset de treino é chamado de &lt;strong&gt;labelled dataset&lt;/strong&gt;, onde a coluna &lt;u&gt;Survived&lt;/u&gt; é chamda de “label” ou &lt;strong&gt;response data&lt;/strong&gt;. Esta abordagem é chamada de aprendizagem supervisionada, onde aprendemos com os dados de treino e aplicamos este aprendizado nos dados de teste. O resultado desta abordagem é binária, portanto estamos aplicando uma técnica de classificação, utilizando duas classes, a saber, &lt;strong&gt;survived&lt;/strong&gt; ou &lt;strong&gt;not survived&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Vamos nos referir ao conjunto de dados progressivamente ao longo deste post, começar a descobrir o conceito de aprendizagem da máquina e suas dimensões variadas. Como você pode observar a partir dos dados, os passageiros e seus atributos compõem os dados do Titanic.&lt;/p&gt;

&lt;div style=&quot;margin-bottom: 5em; margin-top: 4em; background-color: #dcbc14; color: #382d2d&quot;&gt;
&lt;p style=&quot;padding: 1.6em; font-family: courier;&quot;&gt;
Um ponto importante sobre este tutorial: Não estou colocando todo o passo a passo da solução. Aqui não temos os passos para importação de dados e etc. Estou focando na solução técnica para a criação do modelo e resolução do problema. Para ver a solução completa que inclusive enviei para o Kaggle, você deve ver o código que está na sessão entitulada, &lt;u&gt;&lt;b&gt;O repositório&lt;/b&gt;&lt;/u&gt;
&lt;/p&gt;
&lt;/div&gt;

&lt;hr style=&quot;margin-bottom: 3em;&quot; /&gt;

&lt;h2 id=&quot;exploratory-data-analysis&quot;&gt;Exploratory Data Analysis&lt;/h2&gt;

&lt;p&gt;Em nossa fase de &lt;strong&gt;análise exploratória de dados&lt;/strong&gt;, precisamos conhecer nossos dados e compreender sua qualidade para conseguir realizar nossa predição.&lt;/p&gt;

&lt;p&gt;Neste ponto vamos combinar os dados de teste de treino antes de realizar qualquer tipo de análise exploratória. Este propósito ficará mais claro em breve. Por hora vamos combinar os 2 conjuntos de dados, e para que isso seja possível precisamos criar uma coluna adicional no conjunto de teste chamado &lt;code&gt;Survived&lt;/code&gt; e marcá-los todos com &lt;code&gt;-1&lt;/code&gt;, a fim de que sejamos capazes de distinguir em um ponto posterior no tempo os dados de teste. Também há uma necessidade de reorganizar as colunas nos dados de treino para refletir o posicionamento dos dados de teste.&lt;/p&gt;

&lt;pre style=&quot;font-size: 1.2em !important&quot;&gt;
    &lt;code class=&quot;python&quot;&gt;
 testTitanicDS.is_copy = False
 testTitanicDS['Survived']=-1
 trainTitanicDS =  trainTitanicDS[['PassengerId','Pclass','Name','Sex','Age','SibSp','Parch','Ticket','Fare','Cabin','Embarked','Survived']]
 mergedTitanicDS = trainTitanicDS.append(testTitanicDS)
    &lt;/code&gt;
&lt;/pre&gt;

&lt;h2 id=&quot;missing-data&quot;&gt;Missing Data&lt;/h2&gt;

&lt;p&gt;A falta de dados deve ser tratada antes de se aplicar qualquer algoritmo de aprendizado de máquina. Isso faz sentido já que se tivermos dados faltando em alguma linha, nosso resultado pode ser afetado. No código abaixo vamos procurar e exibir os dados de valores missing caso eles existam em nosso dataset.&lt;/p&gt;

&lt;pre style=&quot;font-size: 1.2em !important&quot;&gt;
    &lt;code class=&quot;python&quot;&gt;
 for col in mergedTitanicDS:
    if mergedTitanicDS[col].isnull().sum()&amp;gt;0:
        print(&quot;Missing Values in %s %d&quot; % (col,(mergedTitanicDS[col].isnull().sum())))
    &lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;Como resultado temos a seguinte saída:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;***** output *****

Missing Values in Age 263
Missing Values in Fare 1
Missing Values in Cabin 1014
Missing Values in Embarked 2
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Sendo assim já sabemos que existem muitos valores missing em nosso dataset. Uma vez que os identificamos o próximo passo é examinar para entender qual o motivo ou causa para esse comportamento. Em Machine Learning, os dados são essenciais, e como tal, devemos olhar com toda atenção.&lt;/p&gt;

&lt;p&gt;Existem diversas técnicas para lidar como valores missing. Uma delas seria eliminar toda a linha e trabalhar somente com as linhas contendo dados completos. Em nosso caso vamos atacar cada um dos recursos em falta e definir qual estratégia usar. Vamos iniciar pelo mais simples.&lt;/p&gt;

&lt;h4 id=&quot;embarked&quot;&gt;Embarked&lt;/h4&gt;

&lt;p&gt;Podemos nota que existem apenas 2 valores missing para Embarked. Vamos observar sua distribuição:&lt;/p&gt;

&lt;pre style=&quot;font-size: 1.2em !important&quot;&gt;
    &lt;code class=&quot;python&quot;&gt;
 mergedTitanicDS['Embarked'].value_counts()
    &lt;/code&gt;
&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;***** output *****

S    914
C    270
Q    123
Name: Embarked, dtype: int64
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Bem vindo as variáveis categóricas. Temos pouca massa de manobra aqui. Uma estratégia consistente neste caso poderia ser substituir os valores missing pelo valor de &lt;code&gt;S&lt;/code&gt;, já que o mesmo supera tanto &lt;code&gt;C&lt;/code&gt; quanto &lt;code&gt;Q&lt;/code&gt; em termos de númericos. Com esta abordagem estamos influenciando muito pouco no resultado final.&lt;/p&gt;

&lt;pre style=&quot;font-size: 1.2em !important&quot;&gt;
    &lt;code class=&quot;python&quot;&gt;
 mergedTitanicDS.is_copy = False
 mergedTitanicDS.loc[mergedTitanicDS['Embarked'].isnull(),'Embarked'] = 'S'
    &lt;/code&gt;
&lt;/pre&gt;

&lt;h4 id=&quot;fare&quot;&gt;Fare&lt;/h4&gt;

&lt;p&gt;Há apenas um passageiro com dados missing sobre a tarifa. O passageiro em questão é senhor &lt;strong&gt;“Storey, Mr. Thomas”&lt;/strong&gt;. Se olharmos para os dados do senhor Thomas, veremos que ele viajou na classe &lt;code&gt;Pclass 3&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Neste caso podemos usar como estratégia, obter a média da tarifa paga pelos passageiros da classe &lt;strong&gt;Pclass 3&lt;/strong&gt;.&lt;/p&gt;

&lt;pre style=&quot;font-size: 1em !important&quot;&gt;
    &lt;code class=&quot;python&quot;&gt;
 mergedTitanicDS_Merged.loc[
 	mergedTitanicDS_Merged['Fare'].isnull(),'Fare'] = 
 	mergedTitanicDS_Merged[mergedTitanicDS_Merged['Pclass'] == 3]['Fare'].mean()
    &lt;/code&gt;
&lt;/pre&gt;

&lt;div style=&quot;margin-bottom: 4em; margin-top: 4em; background-color: #dcbc14; color: #382d2d&quot;&gt;
&lt;p style=&quot;padding: 1.6em; font-family: courier;&quot;&gt;
&lt;b style=&quot;font-size: 1.8em&quot;&gt;Think outside the box&lt;/b&gt;&lt;br /&gt;Esta é uma fase muito importante. Geralmente você vai se deparar com situações durante o tratamento de dados em que vai parecer impossível realziar um tratamento. Um exemplo disso em nosso projeto é a feature &lt;u&gt;&lt;b&gt;AGE&lt;/b&gt;&lt;/u&gt;
&lt;/p&gt;
&lt;/div&gt;

&lt;h4 id=&quot;age&quot;&gt;Age&lt;/h4&gt;

&lt;p&gt;A idade é um ponto de dados contínuo. Há &lt;code&gt;263&lt;/code&gt; valores missing, o que representa cerca de &lt;code&gt;19%&lt;/code&gt; dos dados. Existem várias estratégias para substituição da idade, o mais simples é substituir por uma média, mediana ou moda.&lt;/p&gt;

&lt;p&gt;A &lt;strong&gt;armadilha&lt;/strong&gt; nesta estratégia é que todos os passageiros sem idade terão o mesmo valor estático, o que pode resultar em uma criança com a idade de um adulto, sendo assim, essa abordagem irá afetar diretamente no resultado final, tornando nossa privisão falha.&lt;/p&gt;

&lt;p&gt;Como resolver este problema de forma satisfatória? Podemos começar olhando para o nosso dataset a fim de achar algo que nos ajude…&lt;/p&gt;

&lt;h4 id=&quot;name&quot;&gt;Name&lt;/h4&gt;

&lt;p&gt;Opa… &lt;strong&gt;Name&lt;/strong&gt; não possui valores missing, então o que está fazendo aqui? A questão é o que esse recurso tem a nos oferecer para resolver o problema das idades. Vamos lá…&lt;/p&gt;

&lt;p&gt;Olhando para o este recurso veremos que juntamente com o nome temos os títulos. Os títulos nos dizem muito, já que podemos induzir se a pessoa é mais jovem ou mais madura com base neles.&lt;/p&gt;

&lt;p&gt;Vamos introduzir uma nova técnica para chegar nisso, chamamos essa técnica de &lt;strong&gt;feature engineering&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 id=&quot;feature-engineering&quot;&gt;Feature Engineering&lt;/h2&gt;

&lt;p&gt;A &lt;strong&gt;engenharia de recurso&lt;/strong&gt; tenta aumentar a capacidade de previsão dos algoritmos de aprendizado criando recursos de dados brutos que facilitam o processo de aprendizado. Basicamente esse processo tenta criar outros recursos relevantes com base nos recursos brutos existentes nos dados e aumentar a capacidade de previsão do algoritmo de aprendizado.&lt;/p&gt;

&lt;p&gt;Vamos continuar na análise dos nossos recursos agora aplicando um exemplo de feature engineering.&lt;/p&gt;

&lt;h4 id=&quot;title&quot;&gt;Title&lt;/h4&gt;

&lt;p&gt;Como vimos, temos junto aos nomes os &lt;strong&gt;títulos de tratamento&lt;/strong&gt; para cada pessoa. Sendo assim podemos agregar a nossa estratégia de idade a indução da mesma com base no título.&lt;/p&gt;

&lt;p&gt;Neste caso vamos extrair o título do nome e encontrar a média entre idade e o título informado, e depois substituir os &lt;strong&gt;valores missing&lt;/strong&gt; pela média correspondente. Essa estratégia serve para ajudar a evitar substituir o valor missing de uma possível criança pela idade de um adulto. Assim diminuímos a margem de erro no tratamento de dados missing, e obtemos um resultado final eficiente.&lt;/p&gt;

&lt;p&gt;Extrair e cruzar os dados de idade com os dados do título de treinamento é um exemplo de &lt;strong&gt;Feature Engineering&lt;/strong&gt;, que visa tornar o conjunto de dados mais rico para o aprendizado.&lt;/p&gt;

&lt;p&gt;Traduzindo isso em código, nossa primeira tarefa será criar uma coluna chamada &lt;strong&gt;Title&lt;/strong&gt; e extrair o valor do título de tratamento para inserir o valor nela.&lt;/p&gt;

&lt;pre style=&quot;font-size: 1.2em !important&quot;&gt;
    &lt;code class=&quot;python&quot;&gt;
 mergedTitanicDS['Title'] = [nameStr[1].strip().split('.')[0] for nameStr in
 mergedTitanicDS['Name'].str.split(',')]
    &lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;Para visualizar a importância desse novo recurso, vamos olhar a distribuição deste dado. No código abaixo geramos uu gráfico que exibe uma visão interessante da distribuição dos títulos distintos em nosso dataset. Alguns dos títulos refletem Realeza, como por exemplo Jonkheer, que é título de nobreza holandesa.&lt;/p&gt;

&lt;pre style=&quot;font-size: 1.2em !important&quot;&gt;
    &lt;code class=&quot;python&quot;&gt;
 mergedTitanicDS['Title'].value_counts().plot.bar()
    &lt;/code&gt;
&lt;/pre&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;http://blob.vitormeriat.com.br/images/2017/03/18/feature engineering-title.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Sendo assim podemos analizar melhor nossa decisão. Como a grande maioria dos dados são representados pelo título &lt;code&gt;Mr&lt;/code&gt;, e temos 18 títulos diferentes, sendo assim já sabemos de cara que seria um erro ter uma média generalizada.&lt;/p&gt;

&lt;p&gt;Vamos traduzir isso para código. Abaixo temos os 4 passos para o nosso objetivo:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Agregamos a idade pelo título e aplicamos a média;&lt;/li&gt;
  &lt;li&gt;Como será gerada uma nova coluna, vamos renomeá-la para Title;&lt;/li&gt;
  &lt;li&gt;Realizamos o merge da nova coluna em nosso dataset;&lt;/li&gt;
  &lt;li&gt;Substituímos os valores missing pela média de cada título.&lt;/li&gt;
&lt;/ol&gt;

&lt;pre style=&quot;font-size: 1.2em !important&quot;&gt;
    &lt;code class=&quot;python&quot;&gt;
 # 1
 aggAgeByTitleDS = mergedTitanicDS[['Age','Title']].groupby(['Title']).mean().reset_index()

 # 2
 aggAgeByTitleDS.columns = ['Title','Mean_Age']

 # 3
 mergedTitanicDS_Merged = pd.merge(mergedTitanicDS, aggAgeByTitleDS,on=&quot;Title&quot;)

 # 4
 mergedTitanicDS_Merged.loc[mergedTitanicDS_Merged['Age'].isnull(),'Age']=mergedTitanicDS_Merged[mergedTitanicDS_Merged['Age'].isnull()]['Mean_Age']
    &lt;/code&gt;
&lt;/pre&gt;

&lt;h4 id=&quot;cabin&quot;&gt;Cabin&lt;/h4&gt;

&lt;p&gt;Vamos evoluir em nossa análise. A feature &lt;strong&gt;cabin&lt;/strong&gt; parece ser o maior problema, ele é parece ser realmente complexo. Para começar ele represente mais de 70% dos dados missing em nosso dataset.&lt;/p&gt;

&lt;p&gt;Não há muito o que os dados da cabine possam oferecer em relação ao nosso problema. Nada que revele uma predisposição a sobrevivência ou não, algo como a relação de proximidade com o assidente ou coisa do tipo. Como não temos informação adicional ou externa sobre este recurso, uma estratégia que podemos utilizar é criar um novo recurso indicando a existência ou não da cabine.&lt;/p&gt;

&lt;pre style=&quot;font-size: 1.2em !important&quot;&gt;
    &lt;code class=&quot;python&quot;&gt;
 mergedTitanicDS['IsCabinDataEmpty'] = 0
 mergedTitanicDS.loc[mergedTitanicDS['Cabin'].isnull(),'IsCabinDataEmpty'] = 1
    &lt;/code&gt;
&lt;/pre&gt;

&lt;h4 id=&quot;surname&quot;&gt;Surname&lt;/h4&gt;

&lt;p&gt;Outro dado que podemos extrair e que será de extrema importância é o sobrenome do passageiro. Com este dado em mãos podemos realizar uma combinação posterior para inferir as famílias que estavam no Titanic. Podemos resolver isso com apenas uma linha de código.&lt;/p&gt;

&lt;pre style=&quot;font-size: 1.2em !important&quot;&gt;
    &lt;code class=&quot;python&quot;&gt;
 mergedTitanicDS_Merged['Surname'] = [nameStr[0].strip() for 
 	nameStr in mergedTitanicDS_Merged['Name'].str.split(',')]
    &lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;Agora, qual a importância dessa ação? Aqui vai mais uma dica: Procure avaliar cada nova &lt;code&gt;feature&lt;/code&gt; possível. Isso é de suma importância, já que posteriormente conseguimos avaliar se existe ou não alguma relação deste novo recurso e nossa meta.&lt;/p&gt;

&lt;p&gt;Agora já temos o sobrenome, mas isso pode acabar sendo outra armadilha, uma vez que é possível termos passageiros com o mesmo sobrenome e que não sejam da mesma família. Para isso vamos tentar melhorar nossa combinação de dados…&lt;/p&gt;

&lt;h4 id=&quot;family-size&quot;&gt;Family Size&lt;/h4&gt;

&lt;p&gt;Algo que pode nos ajudar nesse sentido é descobrir o tamanho da família. Udsando as features &lt;code&gt;SibSp&lt;/code&gt; que representa a quantidade de irmãos e cônjuges, e &lt;code&gt;Parch&lt;/code&gt; que representa a quantidade de pais e filhos, podemos inferir nosso &lt;strong&gt;Family Size&lt;/strong&gt;.&lt;/p&gt;

&lt;pre style=&quot;font-size: 1.2em !important&quot;&gt;
    &lt;code class=&quot;python&quot;&gt;
 mergedTitanicDS_Merged['FamilySize'] = 
 	mergedTitanicDS_Merged['SibSp'] + mergedTitanicDS_Merged['Parch'] + 1
    &lt;/code&gt;
&lt;/pre&gt;

&lt;h2 id=&quot;data-transformation&quot;&gt;Data Transformation&lt;/h2&gt;

&lt;p&gt;Antes de prosseguirmos, vamos avalidar todos os recursos que criamos e o que já tinhamos a nossa disposição:&lt;/p&gt;

&lt;pre style=&quot;font-size: 1.4em !important&quot;&gt;
    &lt;code class=&quot;python&quot;&gt;
 mergedTitanicDS_Merged.dtypes
    &lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;O resultado será o seguinte:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;PassengerId           int64
Pclass                int64
Name                 object
Sex                  object
Age                 float64
SibSp                 int64
Parch                 int64
Ticket               object
Fare                float64
Cabin                object
Embarked             object
Survived              int64
IsCabinDataEmpty      int64
Title                object
Mean_Age            float64
Surname              object
FamilySize            int64
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Observe os campos cujo o tipo são &lt;code&gt;object&lt;/code&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Name&lt;/li&gt;
  &lt;li&gt;Sex&lt;/li&gt;
  &lt;li&gt;Ticket&lt;/li&gt;
  &lt;li&gt;Cabin&lt;/li&gt;
  &lt;li&gt;Embarked&lt;/li&gt;
  &lt;li&gt;Title&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Para realizarmos nosso algorítimo de forma satisfatória, precisamos “adequar” nossos dados. A priori já sabemos que para trabalhar com estatística consideramos dois tipos de dados: &lt;strong&gt;numérico&lt;/strong&gt; ou &lt;strong&gt;categórico&lt;/strong&gt;.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Nota&lt;/strong&gt;: Uma descrição detalhada pode ser encontrada em meu post &lt;a href=&quot;http://www.vitormeriat.com.br/2017/04/20/variables-in-statistic/&quot;&gt;Variáveis, Estatística e Macnhine Learning&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Sendo assim vamos aplicar alguma transformação para estes dados a fim de torná-los compativeis que nosso algorítimo.&lt;/p&gt;

&lt;h4 id=&quot;sex-embarked-and-title&quot;&gt;Sex, Embarked and Title&lt;/h4&gt;

&lt;p&gt;Temos apenas dois tipos possíveis para sexo: Masculino e Feminino. Esta é uma tranformação simples, já que se trata de uma opção binária. Seguindo esta linha vamos &lt;code&gt;codificar&lt;/code&gt; essa feature para que consigamos realizar uma representação onde para cara linha tenhamos uma coluna &lt;code&gt;male&lt;/code&gt; e &lt;code&gt;female&lt;/code&gt; onde necessáriamente um será positivo (1) e o outro negativo (0).&lt;/p&gt;

&lt;pre style=&quot;font-size: 1.4em !important&quot;&gt;
    &lt;code class=&quot;python&quot;&gt;
 pd.get_dummies(mergedTitanicDS_Merged['Sex'])
    &lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;Teremos como resultado a seguinte saída:&lt;/p&gt;

&lt;p&gt;FIG sex&lt;/p&gt;

&lt;p&gt;O mesmo conseito pode ser aplicado para &lt;strong&gt;Embarked&lt;/strong&gt; e &lt;strong&gt;Title&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;FIG embarked&lt;/p&gt;

&lt;p&gt;FIG title&lt;/p&gt;

&lt;p&gt;Utilizando o &lt;strong&gt;Pandas&lt;/strong&gt; vamos realizar a transformação dos dados categóricos em tipos que possam ser expressos de forma númerica. Logo após fazemos o merge das novas informações em nosso &lt;strong&gt;DataFrame&lt;/strong&gt;.&lt;/p&gt;

&lt;pre style=&quot;font-size: 1.2em !important&quot;&gt;
    &lt;code class=&quot;python&quot;&gt;
 mergedTitanicDS_Merged = pd.concat([mergedTitanicDS_Merged,
 	pd.get_dummies(mergedTitanicDS_Merged['Embarked'])],axis = 1)
 
 mergedTitanicDS_Merged = pd.concat([mergedTitanicDS_Merged,
 	pd.get_dummies(mergedTitanicDS_Merged['Sex'])],axis = 1)
 	
 mergedTitanicDS_Merged = pd.concat([mergedTitanicDS_Merged,
 	pd.get_dummies(mergedTitanicDS_Merged['Title'])],axis = 1)
    &lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;Agora uma coisa pode parecer parecer estranha neste processo. Estamos incluindo um grande número de recursos, o que geralmente é bom para nos dar mais massa de manobra na exploração de relações com nosso alvo. O efeito colateral é que também pode trazer confusão se não for algo bem elaborado.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Nota&lt;/strong&gt;: Variáveis categóricas são conhecidas por ocultar e mascarar muitas informações interessantes em um conjunto de dados. É crucial aprender os métodos de lidar com essas variáveis.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;feature-selection&quot;&gt;Feature Selection&lt;/h2&gt;

&lt;p&gt;A seleção de recursos também é chamada seleção de variáveis (variable selection) ​​ou seleção de atributos (attribute selection). Esta técnica se resume em saber selecionar quis os atributos em seus dados são realmente relevantes para a modelagem preditiva que você está construindo.&lt;/p&gt;

&lt;p&gt;Sendo assim, vamos analisar o que temos de atributos, e ver se precisamos ou não de todos eles para a construção do nosso modelo.&lt;/p&gt;

&lt;hr style=&quot;margin-bottom: 4em; margin-top: 6em;&quot; /&gt;

&lt;h1 id=&quot;o-repositório&quot;&gt;O repositório&lt;/h1&gt;

&lt;p&gt;Você pode olhar o código completo acessando o mesmo no repositório &lt;strong&gt;Meriat Machine Learning Notes&lt;/strong&gt; no meu Github.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/vitormeriat/meriat-ml-notes/blob/master/Titanic%20-%20Machine%20Learning%20from%20Disaster.ipynb&quot;&gt;Titanic: Machine Learning from Disaster&lt;/a&gt;&lt;/p&gt;

&lt;div style=&quot;margin-bottom: 5em;&quot;&gt;&lt;/div&gt;

&lt;h1 id=&quot;referências&quot;&gt;Referências&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.kaggle.com/c/titanic&quot;&gt;Titanic: Machine Learning from Disaster&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://trevorstephens.com/kaggle-titanic-tutorial/getting-started-with-r/&quot;&gt;Titanic: Getting Started With R&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sat, 18 Mar 2017 00:00:00 -0300</pubDate>
        <link>http://localhost:4000/2017/03/18/titanic-machine-learning-from-disaster/</link>
        <guid isPermaLink="true">http://localhost:4000/2017/03/18/titanic-machine-learning-from-disaster/</guid>
        
        
        <category>IA</category>
        
        <category>Machine Learning</category>
        
        <category>Data Science</category>
        
        <category>Deep Learning</category>
        
        <category>Jupyter Notebook</category>
        
        <category>Python</category>
        
      </item>
    
      <item>
        <title>Apresentando o Meriat Machine Learning Notes</title>
        <description>&lt;p&gt;&lt;img style=&quot;width: 100%;&quot; src=&quot;http://blob.vitormeriat.com.br/images/2017/02/15/capa.jpg&quot; class=&quot;absolute-bg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;No último ano tive a oportunidade de trabalhar em projetos envolvendo IoT e Machine Learning. No processo entre aplicação, estudos e desenvolvimento, acabei separando algum material e exemplos que me ajudaram a inicar nesta carreira, bem como no processo de aprendizagem.&lt;/p&gt;

&lt;p&gt;O Meriat Machine Learning Notes nada mais é que um compendio destes estudos que resolvi compartilhar no github utilizando o Jupyter Notebook.&lt;/p&gt;

&lt;p&gt;Minha opção pelo &lt;strong&gt;Jupyter Notebook&lt;/strong&gt;, dentre tantos motivos, se dá pelo fato de conseguir de forma simples documentar e visualizar todo o código e anotações de forma simples. Vale dizer que o código que estou utilizando pode ser facilmente utilizado em outra IDE, conforme sua preferência.&lt;/p&gt;

&lt;p&gt;Algumas implementações são reprodução de exercícios dos quais eu irei apontar o material base.&lt;/p&gt;

&lt;p&gt;O material ainda está em formação, visto que ainda tenho um longo caminho a percorrer. É possível que futuramente eu altere o repositório, já que alguns estudos podem gerar um repositório separado.&lt;/p&gt;

&lt;p&gt;Por hora vou concentrando meus esforços por aqui ;)&lt;/p&gt;

&lt;h3 id=&quot;github&quot;&gt;Github&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;Study Notes on machine learning, data analysis, algorithms and best practices using Python and Jupyter Notebook.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/vitormeriat/meriat-ml-notes&quot;&gt;Meriat Machine Learning Note&lt;/a&gt;&lt;/p&gt;

&lt;div style=&quot;margin-bottom: 8em;&quot;&gt;&lt;/div&gt;

&lt;h3 id=&quot;meriat-machine-learning-note&quot;&gt;Meriat Machine Learning Note&lt;/h3&gt;

&lt;p&gt;A estrutura atual está descrita logo abaixo. Vou me esforçar para manter atualizada.&lt;/p&gt;

&lt;h5 id=&quot;file-handling&quot;&gt;File Handling&lt;/h5&gt;
&lt;p&gt;&lt;code&gt;TBA&lt;/code&gt;&lt;/p&gt;

&lt;h5 id=&quot;sms-spam-filtering&quot;&gt;SMS Spam Filtering&lt;/h5&gt;
&lt;p&gt;&lt;code&gt;TBA&lt;/code&gt;&lt;/p&gt;

&lt;h5 id=&quot;song-recommender&quot;&gt;Song Recommender&lt;/h5&gt;
&lt;p&gt;&lt;code&gt;TBA&lt;/code&gt;&lt;/p&gt;

&lt;h5 id=&quot;basic-math&quot;&gt;Basic Math&lt;/h5&gt;
&lt;p&gt;Basic math notions with python&lt;/p&gt;

&lt;h5 id=&quot;basic-statistic-in-python&quot;&gt;Basic Statistic in Python&lt;/h5&gt;
&lt;p&gt;Basic math statistics with python&lt;/p&gt;

&lt;h5 id=&quot;basic-natural-language-processing&quot;&gt;Basic Natural Language Processing&lt;/h5&gt;
&lt;ol&gt;
  &lt;li&gt;Tokenization&lt;/li&gt;
  &lt;li&gt;Stopword Removal&lt;/li&gt;
  &lt;li&gt;N-Grams&lt;/li&gt;
  &lt;li&gt;WordSense Disambiguation&lt;/li&gt;
  &lt;li&gt;Parts-of-Speech&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;em&gt;This module uses NLTK for the text processing processes. It is important to note that you will need to download nltk_data.&lt;/em&gt;&lt;/p&gt;

&lt;h5 id=&quot;simple-probability-model&quot;&gt;Simple Probability Model&lt;/h5&gt;
&lt;ol&gt;
  &lt;li&gt;Common Ground&lt;/li&gt;
  &lt;li&gt;Limit Theorems&lt;/li&gt;
  &lt;li&gt;Derived Distributions
    &lt;ul&gt;
      &lt;li&gt;Covariance&lt;/li&gt;
      &lt;li&gt;Correlation&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h5 id=&quot;imbalanced-learning-with-gaussians&quot;&gt;Imbalanced Learning with Gaussians&lt;/h5&gt;
&lt;p&gt;&lt;code&gt;TBA&lt;/code&gt;&lt;/p&gt;

&lt;h5 id=&quot;neural-network&quot;&gt;Neural Network&lt;/h5&gt;
&lt;ol&gt;
  &lt;li&gt;Adaline&lt;/li&gt;
  &lt;li&gt;Perceptron&lt;/li&gt;
  &lt;li&gt;Simple Neural Network&lt;/li&gt;
  &lt;li&gt;Train a Linear Classifier&lt;/li&gt;
  &lt;li&gt;A simple implementation of convolutional neural networks&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Wed, 15 Feb 2017 00:00:00 -0200</pubDate>
        <link>http://localhost:4000/2017/02/15/apresentando-meriat-ml-notes/</link>
        <guid isPermaLink="true">http://localhost:4000/2017/02/15/apresentando-meriat-ml-notes/</guid>
        
        
        <category>IA</category>
        
        <category>Machine Learning</category>
        
        <category>Data Science</category>
        
        <category>Deep Learning</category>
        
        <category>Jupyter Notebook</category>
        
        <category>Python</category>
        
      </item>
    
  </channel>
</rss>
